{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f571cc5-36da-4c2f-ae7f-1935516b9131",
   "metadata": {},
   "source": [
    "# User defined variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffc18a25-f28e-419e-adb0-99f9ea9048ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_state_abbr = 'MN' # State USPS abbreviation\n",
    "training_state_name = 'Minnesota' \n",
    "nas_id = 5 # European Frogbit = 1110; Eurasian watermilfoil = 237; Hydrilla verticillata = 6 (get these identifiers from https://nas.er.usgs.gov/api/v2/species)\n",
    "nas_name = 'ZM'\n",
    "# IA = 19; ID = 16; IL = 17; MN = 27; MO = 29; MT = 30; OR = 41;  WA = 53; WI = 55\n",
    "training_fip = 27\n",
    "# Replace last 2 digits with your state's FIP code\n",
    "training_path = 'my_data/' + training_state_abbr + '_ch3/' # leave this alone \n",
    "my_crs = \"EPSG:5070\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9b3dba6-25a5-4217-82a9-be1f5f8cf03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This version allows you to create predictors for in-state models where an AIS exist as well as a second set of predictors for \n",
    "predicting to your state if your AIS is not present. You only need to make the training predictors if you are training and predicting \n",
    "to the same state.  To predict to a different state you need to make predictors for that state too.\n",
    "'''\n",
    "#Import required Python packages. \n",
    "'''\n",
    "If you get an error here, search for the package name online + conda installation; open a new Anaconda PowerShell window;\n",
    "activate your environment and paste in the code you found online into the prompt.  It will most likely be: pip install \"package-name\" \n",
    "for Python packages or conda install conda-forge:: \"package-name\" for cross-platform packages (i.e., Scikit Learn).  \n",
    "'''\n",
    "\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import zipfile\n",
    "import random\n",
    "import io\n",
    "from io import StringIO, BytesIO\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "# Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from typing import Set, Tuple\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.mask import mask\n",
    "import rasterio.plot\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from affine import Affine\n",
    "import networkx as nx\n",
    "from shapely.geometry import Point, LineString, MultiLineString, MultiPoint\n",
    "from shapely.ops import snap, nearest_points\n",
    "from rasterio.transform import from_bounds\n",
    "from shapely.geometry import box\n",
    "from shapely.ops import unary_union\n",
    "from rasterio.merge import merge\n",
    "from rasterio.errors import RasterioIOError\n",
    "from joblib import Parallel, delayed\n",
    "# Machine Learning & Statistical Modeling\n",
    "import sklearn as skl\n",
    "from sklearn.neighbors import KDTree, KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import gaussian_process\n",
    "\n",
    "# Geostatistics & Interpolation\n",
    "import skgstat as skg\n",
    "import gstools as gs\n",
    "from skgstat import models\n",
    "from skgstat.util.likelihood import get_likelihood\n",
    "import scipy\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.spatial import distance_matrix\n",
    "from scipy.optimize import minimize\n",
    "from scipy.spatial import cKDTree\n",
    "from shapely.strtree import STRtree\n",
    "from scipy.interpolate import NearestNDInterpolator\n",
    "import scipy.ndimage\n",
    "import pykrige.kriging_tools as kt\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# APIs & Requests\n",
    "import requests\n",
    "from pygbif import occurrences as occ\n",
    "from pygbif import species\n",
    "from shapely import union_all\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from contextlib import contextmanager\n",
    "import joblib\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from multiprocessing import Manager, Lock\n",
    "import tempfile\n",
    "from shapely.geometry import CAP_STYLE\n",
    "from shapely.ops import unary_union, polygonize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cef442c4-0f8e-4b9f-8783-e00e153a5c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Water quality helper functions\n",
    "def get_usgs_stations_in_state(state_fip, state_abbr, my_path):\n",
    "    \"\"\"\n",
    "    Downloads USGS water monitoring station data and filters it to the specified state boundary.\n",
    "\n",
    "    Parameters:\n",
    "    - state_fip (str): State FIPS code in the form 'US:55'\n",
    "    - state_abbr (str): Two-letter state abbreviation (e.g., 'WI')\n",
    "    - my_path (str): Directory to save shapefile (must end with '/')\n",
    "\n",
    "    Returns:\n",
    "    - GeoDataFrame: Filtered monitoring stations within the state\n",
    "    \"\"\"\n",
    "    # === Step 1: Download USGS station data ===\n",
    "    print(\"üì• Downloading USGS station data...\")\n",
    "    url_base = 'https://www.waterqualitydata.us/data/Station/search?'\n",
    "    request_url = f\"{url_base}countrycode=US&statecode=US:{state_fip}\"\n",
    "    station_df = pd.read_csv(request_url)\n",
    "\n",
    "    stations = station_df[['MonitoringLocationIdentifier', 'LatitudeMeasure', 'LongitudeMeasure']].rename(columns={\n",
    "        'MonitoringLocationIdentifier': 'station_id',\n",
    "        'LatitudeMeasure': 'latitude',\n",
    "        'LongitudeMeasure': 'longitude'\n",
    "    })\n",
    "\n",
    "    stations_gdf = gpd.GeoDataFrame(\n",
    "        stations,\n",
    "        geometry=gpd.points_from_xy(stations.longitude, stations.latitude),\n",
    "        crs=\"EPSG:4326\"\n",
    "    ).to_crs(\"EPSG:5070\")\n",
    "\n",
    "    # === Step 2: Download state boundaries shapefile ===\n",
    "    print(\"üó∫Ô∏è  Downloading state boundary shapefile...\")\n",
    "    state_boundary_url = 'http://www2.census.gov/geo/tiger/TIGER2012/STATE/tl_2012_us_state.zip'\n",
    "    r = requests.get(state_boundary_url)\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    z.extractall(path=my_path)\n",
    "    \n",
    "    # Load shapefile\n",
    "    shp_path = os.path.join(my_path, [f for f in z.namelist() if f.endswith('.shp')][0])\n",
    "    state_boundary = gpd.read_file(shp_path).to_crs(\"EPSG:5070\")\n",
    "\n",
    "    # === Step 3: Filter to selected state ===\n",
    "    state = state_boundary[state_boundary['STUSPS'] == state_abbr]\n",
    "    if state.empty:\n",
    "        raise ValueError(f\"No state found for abbreviation '{state_abbr}'\")\n",
    "\n",
    "    # Match CRS and filter points within state\n",
    "    stations_gdf = stations_gdf.to_crs(state.crs)\n",
    "    filtered_stations = gpd.sjoin(\n",
    "        stations_gdf, state, how=\"inner\", predicate=\"within\"\n",
    "    ).drop(columns=[\"index_right\"])\n",
    "\n",
    "    # === Step 4: Save to file and return ===\n",
    "    out_file = os.path.join(my_path, f\"usgs_stations_{state_abbr}.shp\")\n",
    "    filtered_stations.to_file(out_file)\n",
    "    print(f\"‚úÖ Filtered station shapefile saved to: {out_file}\")\n",
    "\n",
    "    return filtered_stations\n",
    "\n",
    "\n",
    "\n",
    "def find_nearest(row, other_gdf):\n",
    "    # Find the index of the nearest geometry\n",
    "    nearest_idx = other_gdf.distance(row.geometry).idxmin()\n",
    "    return other_gdf.loc[nearest_idx]\n",
    "def add_nearest(gdf1, gdf2):\n",
    "    nearest_neighbors = gdf1.apply(lambda row: find_nearest(row, gdf2), axis=1)\n",
    "    # Add nearest neighbor information to the first GeoDataFrame\n",
    "    gdf1['nearest_id'] = nearest_neighbors['station_id']\n",
    "    gdf1['median'] = nearest_neighbors['median']\n",
    "    parameter_added = gdf1\n",
    "    return parameter_added\n",
    "def thin_geodataframe(gdf, min_dist=100):\n",
    "    \"\"\"\n",
    "    Spatially thin a GeoDataFrame using a minimum distance (in meters).\n",
    "    Ensures points are at least `min_dist` apart.\n",
    "    \"\"\"\n",
    "    if gdf.crs.to_epsg() != 5070:\n",
    "        gdf = gdf.to_crs(\"EPSG:5070\")\n",
    "\n",
    "    # Create mapping from geometry to index\n",
    "    geom_to_index = {id(geom): idx for idx, geom in zip(gdf.index, gdf.geometry)}\n",
    "    strtree = STRtree(gdf.geometry.values)\n",
    "\n",
    "    kept_geoms = []\n",
    "    taken_ids = set()\n",
    "\n",
    "    for geom in gdf.geometry:\n",
    "        if id(geom) in taken_ids:\n",
    "            continue\n",
    "\n",
    "        kept_geoms.append(geom)\n",
    "        buffered = geom.buffer(min_dist)\n",
    "        for neighbor in strtree.query(buffered):\n",
    "            taken_ids.add(id(neighbor))\n",
    "\n",
    "    return gpd.GeoDataFrame(geometry=kept_geoms, crs=gdf.crs)\n",
    "\n",
    "# Data acquisition & formating\n",
    "def download_wqp_csv(url, max_retries=3, backoff=5):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            with requests.get(url, stream=True, timeout=60) as r:\n",
    "                r.raise_for_status()\n",
    "                content = r.content  # read all bytes\n",
    "            return io.BytesIO(content)\n",
    "        except (requests.exceptions.RequestException, requests.exceptions.ChunkedEncodingError) as e:\n",
    "            print(f\"Download failed (attempt {attempt+1}/{max_retries}): {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Retrying in {backoff} seconds...\")\n",
    "                time.sleep(backoff)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "\n",
    "\n",
    "def get_water_quality(characteristic, state_fip, stations_gdf, my_path):\n",
    "    \"\"\"\n",
    "    Download, clean, summarize, and spatially join water quality data from WQP.\n",
    "\n",
    "    Parameters:\n",
    "    - characteristic (str): Water quality variable (e.g., 'pH', 'Calcium', 'Nitrogen', 'Phosphorus', 'Oxygen', 'Salinity', 'Temperature, water')\n",
    "    - state_fip (str): State FIPS code (e.g., '55' for Wisconsin)\n",
    "    - stations_gdf (GeoDataFrame): Monitoring stations to match to data\n",
    "    - my_path (str): Path to save output shapefile (include trailing slash)\n",
    "\n",
    "    Returns:\n",
    "    - GeoDataFrame: Stations with median value for the specified characteristic\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    \n",
    "    # === Step 1: Download data ===\n",
    "    print(f\"üì• Downloading {characteristic} data from WQP...\")\n",
    "    url_base = 'https://www.waterqualitydata.us/data/Result/search?'\n",
    "    request_url = f\"{url_base}countrycode=US&statecode=US:{state_fip}&characteristicName={characteristic}&mimeType=csv\"\n",
    "    csv_buffer = download_wqp_csv(request_url)\n",
    "    df = pd.read_csv(csv_buffer)\n",
    "    print(f\"‚úÖ Retrieved {len(df)} records\")\n",
    "\n",
    "    # === Step 2: Clean and rename ===\n",
    "    df = df.dropna(subset=['ResultMeasureValue'])\n",
    "    df['ResultMeasureValue'] = pd.to_numeric(df['ResultMeasureValue'], errors='coerce')\n",
    "    \n",
    "    df_clean = df[['CharacteristicName', 'ActivityStartDate', 'MonitoringLocationIdentifier',\n",
    "                   'ResultMeasureValue', 'ResultMeasure/MeasureUnitCode']].rename(columns={\n",
    "        \"CharacteristicName\": \"Characteristic\",\n",
    "        \"ActivityStartDate\": \"date\",\n",
    "        \"MonitoringLocationIdentifier\": \"station_id\",\n",
    "        \"ResultMeasureValue\": \"value\",\n",
    "        \"ResultMeasure/MeasureUnitCode\": \"unit\"\n",
    "    })\n",
    "\n",
    "    # === Step 3: Standardize units ===\n",
    "    df_clean['unit'] = df_clean['unit'].astype(str).str.strip().str.lower()\n",
    "\n",
    "    char_lower = characteristic.lower()\n",
    "    # Process Calcium, Nitrogen, Phosphorus as before\n",
    "    if char_lower in ['calcium', 'nitrogen', 'phosphorus']:\n",
    "        df_clean.loc[df_clean['unit'].isin(['ug/l', 'ppb', 'mg/g']), 'value'] /= 1000\n",
    "        df_clean['unit'] = df_clean['unit'].replace({\n",
    "            'mg/l': 'mg/l', 'mg/l as p': 'mg/l', 'mg/l po4': 'mg/l',\n",
    "            'mg/kg': 'mg/l', 'mg/kg as p': 'mg/l',\n",
    "            'ug/l': 'mg/l', 'ppb': 'mg/l', 'mg/g': 'mg/l'\n",
    "        })\n",
    "\n",
    "    elif char_lower == 'ph':\n",
    "        df_clean = df_clean[(df_clean['unit'].isna()) | (df_clean['unit'].isin(['std units', '', 'na']))]\n",
    "        df_clean['unit'] = 'std units'\n",
    "\n",
    "    elif char_lower == 'oxygen':\n",
    "        # Convert mg/L oxygen to % saturation using multiplier 12.67 (approximate)\n",
    "        df_clean.loc[df_clean['unit'] == 'mg/l', 'value'] *= 12.67\n",
    "        df_clean['unit'] = df_clean['unit'].replace({'mg/l': '% saturatn', '% by vol': '% saturatn'})\n",
    "\n",
    "    elif char_lower == 'salinity':\n",
    "        # Convert salinity to PSU (practical salinity units)\n",
    "        # Common units include ppt, PSU, mg/L (rare, then convert to ppt)\n",
    "        # 1 ppt ‚âà 1 PSU\n",
    "        # Convert mg/L to ppt by dividing by 1000 (assuming 1 ppt = 1000 mg/L)\n",
    "        # For simplicity, unify all to 'ppt'\n",
    "        # Handle common unit variants\n",
    "        def convert_salinity(row):\n",
    "            u = row['unit']\n",
    "            v = row['value']\n",
    "            if u in ['psu', 'ppt', 'parts per thousand', '‚Ä∞']:\n",
    "                return v\n",
    "            elif u in ['mg/l', 'mg/l as cl', 'mg/l as nacl']:\n",
    "                return v / 1000  # convert mg/L to ppt approx\n",
    "            elif u in ['g/l']:\n",
    "                return v * 1000  # g/L to ppt (1000 ppt per g/L)\n",
    "            else:\n",
    "                # unknown units, keep as is but warn\n",
    "                return v\n",
    "\n",
    "        df_clean['value'] = df_clean.apply(convert_salinity, axis=1)\n",
    "        df_clean['unit'] = 'ppt'\n",
    "\n",
    "    elif char_lower == 'temperature, water':\n",
    "        # Convert temperature to Celsius if needed\n",
    "        def convert_temp(row):\n",
    "            u = row['unit']\n",
    "            v = row['value']\n",
    "            if u in ['c', 'deg c', '¬∞c', 'celsius']:\n",
    "                return v\n",
    "            elif u in ['f', 'deg f', '¬∞f', 'fahrenheit']:\n",
    "                return (v - 32) * 5.0 / 9.0\n",
    "            elif u in ['k', 'kelvin']:\n",
    "                return v - 273.15\n",
    "            else:\n",
    "                # unknown units, keep as is but warn\n",
    "                return v\n",
    "\n",
    "        df_clean['value'] = df_clean.apply(convert_temp, axis=1)\n",
    "        df_clean['unit'] = 'c'\n",
    "\n",
    "    # === Step 4: Filter implausible values ===\n",
    "    if char_lower == 'calcium':\n",
    "        df_clean = df_clean[(df_clean['unit'] == 'mg/l') & (df_clean['value'].between(0, 300))]\n",
    "    elif char_lower == 'ph':\n",
    "        df_clean = df_clean[(df_clean['value'].between(4, 14))]\n",
    "    elif char_lower == 'nitrogen':\n",
    "        df_clean = df_clean[(df_clean['unit'] == 'mg/l') & (df_clean['value'].between(0, 500))]\n",
    "    elif char_lower == 'phosphorus':\n",
    "        df_clean = df_clean[(df_clean['unit'] == 'mg/l') & (df_clean['value'].between(0, 1))]\n",
    "    elif char_lower == 'oxygen':\n",
    "        df_clean = df_clean[(df_clean['unit'] == '% saturatn') & (df_clean['value'].between(0, 200))]\n",
    "    elif char_lower == 'salinity':\n",
    "        # plausible range for salinity in ppt (freshwater to ocean)\n",
    "        df_clean = df_clean[(df_clean['unit'] == 'ppt') & (df_clean['value'].between(0, 42))]\n",
    "    elif char_lower == 'temperature, water':\n",
    "        # plausible range in Celsius (e.g., -5 to 40 C)\n",
    "        df_clean = df_clean[(df_clean['unit'] == 'c') & (df_clean['value'].between(-5, 40))]\n",
    "\n",
    "    # === Step 5: Aggregate by station ===\n",
    "    summary = df_clean.groupby('station_id')['value'].median().reset_index()\n",
    "    summary.rename(columns={'value': 'median'}, inplace=True)\n",
    "\n",
    "    # === Step 6: Spatial join with training stations ===\n",
    "    stations_with = pd.merge(stations_gdf, summary, on='station_id', how='inner')\n",
    "    stations_missing = stations_gdf[~stations_gdf['station_id'].isin(stations_with['station_id'])]\n",
    "\n",
    "    # === Step 7: Fill missing via nearest station ===\n",
    "    print(f\"üîÑ Imputing missing {characteristic} values...\")\n",
    "    stations_filled = add_nearest(stations_missing, stations_with)\n",
    "    final_gdf = pd.concat([stations_with, stations_filled], axis=0).drop(columns=[\"nearest_id\", \"latitude\", \"longitude\"], errors='ignore')\n",
    "\n",
    "    # === Step 8: Save shapefile ===\n",
    "    out_name = f\"usgs_{characteristic.lower().replace(' ', '_').replace(',', '').replace('-', '_')}.shp\"\n",
    "    out_path = os.path.join(my_path, out_name)\n",
    "    final_gdf.to_file(out_path)\n",
    "    print(f\"‚úÖ {characteristic} data saved to: {out_path}\")\n",
    "\n",
    "    return final_gdf\n",
    "\n",
    "\n",
    "# Functions for making predictors\n",
    "def interpolate(stations_w_data, filename, bandname=\"Interpolated_Band\", pixel_size=100, use_kriging='auto'):\n",
    "    # Ensure CRS and extract coordinates\n",
    "    stations_w_data = stations_w_data.copy()\n",
    "    stations_w_data = stations_w_data.set_crs(\"EPSG:5070\")\n",
    "    stations_w_data['x'] = stations_w_data.geometry.x\n",
    "    stations_w_data['y'] = stations_w_data.geometry.y\n",
    "    samples_df = stations_w_data[['x', 'y', 'median']]\n",
    "    coords = np.column_stack((samples_df['x'], samples_df['y']))\n",
    "    vals = samples_df['median']\n",
    "\n",
    "    # Scatter plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    art = ax.scatter(samples_df['x'], samples_df['y'], s=10, c=vals, cmap='plasma', vmin=0, vmax=vals.max())\n",
    "    plt.colorbar(art)\n",
    "    plt.title(\"Original Data Points\")\n",
    "    plt.show()\n",
    "\n",
    "    # Define bounds and grid size\n",
    "    xmin, xmax = samples_df['x'].min(), samples_df['x'].max()\n",
    "    ymin, ymax = samples_df['y'].min(), samples_df['y'].max()\n",
    "\n",
    "    # Define grid size based on pixel resolution\n",
    "    ncols = int(np.ceil((xmax - xmin) / pixel_size))\n",
    "    nrows = int(np.ceil((ymax - ymin) / pixel_size))\n",
    "    xres = pixel_size\n",
    "    yres = pixel_size\n",
    "    \n",
    "    print(f\"üìè Pixel size: {xres} m x {yres} m\")\n",
    "    print(f\"üì¶ Grid dimensions: {ncols} cols √ó {nrows} rows\")\n",
    "    \n",
    "    # Smart fallback if 'auto' is used\n",
    "    if use_kriging == 'auto':\n",
    "        use_kriging = (ncols * nrows <= 1e6)\n",
    "    \n",
    "    # Interpolation block\n",
    "    if use_kriging:\n",
    "        try:\n",
    "            print(\"üîπ Attempting Ordinary Kriging interpolation...\")\n",
    "            maxlag = np.median(pdist(coords))\n",
    "            n_lags = min(30, max(10, int(len(coords) / 10)))\n",
    "            V = skg.Variogram(coords, vals, model='exponential', maxlag=maxlag, n_lags=n_lags, normalize=False)\n",
    "            ok = skg.OrdinaryKriging(V, min_points=1, max_points=8, mode='exact')\n",
    "    \n",
    "            grid_x = np.linspace(xmin, xmax, ncols)\n",
    "            grid_y = np.linspace(ymin, ymax, nrows)\n",
    "            xx, yy = np.meshgrid(grid_x, grid_y)\n",
    "    \n",
    "            field = ok.transform(xx.flatten(), yy.flatten()).reshape(xx.shape)\n",
    "            print(\"‚úÖ Kriging completed successfully.\")\n",
    "    \n",
    "        except (MemoryError, ValueError, np.linalg.LinAlgError):\n",
    "            print(\"‚ö†Ô∏è Kriging failed, switching to Nearest Neighbor interpolation...\")\n",
    "            use_kriging = False  # Fallback to KNN\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Skipping Kriging; using Nearest Neighbor interpolation...\")\n",
    "    \n",
    "    # KNN Interpolation (used either by default or after Kriging fails)\n",
    "    if not use_kriging:\n",
    "        grid_x = np.linspace(xmin, xmax, ncols)\n",
    "        grid_y = np.linspace(ymin, ymax, nrows)\n",
    "        xx, yy = np.meshgrid(grid_x, grid_y)\n",
    "    \n",
    "        knn = KNeighborsRegressor(n_neighbors=min(8, len(coords)), weights='distance', algorithm='kd_tree', n_jobs=-1)\n",
    "        knn.fit(coords, vals)\n",
    "        query_points = np.column_stack((xx.ravel(), yy.ravel()))\n",
    "        field = knn.predict(query_points).reshape(xx.shape)\n",
    "    \n",
    "        print(\"‚úÖ Nearest Neighbor interpolation completed.\")\n",
    "\n",
    "\n",
    "    # Fill NaNs\n",
    "    def fill_missing_values(field):\n",
    "        mask = np.isnan(field)\n",
    "        if np.all(mask):\n",
    "            raise ValueError(\"All values are NaN; interpolation cannot be performed.\")\n",
    "        nearest_indices = scipy.ndimage.distance_transform_edt(mask, return_distances=False, return_indices=True)\n",
    "        field[mask] = field[tuple(nearest_indices[i][mask] for i in range(field.ndim))]\n",
    "        return field\n",
    "\n",
    "    field = fill_missing_values(field)\n",
    "\n",
    "    # Prepare raster\n",
    "    arr = np.flip(field, axis=0).astype(np.float32)\n",
    "    transform = from_origin(xmin, ymax, xres, yres)\n",
    "\n",
    "    with rasterio.open(filename, 'w', driver='GTiff',\n",
    "                       height=arr.shape[0], width=arr.shape[1],\n",
    "                       count=1, dtype=arr.dtype,\n",
    "                       crs=\"EPSG:5070\", transform=transform) as dst:\n",
    "        dst.write(arr, 1)\n",
    "        dst.set_band_description(1, bandname)\n",
    "\n",
    "    print(f\"‚úÖ Raster saved: {filename}\")\n",
    "\n",
    "    # Plot final raster\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(arr, extent=[xmin, xmax, ymin, ymax], cmap='plasma', origin=\"upper\")\n",
    "    plt.colorbar(label=bandname)\n",
    "    plt.title(\"Interpolated Raster\")\n",
    "    plt.xlabel(\"X Coordinate\")\n",
    "    plt.ylabel(\"Y Coordinate\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def combine_geotiffs(input_files, output_file):\n",
    "    \"\"\"\n",
    "    Combine single- or multi-band GeoTIFFs into one multi-band GeoTIFF,\n",
    "    preserving band names where available.\n",
    "\n",
    "    Parameters:\n",
    "        input_files (list): List of input raster file paths.\n",
    "        output_file (str): Path to output multi-band GeoTIFF.\n",
    "        state (str): Optional metadata tag (not used in core logic).\n",
    "    \"\"\"\n",
    "    all_bands = []\n",
    "    band_descriptions = []\n",
    "    meta = None\n",
    "\n",
    "    for file in input_files:\n",
    "        try:\n",
    "            with rasterio.open(file) as src:\n",
    "                if meta is None:\n",
    "                    meta = src.meta.copy()\n",
    "                    meta.update({\n",
    "                        'count': 0,  # Will be updated after reading all bands\n",
    "                        'dtype': src.dtypes[0]  # Assumes consistent dtype\n",
    "                    })\n",
    "\n",
    "                for bidx in range(1, src.count + 1):\n",
    "                    band_data = src.read(bidx)\n",
    "                    all_bands.append(band_data)\n",
    "\n",
    "                    # Try to get band description (e.g., band name)\n",
    "                    desc = src.descriptions[bidx - 1] or f'band_{bidx}'\n",
    "                    band_descriptions.append(desc)\n",
    "\n",
    "        except RasterioIOError:\n",
    "            print(f\"‚ö†Ô∏è Warning: Could not read {file}, skipping...\")\n",
    "\n",
    "    if not all_bands:\n",
    "        raise RuntimeError(\"‚ùå No valid input rasters found.\")\n",
    "\n",
    "    meta['count'] = len(all_bands)\n",
    "\n",
    "    with rasterio.open(output_file, 'w', **meta) as dst:\n",
    "        for idx, band in enumerate(all_bands, start=1):\n",
    "            dst.write(band, idx)\n",
    "            if band_descriptions[idx - 1]:\n",
    "                dst.set_band_description(idx, band_descriptions[idx - 1])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b791c42-4834-470c-8822-f566d061f157",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "‚ö†Ô∏è  Note on Warnings:\n",
    "Some warnings may appear during this script. These warnings do not indicate pipeline failure and can be safely ignored unless followed by an error.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca726f14-f463-46f8-9222-44cf017372f8",
   "metadata": {},
   "source": [
    "# Start of Water Quality predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ec64d6-9d31-4ac3-8e7c-932c56fda9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_stations_gdf = get_usgs_stations_in_state(\n",
    "    state_fip= training_fip,      \n",
    "    state_abbr= training_state_abbr,\n",
    "    my_path= training_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266ef9a8-ee47-48e7-b706-09fe339c1812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcium\n",
    "training_ca = get_water_quality(\"Calcium\", state_fip=training_fip, stations_gdf=training_stations_gdf, my_path=training_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f991fd1a-3fdb-46e1-917e-3a6be419f53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pH\n",
    "training_pH= get_water_quality(\"pH\", state_fip=training_fip, stations_gdf=training_stations_gdf, my_path=training_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fa2053-0ad3-41be-9496-f6d05b036e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dissolved Oxygen\n",
    "training_DO = get_water_quality('Oxygen', state_fip=training_fip, stations_gdf=training_stations_gdf, my_path=training_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e549808a-df07-4d91-8210-874023bf2b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nitrogen\n",
    "training_N = get_water_quality(\"Nitrogen\", state_fip=training_fip, stations_gdf=training_stations_gdf, my_path=training_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60b64d5-1d3b-419b-8017-5f8501b7ffff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phosphorus\n",
    "training_Phos = get_water_quality(\"Phosphorus\", state_fip=training_fip, stations_gdf=training_stations_gdf, my_path=training_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2a8304-f4bf-49d8-9151-9016b33782ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salinity\n",
    "training_Salinity = get_water_quality('Salinity', state_fip=training_fip, stations_gdf=training_stations_gdf, my_path=training_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d0682e-0d92-4867-89bc-eb919b22946b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Water Temperature\n",
    "training_Temp = get_water_quality('Temperature', state_fip=training_fip, stations_gdf=training_stations_gdf, my_path=training_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d10750f-473e-4d16-9335-13702b340562",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate(training_ca, training_path + training_state_abbr + '_ca.tif', 'Ca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f62f2b-8f7a-4595-b90a-9b9c39cc5331",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate(training_pH, training_path + training_state_abbr + '_pH.tif', 'pH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ebd38d-ab62-4dfd-b26d-013b984c8324",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate(training_N, training_path + training_state_abbr + '_N.tif', 'Nitrogen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6833a24d-7038-4b02-9d3b-94f40d9b7771",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate(training_DO, training_path + training_state_abbr + '_do.tif', 'DO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cce07c-fcda-4652-befb-ad4f1cd729a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate(training_Phos, training_path + training_state_abbr + '_phos.tif', 'Phos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5219e3-db6c-49a2-95ed-b25ac40fd1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate(training_Salinity, training_path + training_state_abbr + '_salinity.tif', 'Salinity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59f4d9e-fa54-4abd-97be-4952c6b4ef46",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate(training_Temp, training_path + training_state_abbr + '_temp.tif', 'Temperature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d5fd19-d150-4034-9825-641a393420f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the predictors you just created and export a final combined raster \n",
    "# Define input files\n",
    "input_files_wq_training = [\n",
    "    training_path + training_state_abbr + \"_ca.tif\", training_path + training_state_abbr + \"_pH.tif\", training_path + training_state_abbr + \"_N.tif\",\n",
    "    training_path + training_state_abbr + \"_do.tif\", training_path + training_state_abbr + \"_phos.tif\"]\n",
    "#Combine rasters into a multi-band GeoTIFF\n",
    "output_file_wq_training = training_path + training_state_abbr + \"_combined_wq.tif\" # this will replace the other combined tif you made.\n",
    "combine_geotiffs(input_files_wq_training, output_file_wq_training)\n",
    "\n",
    "print(f\"‚úÖ Multi-band raster saved as {output_file_wq_training}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c13a2da-3989-412c-a3a8-fc4948c84adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nhd_waterbodies(state_name, local_path, output_prefix=None, save_files=True,\n",
    "                         make_background_water=False, training_path=None, training_state_abbr=None):\n",
    "    \"\"\"\n",
    "    Downloads, extracts, and loads NHD shapefiles for a U.S. state, returning lakes, rivers, and flowlines.\n",
    "    \"\"\"\n",
    "    if output_prefix is None:\n",
    "        output_prefix = state_name.lower()\n",
    "\n",
    "    os.makedirs(local_path, exist_ok=True)\n",
    "\n",
    "    # Use GPKG filepaths for caching\n",
    "    lakes_gpkg = os.path.join(local_path, f\"{output_prefix}_lakes.gpkg\")\n",
    "    rivers_gpkg = os.path.join(local_path, f\"{output_prefix}_rivers.gpkg\")\n",
    "    streams_gpkg = os.path.join(local_path, f\"{output_prefix}_streams.gpkg\")\n",
    "\n",
    "    if save_files and all(os.path.exists(f) for f in [lakes_gpkg, rivers_gpkg, streams_gpkg]):\n",
    "        print(\"üìÇ Found existing GPKG files. Loading from disk...\")\n",
    "        lakes = gpd.read_file(lakes_gpkg)\n",
    "        rivers = gpd.read_file(rivers_gpkg)\n",
    "        streams = gpd.read_file(streams_gpkg)\n",
    "    else:\n",
    "        # === Step 1‚Äì5: Download, extract, load ===\n",
    "        url = f\"https://prd-tnm.s3.amazonaws.com/StagedProducts/Hydrography/NHD/State/Shape/NHD_H_{state_name}_State_Shape.zip\"\n",
    "        print(f\"üì• Downloading NHD data for {state_name}...\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"‚ùå Failed to download NHD data for {state_name}\")\n",
    "\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".zip\") as tmp_file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                tmp_file.write(chunk)\n",
    "\n",
    "        with zipfile.ZipFile(tmp_file.name, 'r') as z:\n",
    "            z.extractall(path=local_path)\n",
    "        print(\"‚úÖ Extraction complete.\")\n",
    "\n",
    "        shp_files = glob.glob(os.path.join(local_path, \"**\", \"*.shp\"), recursive=True)\n",
    "        if not shp_files:\n",
    "            raise FileNotFoundError(\"‚ùå No shapefiles found in extracted directory.\")\n",
    "\n",
    "        lakes_path = next((f for f in shp_files if \"nhdwaterbody\" in f.lower()), None)\n",
    "        if not lakes_path:\n",
    "            raise FileNotFoundError(\"‚ùå No NHDWaterbody shapefile found.\")\n",
    "        lakes = gpd.read_file(lakes_path)\n",
    "\n",
    "        rivers_path = next((f for f in shp_files if \"nhdarea\" in f.lower()), None)\n",
    "        if not rivers_path:\n",
    "            raise FileNotFoundError(\"‚ùå No NHDArea shapefile found.\")\n",
    "        rivers = gpd.read_file(rivers_path)\n",
    "\n",
    "        print(\"üìÑ Loading and combining stream flowlines...\")\n",
    "        flowline_paths = [f for f in shp_files if \"nhdflowline\" in f.lower()]\n",
    "        if not flowline_paths:\n",
    "            raise FileNotFoundError(\"‚ùå No NHDFlowline shapefiles found.\")\n",
    "        stream_dfs = [gpd.read_file(fp) for fp in flowline_paths]\n",
    "        streams = gpd.GeoDataFrame(pd.concat(stream_dfs, ignore_index=True), crs=stream_dfs[0].crs)\n",
    "\n",
    "        if save_files:\n",
    "            print(\"üíæ Saving as GeoPackage files...\")\n",
    "            lakes.to_file(lakes_gpkg, driver=\"GPKG\")\n",
    "            rivers.to_file(rivers_gpkg, driver=\"GPKG\")\n",
    "            streams.to_file(streams_gpkg, driver=\"GPKG\")\n",
    "\n",
    "    # ‚úÖ Now return regardless of source (cached or new)\n",
    "    result = {\n",
    "        \"lakes\": lakes,\n",
    "        \"rivers\": rivers,\n",
    "        \"streams\": streams\n",
    "    }\n",
    "\n",
    "    if make_background_water and training_path and training_state_abbr:\n",
    "        bg_water = pd.concat([lakes, rivers], ignore_index=True)\n",
    "        output_file = os.path.join(training_path, f\"{training_state_abbr}_bg_water.shp\")\n",
    "        bg_water.to_file(output_file)\n",
    "        result[\"bg_water\"] = bg_water\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def buffer_water_layers(\n",
    "    streams: gpd.GeoDataFrame = None,\n",
    "    lakes: gpd.GeoDataFrame = None,\n",
    "    rivers: gpd.GeoDataFrame = None,\n",
    "    simplify_tolerance: float = None,\n",
    "    filter_streams: bool = True,\n",
    "    save_path: str = None,\n",
    "    use_cached: bool = True,\n",
    "    export_merged_filename: str = None\n",
    ") -> tuple[gpd.GeoDataFrame, gpd.GeoDataFrame, gpd.GeoDataFrame, gpd.GeoDataFrame, gpd.GeoDataFrame]:\n",
    "    \"\"\"\n",
    "    Buffer and merge water layers, optionally using cached files.\n",
    "    \"\"\"\n",
    "\n",
    "    crs = \"EPSG:5070\"\n",
    "\n",
    "    # Define cache paths\n",
    "    if save_path:\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        stream_file = os.path.join(save_path, \"buffered_streams.gpkg\")\n",
    "        lake_file = os.path.join(save_path, \"buffered_lakes.gpkg\")\n",
    "        river_file = os.path.join(save_path, \"buffered_rivers.gpkg\")\n",
    "        full_stream_file = os.path.join(save_path, \"full_streams.shp\")\n",
    "\n",
    "        cache_exists = all(os.path.exists(f) for f in [stream_file, lake_file, river_file, full_stream_file])\n",
    "        if use_cached and cache_exists:\n",
    "            print(\"üìÇ Loading buffered layers from cache...\")\n",
    "            buffered_streams = gpd.read_file(stream_file)\n",
    "            buffered_lakes = gpd.read_file(lake_file)\n",
    "            buffered_rivers = gpd.read_file(river_file)\n",
    "            full_streams = gpd.read_file(full_stream_file)\n",
    "\n",
    "            buffered_water = pd.concat([buffered_streams, buffered_lakes, buffered_rivers], ignore_index=True)\n",
    "            buffered_water = buffered_water[buffered_water.geometry.notnull()]\n",
    "            buffered_water[\"waterID\"] = range(1, len(buffered_water) + 1)\n",
    "\n",
    "            if export_merged_filename and not os.path.exists(export_merged_filename):\n",
    "                print(f\"üíæ Merging and exporting combined buffered layer to {export_merged_filename}...\")\n",
    "                buffered_water.to_file(export_merged_filename)\n",
    "\n",
    "            return buffered_streams, buffered_lakes, buffered_rivers, full_streams, buffered_water\n",
    "\n",
    "    # If no cache or not using cache, we require streams/lakes/rivers\n",
    "    if streams is None or lakes is None or rivers is None:\n",
    "        raise ValueError(\"streams, lakes, and rivers must be provided if not using cached files.\")\n",
    "\n",
    "    # === Process from scratch ===\n",
    "    print(\"[1/6] Projecting all layers...\")\n",
    "    streams = streams.to_crs(crs)\n",
    "    lakes = lakes.to_crs(crs)\n",
    "    rivers = rivers.to_crs(crs)\n",
    "    full_streams = streams.copy()\n",
    "\n",
    "    print(\"[2/6] Filtering lakes and rivers with areasqkm ‚â• 0.25...\")\n",
    "    lakes = lakes[lakes.is_valid & lakes.geometry.notnull() & (lakes[\"areasqkm\"] >= 0.25)]\n",
    "    rivers = rivers[rivers.is_valid & rivers.geometry.notnull() & (rivers[\"areasqkm\"] >= 0.25)]\n",
    "\n",
    "    print(\"[3/6] Preparing stream geometry...\")\n",
    "    # Drop invalid or missing geometries before simplification\n",
    "    filtered_streams = streams.copy()\n",
    "    filtered_streams = filtered_streams[filtered_streams.geometry.notnull()].copy()\n",
    "    filtered_streams = filtered_streams[filtered_streams.is_valid].copy()\n",
    "    \n",
    "    # Now apply simplification\n",
    "    if simplify_tolerance is not None:\n",
    "        print(f\"[4/6] Simplifying stream geometries with tolerance {simplify_tolerance}...\")\n",
    "        filtered_streams[\"geometry\"] = filtered_streams.geometry.simplify(\n",
    "            tolerance=simplify_tolerance, preserve_topology=True)\n",
    "    else:\n",
    "        print(\"[4/6] Skipping simplification.\")\n",
    "\n",
    "    print(\"[5/6] Buffering lakes and rivers...\")\n",
    "    lakes[\"geometry\"] = lakes.geometry.buffer(50)\n",
    "    rivers[\"geometry\"] = rivers.geometry.buffer(50)\n",
    "\n",
    "    print(\"[6/6] Buffering stream geometries...\")\n",
    "    filtered_streams[\"geometry\"] = filtered_streams.geometry.buffer(50, cap_style=CAP_STYLE.flat)\n",
    "\n",
    "    # Optional save\n",
    "    if save_path:\n",
    "        print(\"üíæ Saving buffered layers...\")\n",
    "        filtered_streams.to_file(stream_file, driver=\"GPKG\")\n",
    "        lakes.to_file(lake_file, driver=\"GPKG\")\n",
    "        rivers.to_file(river_file, driver=\"GPKG\")\n",
    "        full_streams.to_file(full_stream_file)\n",
    "\n",
    "    buffered_water = pd.concat([filtered_streams, lakes, rivers], ignore_index=True)\n",
    "    buffered_water = buffered_water[buffered_water.geometry.notnull()]\n",
    "    buffered_water['waterID'] = range(1, len(buffered_water) + 1)\n",
    "\n",
    "    if export_merged_filename:\n",
    "        print(f\"üß± Merging and exporting combined buffered layer to {export_merged_filename}...\")\n",
    "        buffered_water.to_file(export_merged_filename)\n",
    "\n",
    "    return filtered_streams, lakes, rivers, full_streams, buffered_water\n",
    "# Biological predictor and training/background data helper functions\n",
    "def make_bg_data(\n",
    "    waterbody_gdf,\n",
    "    training_path,\n",
    "    training_state_abbr,\n",
    "    nas_name,\n",
    "    seed=42,\n",
    "    max_attempts=100,\n",
    "    thin=True,\n",
    "    min_dist=1000\n",
    "):\n",
    "    target_crs = \"EPSG:5070\"\n",
    "    \n",
    "    # Auto-load presence data\n",
    "    pos_data_path = f\"{training_path}{training_state_abbr}_{nas_name}_pos_data.shp\"\n",
    "    species_gdf_thin = gpd.read_file(pos_data_path)\n",
    "\n",
    "    # Reproject everything if needed\n",
    "    if waterbody_gdf.crs != target_crs:\n",
    "        waterbody_gdf = waterbody_gdf.to_crs(target_crs)\n",
    "    if species_gdf_thin.crs != target_crs:\n",
    "        species_gdf_thin = species_gdf_thin.to_crs(target_crs)\n",
    "\n",
    "    # Filter waterbodies by area\n",
    "    if 'areasqkm' not in waterbody_gdf.columns:\n",
    "        raise ValueError(\"Expected 'areasqkm' column in waterbody_gdf\")\n",
    "    waterbody_gdf = waterbody_gdf[waterbody_gdf['areasqkm'] > 0.25].copy()\n",
    "\n",
    "    presence_points = species_gdf_thin[species_gdf_thin['Present'] == 1]\n",
    "    sindex = presence_points.sindex\n",
    "\n",
    "    def has_presence(geom):\n",
    "        if geom.is_empty or not geom.is_valid:\n",
    "            return True\n",
    "        bounds = geom.bounds\n",
    "        candidates = list(sindex.intersection(bounds))\n",
    "        return any(presence_points.iloc[i].geometry.intersects(geom) for i in candidates)\n",
    "\n",
    "    non_ais_waterbodies = waterbody_gdf[~waterbody_gdf['geometry'].apply(has_presence)].copy()\n",
    "\n",
    "    random.seed(seed)\n",
    "    bg_points = []\n",
    "    for geom in non_ais_waterbodies.geometry.values:\n",
    "        minx, miny, maxx, maxy = geom.bounds\n",
    "        for _ in range(max_attempts):\n",
    "            x = random.uniform(minx, maxx)\n",
    "            y = random.uniform(miny, maxy)\n",
    "            pt = Point(x, y)\n",
    "            if geom.contains(pt):\n",
    "                bg_points.append(pt)\n",
    "                break\n",
    "\n",
    "    bg_gdf = gpd.GeoDataFrame(geometry=bg_points, crs=target_crs)\n",
    "\n",
    "    if thin and not bg_gdf.empty:\n",
    "        bg_gdf = thin_geodataframe(bg_gdf, min_dist=min_dist)\n",
    "\n",
    "    bg_gdf['Present'] = 0\n",
    "\n",
    "    # Save to shapefile\n",
    "    output_path = f\"{training_path}{training_state_abbr}_{nas_name}_bg_data.shp\"\n",
    "    bg_gdf.to_file(output_path)\n",
    "\n",
    "    return bg_gdf\n",
    "\n",
    "def process_nas_occurrences(\n",
    "    state,\n",
    "    crs,\n",
    "    path,\n",
    "    target_species_id=None,\n",
    "    target_species_name=None,\n",
    "    buffer_shapefile_suffix=\"_buffered_water.shp\",\n",
    "    species_columns=None,\n",
    "    snap_tolerance=100,\n",
    "    make_background=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Downloads, filters, spatially joins, and pivots NAS invasive species data.\n",
    "\n",
    "    Parameters:\n",
    "        state (str): US state abbreviation (e.g., 'MN')\n",
    "        crs (str or int): Target CRS (e.g., 'EPSG:5070')\n",
    "        path (str): Path to save/load shapefiles (must end with '/')\n",
    "        target_species_id (str, optional): SpeciesID to filter for a single species\n",
    "        target_species_name (str, optional): Common name to include in filename if target_species_id is used\n",
    "        buffer_shapefile_suffix (str): Suffix for the buffered waterbodies shapefile\n",
    "        species_columns (list): List of invasive group column names\n",
    "        snap_tolerance (int): Max distance for nearest spatial join in meters\n",
    "        make_background (bool): Whether to generate and save background points\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame: Final dataframe with invasive species richness per lake\n",
    "    \"\"\"\n",
    "    if species_columns is None:\n",
    "        species_columns = ['Inv_Algae', 'Inv_Crustaceans', 'Inv_Fish', 'Inv_Mollusks', 'Inv_Plants']\n",
    "\n",
    "    # Step 1: Call NAS API\n",
    "    url = f\"http://nas.er.usgs.gov/api/v2/occurrence/search?state={state}\"\n",
    "    response = requests.get(url, timeout=None).json()\n",
    "    results = pd.json_normalize(response, 'results')\n",
    "\n",
    "    # Step 2: Filter and clean\n",
    "    all_nas_data = results[results['status'] == 'established'].dropna()\n",
    "    all_nas_data = all_nas_data[[\"speciesID\", \"commonName\", \"group\", \"decimalLatitude\", \"decimalLongitude\"]]\n",
    "\n",
    "    # Step 3: Create GeoDataFrame and save\n",
    "    nas_gdf = gpd.GeoDataFrame(\n",
    "        all_nas_data,\n",
    "        geometry=gpd.points_from_xy(all_nas_data.decimalLongitude, all_nas_data.decimalLatitude),\n",
    "        crs=\"EPSG:4326\"\n",
    "    ).to_crs(crs).drop(columns=[\"decimalLatitude\", \"decimalLongitude\"])\n",
    "\n",
    "    species_gdf = nas_gdf[nas_gdf[\"speciesID\"] == target_species_id]\n",
    "    species_gdf_thin = thin_geodataframe(species_gdf, min_dist=100)\n",
    "    species_gdf_thin['Present'] = 1\n",
    "    species_filename = f\"{path}{state}_{target_species_name}_pos_data.shp\"\n",
    "    species_gdf_thin.to_file(species_filename)\n",
    "\n",
    "    nas_gdf = nas_gdf[nas_gdf[\"speciesID\"] != target_species_id]\n",
    "    nas_gdf.to_file(f\"{path}{state}_nas.shp\")\n",
    "\n",
    "    nas_gdf = gpd.read_file(f\"{path}{state}_nas.shp\").to_crs(crs)\n",
    "    buffered_water = gpd.read_file(f\"{path}{state}{buffer_shapefile_suffix}\").to_crs(crs)\n",
    "\n",
    "    NAS_ais_obs_df = gpd.sjoin_nearest(nas_gdf[['speciesID', 'commonName', 'group', 'geometry']],\n",
    "                                       buffered_water, how=\"inner\", max_distance=snap_tolerance)\n",
    "\n",
    "    NAS_ais_obs_df['group'] = NAS_ais_obs_df['group'].replace({\n",
    "        'Algae': 'Inv_Algae',\n",
    "        'Plants': 'Inv_Plants',\n",
    "        'Fishes': 'Inv_Fish',\n",
    "        'Crustaceans-Cladocerans': 'Inv_Crustaceans',\n",
    "        'Crustaceans-Amphipods': 'Inv_Crustaceans',\n",
    "        'Mollusks-Bivalves': 'Inv_Mollusks',\n",
    "        'Mollusks-Gastropods': 'Inv_Mollusks'\n",
    "    })\n",
    "\n",
    "    grouped = NAS_ais_obs_df[['waterID', 'commonName', 'group']].groupby(['waterID', 'group'])['commonName'].nunique().reset_index()\n",
    "    pivot_df = grouped.pivot(index='waterID', columns='group', values='commonName').reset_index().fillna(0)\n",
    "    lakes_w_invasives = pd.merge(buffered_water, pivot_df, on='waterID', how='left')\n",
    "\n",
    "    for col in species_columns:\n",
    "        if col not in lakes_w_invasives.columns:\n",
    "            lakes_w_invasives[col] = 0.0\n",
    "        else:\n",
    "            lakes_w_invasives[col] = lakes_w_invasives[col].astype('float64')\n",
    "\n",
    "    inv_rich = lakes_w_invasives[species_columns + ['geometry']].copy()\n",
    "    inv_gdf = gpd.GeoDataFrame(inv_rich, geometry='geometry', crs=lakes_w_invasives.crs)\n",
    "\n",
    "    # Step 11: Optional background generation\n",
    "    if make_background:\n",
    "        make_bg_data(\n",
    "            waterbody_gdf=buffered_water,\n",
    "            training_path=path,\n",
    "            training_state_abbr=state,\n",
    "            nas_name=target_species_name,\n",
    "            seed=42,\n",
    "            max_attempts=100,\n",
    "            thin=True,\n",
    "            min_dist=1000\n",
    "        )\n",
    "\n",
    "    return inv_gdf\n",
    "\n",
    "\n",
    "def export_inv_richness(inv_rich_gdf, output_path):\n",
    "    # Convert input data to EPSG:5070 BEFORE getting bounds\n",
    "    inv_rich_gdf = inv_rich_gdf\n",
    "\n",
    "    data_columns = ['Inv_Algae', 'Inv_Crustaceans', 'Inv_Fish', 'Inv_Mollusks', 'Inv_Plants']\n",
    "\n",
    "    # Ensure valid geometry\n",
    "    inv_rich_gdf = inv_rich_gdf[inv_rich_gdf.geometry.notnull() & inv_rich_gdf.geometry.is_valid]\n",
    "\n",
    "    # Ensure all required columns exist\n",
    "    for col in data_columns:\n",
    "        if col not in inv_rich_gdf.columns:\n",
    "            inv_rich_gdf[col] = 0\n",
    "    \n",
    "    # Warn if file exists\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"Warning: {output_path} already exists and will be overwritten.\")\n",
    "\n",
    "    if inv_rich_gdf.empty:\n",
    "        raise ValueError(\"Error: The input GeoDataFrame is empty!\")\n",
    "\n",
    "    # Get bounding box in CRS 5070\n",
    "    xmin, ymin, xmax, ymax = inv_rich_gdf.total_bounds  \n",
    "    pixel_size = 100 \n",
    "\n",
    "    # Ensure bounds are valid\n",
    "    if xmin == xmax:\n",
    "        xmin -= pixel_size\n",
    "        xmax += pixel_size\n",
    "    if ymin == ymax:\n",
    "        ymin -= pixel_size\n",
    "        ymax += pixel_size\n",
    "\n",
    "    # Recalculate width and height\n",
    "    width = max(1, int((xmax - xmin) / pixel_size))\n",
    "    height = max(1, int((ymax - ymin) / pixel_size))\n",
    "\n",
    "    # Correct transform in CRS 5070\n",
    "    transform = Affine(pixel_size, 0, xmin, 0, -pixel_size, ymax)\n",
    "    num_bands = len(data_columns) + 1  # Extra band for sum\n",
    "    raster = np.zeros((num_bands, height, width), dtype=np.float32)\n",
    "\n",
    "    # Debug: Check valid geometries\n",
    "    valid_geom_count = inv_rich_gdf.geometry.notnull().sum()\n",
    "    print(f\"Valid geometries count: {valid_geom_count}\")\n",
    "\n",
    "    # Rasterize each data column\n",
    "    for i, column in enumerate(data_columns):\n",
    "        shapes = [(geom, value) for geom, value in zip(inv_rich_gdf.geometry, inv_rich_gdf[column]) if geom is not None and not np.isnan(value)]\n",
    "        if not shapes:\n",
    "            print(f\"Skipping {column}: No valid geometries!\")\n",
    "            continue  # Skip empty bands\n",
    "        raster_band = rasterize(\n",
    "            shapes,\n",
    "            out_shape=(height, width),\n",
    "            transform=transform,\n",
    "            fill=0,\n",
    "            dtype='float32',\n",
    "            default_value=1,  # Ensure full coverage of waterbody\n",
    "            all_touched=True  # Ensures every pixel within the geometry gets a value\n",
    "        )\n",
    "        shapes = [(geom, value) for geom, value in zip(inv_rich_gdf.geometry, inv_rich_gdf[column])]\n",
    "        raster[i] = raster_band  # Keep original values\n",
    "        print(f\"Rasterized {column}: Min={raster_band.min()}, Max={raster_band.max()}, Non-zero pixels={np.count_nonzero(raster_band)}\")\n",
    "\n",
    "    # Compute the sum band\n",
    "    raster[-1] = np.sum(raster[:-1], axis=0)\n",
    "    print(f\"Final Sum Band: Min={raster[-1].min()}, Max={raster[-1].max()}, Non-zero pixels={np.count_nonzero(raster[-1])}\")\n",
    "\n",
    "    # Save as a multi-band GeoTIFF in EPSG:5070\n",
    "    with rasterio.open(\n",
    "        output_path, 'w',\n",
    "        driver='GTiff',\n",
    "        height=height,\n",
    "        width=width,\n",
    "        count=num_bands,\n",
    "        dtype=raster.dtype,\n",
    "        crs=\"EPSG:5070\",\n",
    "        transform=transform\n",
    "    ) as dst:\n",
    "        for band in range(num_bands):\n",
    "            dst.write(raster[band], band + 1)\n",
    "        band_names = data_columns + [\"Inv_Richness\"]\n",
    "        for band, name in enumerate(band_names, start=1):\n",
    "            dst.set_band_description(band, name)\n",
    "\n",
    "    # Debug: Plot overlay of geometries on raster\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.imshow(raster[-1], cmap='viridis', extent=[xmin, xmax, ymin, ymax], origin=\"upper\")\n",
    "    gpd.GeoSeries(inv_rich_gdf.geometry).plot(ax=ax, facecolor=\"none\", edgecolor=\"red\")\n",
    "    plt.title(\"Overlaying Geometries on Raster\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot bands\n",
    "    fig, axes = plt.subplots(1, num_bands, figsize=(15, 5))\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(raster[i], cmap='viridis')\n",
    "        ax.set_title(band_names[i])\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Raster file saved as '{output_path}' in CRS {\"EPSG:5070\"} with band names: {band_names}\")\n",
    "\n",
    "def process_and_export_native_fish_raster(\n",
    "    my_path: str,\n",
    "    my_crs: str,\n",
    "    state_abbr: str,\n",
    "    resolution: int = 100,\n",
    "    fish_shapefile: str = 'usgs_native_fish_rich.shp',\n",
    "    state_shapefile: str = 'tl_2012_us_state.shp',\n",
    "    buffer_shapefile_suffix: str = '_buffered_water.shp'\n",
    "):\n",
    "    def clip_points_by_polygon(points_gdf, polygon_gdf):\n",
    "        if points_gdf.crs != polygon_gdf.crs:\n",
    "            points_gdf = points_gdf.to_crs(polygon_gdf.crs)\n",
    "        clipped_points = gpd.sjoin(points_gdf, polygon_gdf, how='inner')\n",
    "        return clipped_points.drop(columns=polygon_gdf.columns.difference(['geometry']))\n",
    "\n",
    "    def sum_numeric_columns(gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "        sum_columns = [col for col in gdf.columns if col.isdigit() and len(col) == 6]\n",
    "        gdf[\"Native_Fish_Richness\"] = gdf[sum_columns].sum(axis=1)\n",
    "        return gdf[[\"Native_Fish_Richness\", \"geometry\"]]\n",
    "\n",
    "    def spatial_join_with_nearest(poly_gdf: gpd.GeoDataFrame, point_gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "        if poly_gdf.crs != point_gdf.crs:\n",
    "            poly_gdf = poly_gdf.to_crs(point_gdf.crs)\n",
    "        return gpd.sjoin_nearest(poly_gdf, point_gdf, how=\"left\", distance_col=\"distance\")\n",
    "\n",
    "    def export_native_raster(joined_gdf: gpd.GeoDataFrame, my_path, state_abbr, column_name: str = \"Native_Fish_Richness\"):\n",
    "        bounds = joined_gdf.total_bounds\n",
    "        transform = rasterio.transform.from_origin(bounds[0], bounds[3], resolution, resolution)\n",
    "        out_shape = (\n",
    "            int(np.ceil((bounds[3] - bounds[1]) / resolution)),\n",
    "            int(np.ceil((bounds[2] - bounds[0]) / resolution))\n",
    "        )\n",
    "\n",
    "        raster = rasterize(\n",
    "            [(geom, value) for geom, value in zip(joined_gdf.geometry, joined_gdf[column_name])],\n",
    "            out_shape=out_shape,\n",
    "            transform=transform,\n",
    "            fill=0,\n",
    "            dtype=rasterio.float32\n",
    "        )\n",
    "\n",
    "        parent_dir = os.path.abspath(os.path.join(my_path, os.pardir))\n",
    "        output_filename = os.path.join(my_path, f\"{state_abbr}_{column_name}.tif\")\n",
    "\n",
    "        with rasterio.open(\n",
    "            output_filename, \"w\",\n",
    "            driver=\"GTiff\",\n",
    "            height=out_shape[0],\n",
    "            width=out_shape[1],\n",
    "            count=1,\n",
    "            dtype=rasterio.float32,\n",
    "            crs=\"EPSG:5070\",\n",
    "            transform=transform\n",
    "        ) as dst:\n",
    "            dst.write(raster, 1)\n",
    "            dst.set_band_description(1, column_name)\n",
    "\n",
    "        # Optional plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(raster, cmap='viridis', extent=[bounds[0], bounds[2], bounds[1], bounds[3]])\n",
    "        plt.colorbar(label=f'{column_name} Richness')\n",
    "        plt.title('Rasterized Native Fish Richness')\n",
    "        plt.xlabel('Longitude')\n",
    "        plt.ylabel('Latitude')\n",
    "        plt.show()\n",
    "\n",
    "    # Load or download native fish data\n",
    "    fish_path = os.path.join(my_path, fish_shapefile)\n",
    "    if os.path.exists(fish_path):\n",
    "        fish_gdf = gpd.read_file(fish_path).to_crs(my_crs)\n",
    "    else:\n",
    "        url = \"https://www.sciencebase.gov/catalog/file/get/6086df60d34eadd49d31b04a?f=__disk__ad%2F21%2Ffc%2Fad21fc677379f4e45caa4bd506ca1c587d5f01f7\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            csv_data = StringIO(response.text)\n",
    "            fish_df = pd.read_csv(csv_data)\n",
    "            fish_gdf = gpd.GeoDataFrame(\n",
    "                fish_df,\n",
    "                geometry=gpd.points_from_xy(fish_df.longitude, fish_df.latitude),\n",
    "                crs=\"EPSG:4326\"\n",
    "            ).to_crs(my_crs)\n",
    "        else:\n",
    "            raise RuntimeError(f\"Failed to download native fish data: {response.status_code}\")\n",
    "\n",
    "    # Load state boundary and buffered water\n",
    "    state_boundary = gpd.read_file(os.path.join(my_path, state_shapefile)).dropna().to_crs(my_crs)\n",
    "    state_boundary = state_boundary[state_boundary['STUSPS'] == state_abbr]\n",
    "    buffered_water_path = os.path.join(my_path, state_abbr + buffer_shapefile_suffix)\n",
    "    buffered_water = gpd.read_file(buffered_water_path).to_crs(my_crs)\n",
    "\n",
    "    # Process native fish richness\n",
    "    clipped_fish = clip_points_by_polygon(fish_gdf, state_boundary)\n",
    "    native_fish_gdf = sum_numeric_columns(clipped_fish)\n",
    "    water_with_fish = spatial_join_with_nearest(buffered_water, native_fish_gdf)\n",
    "\n",
    "    # Convert int columns to float\n",
    "    for col in water_with_fish.select_dtypes(include=['int64']).columns:\n",
    "        water_with_fish[col] = water_with_fish[col].astype('float64')\n",
    "\n",
    "    export_native_raster(water_with_fish, my_path, state_abbr)\n",
    "\n",
    "def extract_roads_and_endpoints(\n",
    "    local_path,\n",
    "    state_abbr,\n",
    "    state_fips,\n",
    "    state_name,\n",
    "    buffered_water,\n",
    "    nas_id,\n",
    "    my_crs,\n",
    "    snap_dist=500\n",
    "):\n",
    "    def download_tiger_roads(state_fips, local_path):\n",
    "        url = f'https://www2.census.gov/geo/tiger/TIGER2022/PRISECROADS/tl_2022_{state_fips}_prisecroads.zip'\n",
    "        r = requests.get(url)\n",
    "        if r.status_code != 200:\n",
    "            raise Exception(f\"‚ùå Failed to download from {url}\")\n",
    "        z = zipfile.ZipFile(BytesIO(r.content))\n",
    "        z.extractall(path=local_path)\n",
    "\n",
    "    def download_ramps(local_path):\n",
    "        url = \"https://www.sciencebase.gov/catalog/file/get/63b81b50d34e92aad3cc004d?facet=Boatramps_United_States_final_20230104\"\n",
    "        extensions = {'.dbf', '.prj', '.shp', '.shx'}\n",
    "        response = requests.get(url, stream=True, timeout=60)\n",
    "        response.raise_for_status()\n",
    "        with zipfile.ZipFile(BytesIO(response.content)) as zip_ref:\n",
    "            os.makedirs(local_path, exist_ok=True)\n",
    "            for f in zip_ref.namelist():\n",
    "                if os.path.splitext(f)[1].lower() in extensions:\n",
    "                    zip_ref.extract(f, local_path)\n",
    "\n",
    "    def get_endpoints(geometry):\n",
    "        if geometry.geom_type == 'LineString':\n",
    "            return [geometry.coords[0], geometry.coords[-1]]\n",
    "        return []\n",
    "\n",
    "    def nas_api_call(nas_id, state_abbr):\n",
    "        url = f\"http://nas.er.usgs.gov/api/v2/occurrence/search?species_ID={nas_id}&state={state_abbr}\"\n",
    "        response = requests.get(url).json()\n",
    "        return pd.json_normalize(response, 'results')\n",
    "\n",
    "    def sjoin_nearest_to_centroid_replace_geom(left_gdf, right_gdf, **kwargs):\n",
    "        left_centroids = left_gdf.copy()\n",
    "        left_centroids[\"geometry_centroid\"] = left_centroids.geometry.centroid\n",
    "        left_centroids = left_centroids.set_geometry(\"geometry_centroid\")\n",
    "        right_temp = right_gdf.copy()\n",
    "        right_temp[\"geometry_right\"] = right_temp.geometry\n",
    "        right_temp = right_temp.set_geometry(\"geometry_right\")\n",
    "        joined = gpd.sjoin_nearest(left_centroids, right_temp, how=\"left\", **kwargs)\n",
    "        joined[\"distance\"] = joined.geometry_centroid.distance(joined[\"geometry_right\"])\n",
    "        result = left_gdf.copy()\n",
    "        result[\"geometry\"] = joined[\"geometry_right\"]\n",
    "        result[\"epointID\"] = joined[\"epointID\"]\n",
    "        result[\"distance_to_nearest\"] = joined[\"distance\"]\n",
    "        return result\n",
    "\n",
    "    # Step 1: Download ramps and roads\n",
    "    download_ramps(os.path.abspath(os.path.join(local_path)))\n",
    "    download_tiger_roads(state_fips, local_path)\n",
    "\n",
    "    # Step 2: Load data\n",
    "    ramps = gpd.read_file(local_path + 'Boatramps_United_States_final_20230104.shp').set_crs(my_crs, allow_override=True)\n",
    "    ramp_geo = ramps.loc[ramps['State'] == state_name, ['geometry']].copy()\n",
    "    ramp_geo['ramp_ID'] = range(1, len(ramp_geo) + 1)\n",
    "\n",
    "    buffered_water = buffered_water.to_crs(my_crs)\n",
    "    pos_data = nas_api_call(nas_id, state_abbr)\n",
    "    pos_gdf = gpd.GeoDataFrame(\n",
    "        pos_data[[\"decimalLatitude\", \"decimalLongitude\"]],\n",
    "        geometry=gpd.points_from_xy(pos_data.decimalLongitude, pos_data.decimalLatitude),\n",
    "        crs=\"EPSG:4326\"\n",
    "    ).to_crs(my_crs)\n",
    "\n",
    "    # Step 3: Identify water presence/absence\n",
    "    water_check = buffered_water.sjoin(pos_gdf, how=\"left\", predicate=\"contains\")\n",
    "    water_check = water_check.drop_duplicates(subset=\"waterID\")\n",
    "    neg_water = water_check[water_check['index_right'].isna()].drop(columns=[\"index_right\", \"decimalLatitude\", \"decimalLongitude\"], errors=\"ignore\")\n",
    "    pos_water = water_check.dropna(subset=[\"index_right\"]).drop(columns=[\"index_right\", \"decimalLatitude\", \"decimalLongitude\"], errors=\"ignore\")\n",
    "    pos_water[\"Present\"], neg_water[\"Present\"] = 1.0, 0.0\n",
    "    water_with_presence = pd.concat([pos_water, neg_water])\n",
    "\n",
    "    # Step 4: Match ramps to water\n",
    "    ramps_in_water = ramp_geo.sjoin(water_with_presence, how=\"left\", predicate=\"within\")\n",
    "    ramps_not_in_water = ramps_in_water[ramps_in_water['index_right'].isna()].drop(columns=[\"index_right\"], errors=\"ignore\").copy()\n",
    "    ramps_in_water = ramps_in_water.dropna(subset=[\"index_right\"]).drop(columns=[\"index_right\"], errors=\"ignore\")\n",
    "    ramps_in_water[\"waterID\"] = ramps_in_water[\"waterID\"].astype(\"int64\")\n",
    "\n",
    "    water_ids_with_ramps = ramps_in_water['waterID'].tolist()\n",
    "    water_no_ramps = water_with_presence[~water_with_presence['waterID'].isin(water_ids_with_ramps)]\n",
    "\n",
    "    # Step 5: Extract and deduplicate road endpoints\n",
    "    my_roads = gpd.read_file(os.path.join(local_path, f\"tl_2022_{state_fips}_prisecroads.shp\")).to_crs(my_crs)\n",
    "    endpoints = my_roads['geometry'].apply(get_endpoints).explode()\n",
    "    endpoints_df = pd.DataFrame(endpoints.tolist(), columns=['x', 'y']).drop_duplicates()\n",
    "    endpoints_gdf = gpd.GeoDataFrame(endpoints_df, geometry=gpd.points_from_xy(endpoints_df['x'], endpoints_df['y']), crs=my_crs).drop(columns=['x', 'y'])\n",
    "    endpoints_gdf['epointID'] = range(1, len(endpoints_gdf) + 1)\n",
    "\n",
    "    # Step 6: Snap ramps and lakes to endpoints\n",
    "    ramps_in_water_sj = sjoin_nearest_to_centroid_replace_geom(ramps_in_water, endpoints_gdf)\n",
    "    lakes_no_ramp_sj = sjoin_nearest_to_centroid_replace_geom(water_no_ramps, endpoints_gdf)\n",
    "    lakes_no_ramp_epoints = lakes_no_ramp_sj[['waterID', 'geometry', 'Present', 'epointID', 'distance_to_nearest']].dropna()\n",
    "    ramps_in_water_epoints = ramps_in_water_sj[['waterID', 'geometry', 'Present', 'epointID', 'distance_to_nearest']].dropna()\n",
    "    all_endpoints = pd.concat([ramps_in_water_epoints, lakes_no_ramp_epoints])\n",
    "    pos_endpoints = all_endpoints.loc[all_endpoints['Present'] == 1.0]\n",
    "    neg_endpoints = all_endpoints.loc[all_endpoints['Present'] == 0.0]\n",
    "\n",
    "    return my_roads, pos_endpoints, neg_endpoints, ramps_in_water_epoints, lakes_no_ramp_epoints\n",
    "\n",
    "def build_network_optimized(road_network_gdf, precision=6):\n",
    "    \"\"\"\n",
    "    Build a NetworkX graph from a GeoDataFrame of road LineStrings.\n",
    "\n",
    "    Nodes are rounded to a given precision to reduce floating-point redundancy.\n",
    "\n",
    "    Args:\n",
    "        road_network_gdf (GeoDataFrame): Must contain LineString or MultiLineString geometries.\n",
    "        precision (int): Number of decimal places to round coordinates to for node deduplication.\n",
    "\n",
    "    Returns:\n",
    "        networkx.Graph: Graph with nodes as (x, y) tuples and edges weighted by length.\n",
    "    \"\"\"\n",
    "    import networkx as nx\n",
    "    from shapely.geometry import LineString\n",
    "\n",
    "    G = nx.Graph()\n",
    "\n",
    "    for geom in road_network_gdf.geometry:\n",
    "        if geom is None:\n",
    "            continue\n",
    "\n",
    "        # Handle both LineString and MultiLineString\n",
    "        if isinstance(geom, LineString):\n",
    "            lines = [geom]\n",
    "        elif hasattr(geom, 'geoms'):  # MultiLineString\n",
    "            lines = list(geom.geoms)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        for line in lines:\n",
    "            coords = list(line.coords)\n",
    "            for u, v in zip(coords[:-1], coords[1:]):\n",
    "                if len(u) < 2 or len(v) < 2:\n",
    "                    continue  # skip invalid coordinate pairs\n",
    "\n",
    "                u_rounded = tuple(round(c, precision) for c in u[:2])\n",
    "                v_rounded = tuple(round(c, precision) for c in v[:2])\n",
    "                dist = LineString([u, v]).length\n",
    "\n",
    "                # Add undirected edge with distance as weight\n",
    "                G.add_edge(u_rounded, v_rounded, weight=dist)\n",
    "\n",
    "    return G\n",
    "\n",
    "def build_nearest_node_index(graph):\n",
    "    \"\"\"\n",
    "    Build a KDTree for efficient nearest-node lookup in a NetworkX graph.\n",
    "\n",
    "    Args:\n",
    "        graph (networkx.Graph): A graph with 2D tuple coordinates as nodes.\n",
    "\n",
    "    Returns:\n",
    "        cKDTree: KD-tree built from graph node coordinates.\n",
    "        list: List of node tuples in the same order as the KD-tree.\n",
    "    \"\"\"\n",
    "    # Ensure all nodes are 2D points (x, y)\n",
    "    nodes = [node for node in graph.nodes if len(node) == 2]\n",
    "    coords = [list(node) for node in nodes]  # convert tuples to lists for KDTree\n",
    "\n",
    "    tree = cKDTree(coords)\n",
    "    return tree, nodes\n",
    "    \n",
    "def find_nearest_node_kd_tree(kd_tree, point, precision=6):\n",
    "    \"\"\"\n",
    "    Find the nearest graph node to a given point using a KD-tree.\n",
    "\n",
    "    Args:\n",
    "        kd_tree (cKDTree): KDTree built from graph node coordinates.\n",
    "        point (shapely.geometry.Point): Point to find the nearest node to.\n",
    "        precision (int): Decimal places to round the coordinates.\n",
    "\n",
    "    Returns:\n",
    "        tuple: The (x, y) coordinates of the nearest node, rounded.\n",
    "    \"\"\"\n",
    "    _, index = kd_tree.query([point.x, point.y])\n",
    "    nearest_coords = kd_tree.data[index]\n",
    "    return tuple(round(coord, precision) for coord in nearest_coords)\n",
    "    \n",
    "def assign_endpoints_multi_source_dijkstra(presences_gdf, endpoints_gdf, G, kd_tree, precision=6):\n",
    "    \"\"\"\n",
    "    Assign each endpoint to its nearest presence based on road network distance.\n",
    "\n",
    "    Args:\n",
    "        presences_gdf (GeoDataFrame): GeoDataFrame of presence points with 'epointID'.\n",
    "        endpoints_gdf (GeoDataFrame): GeoDataFrame of negative endpoints with 'epointID'.\n",
    "        G (networkx.Graph): Graph of road network.\n",
    "        kd_tree (cKDTree): KDTree built from graph nodes.\n",
    "        precision (int): Decimal precision for rounding coordinates.\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: Each dict contains 'epointID', 'target_point_id', and 'distance_roads'.\n",
    "    \"\"\"\n",
    "    # Step 1: Snap presence points to graph nodes\n",
    "    presence_nodes = {}\n",
    "    node_to_presence_id = {}\n",
    "\n",
    "    for _, row in tqdm(presences_gdf.iterrows(), total=len(presences_gdf), desc=\"Snapping presences\"):\n",
    "        node = find_nearest_node_kd_tree(kd_tree, row.geometry.centroid, precision)\n",
    "        if not G.has_node(node):\n",
    "            continue  # Skip if the node isn't actually in the graph\n",
    "        presence_nodes[row[\"epointID\"]] = node\n",
    "        node_to_presence_id[node] = row[\"epointID\"]\n",
    "\n",
    "    if not presence_nodes:\n",
    "        raise ValueError(\"No presence points could be snapped to graph nodes.\")\n",
    "\n",
    "    # Step 2: Multi-source Dijkstra from all presence nodes\n",
    "    source_nodes = list(presence_nodes.values())\n",
    "    distances, predecessors = nx.multi_source_dijkstra(G, sources=source_nodes, weight='weight')\n",
    "\n",
    "    # Step 3: For each negative endpoint, find the closest presence\n",
    "    results = []\n",
    "\n",
    "    for _, row in tqdm(endpoints_gdf.iterrows(), total=len(endpoints_gdf), desc=\"Processing negatives\"):\n",
    "        neg_id = row[\"epointID\"]\n",
    "        neg_point = row.geometry.centroid\n",
    "        neg_node = find_nearest_node_kd_tree(kd_tree, neg_point, precision)\n",
    "\n",
    "        if not G.has_node(neg_node):\n",
    "            results.append({\n",
    "                \"epointID\": neg_id,\n",
    "                \"target_point_id\": None,\n",
    "                \"distance_roads\": np.inf\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        if neg_node in source_nodes:\n",
    "            # Direct match to a presence node\n",
    "            dist = 0.0\n",
    "            target_id = node_to_presence_id[neg_node]\n",
    "        elif neg_node in distances:\n",
    "            # Trace back the path to the nearest source\n",
    "            path = []\n",
    "            current = neg_node\n",
    "            while current not in source_nodes:\n",
    "                prev = predecessors.get(current)\n",
    "                if not prev:\n",
    "                    break\n",
    "                current = prev[0]  # Multi-source returns list of predecessors\n",
    "                path.append(current)\n",
    "\n",
    "            source_node = current if current in source_nodes else None\n",
    "            target_id = node_to_presence_id.get(source_node, None)\n",
    "            dist = distances[neg_node]\n",
    "        else:\n",
    "            # Not reachable\n",
    "            dist = np.inf\n",
    "            target_id = None\n",
    "\n",
    "        results.append({\n",
    "            \"epointID\": neg_id,\n",
    "            \"target_point_id\": target_id,\n",
    "            \"distance_roads\": dist\n",
    "        })\n",
    "\n",
    "    return results\n",
    "    \n",
    "def vector_to_raster(gdf, output_path, nas_name, value_field=None, resolution=100):\n",
    "    # Ensure geometries are valid\n",
    "    gdf = gdf[~gdf.geometry.isna()]\n",
    "    gdf = gdf[gdf.geometry.is_valid]\n",
    "\n",
    "    # Ensure it has a CRS\n",
    "    if gdf.crs is None:\n",
    "        raise ValueError(\"GeoDataFrame must have a CRS.\")\n",
    "\n",
    "    # Reproject to projected CRS if necessary\n",
    "    if gdf.crs.is_geographic:\n",
    "        gdf = gdf.to_crs(\"EPSG:5070\")  # NAD83 / Conus Albers (meters)\n",
    "\n",
    "    # Get bounds\n",
    "    minx, miny, maxx, maxy = gdf.total_bounds\n",
    "    width = int(np.ceil((maxx - minx) / resolution))\n",
    "    height = int(np.ceil((maxy - miny) / resolution))\n",
    "\n",
    "    # Check again\n",
    "    if width <= 0 or height <= 0:\n",
    "        raise ValueError(\"Calculated width or height is <= 0. Check your resolution and geometry bounds.\")\n",
    "\n",
    "    # Create transform\n",
    "    transform = from_origin(minx, maxy, resolution, resolution)\n",
    "\n",
    "    # Prepare shapes\n",
    "    shapes = (\n",
    "        ((geom, value) for geom, value in zip(gdf.geometry, gdf[value_field]))\n",
    "        if value_field else\n",
    "        ((geom, 1) for geom in gdf.geometry)\n",
    "    )\n",
    "\n",
    "    # Rasterize\n",
    "    raster = rasterize(\n",
    "        shapes=shapes,\n",
    "        out_shape=(height, width),\n",
    "        transform=transform,\n",
    "        fill= np.nan,\n",
    "        dtype='float32' if not value_field else 'float32'\n",
    "    )\n",
    "\n",
    "    # Write to GeoTIFF\n",
    "    band_name = f\"distance_road_{nas_name}\"\n",
    "    with rasterio.open(\n",
    "        output_path,\n",
    "        \"w\",\n",
    "        driver=\"GTiff\",\n",
    "        height=height,\n",
    "        width=width,\n",
    "        count=1,\n",
    "        dtype=raster.dtype,\n",
    "        crs=gdf.crs,\n",
    "        transform=transform,\n",
    "    ) as dst:\n",
    "        dst.write(raster, 1)\n",
    "        dst.set_band_description(1, band_name)\n",
    "\n",
    "    print(f\"Raster written to {output_path}\")\n",
    "\n",
    "def compute_and_export_road_distances(\n",
    "    my_roads: gpd.GeoDataFrame,\n",
    "    pos_endpoints: gpd.GeoDataFrame,\n",
    "    neg_endpoints: gpd.GeoDataFrame,\n",
    "    ramps_epoints: gpd.GeoDataFrame,\n",
    "    lakes_epoints: gpd.GeoDataFrame,\n",
    "    buffered_water: gpd.GeoDataFrame,\n",
    "    my_crs: str,\n",
    "    my_path: str,\n",
    "    my_state: str,\n",
    "    nas_name: str,\n",
    "    resolution: int = 100\n",
    ") -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Computes shortest road network distances from invasive presence points to absence points,\n",
    "    joins those distances to ramp/lake endpoints, and outputs a raster and plot.\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame: Waterbodies with distance_roads_total assigned.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure CRS consistency\n",
    "    my_roads = my_roads.to_crs(my_crs)\n",
    "    pos_endpoints = pos_endpoints.to_crs(my_crs)\n",
    "    neg_endpoints = neg_endpoints.to_crs(my_crs)\n",
    "\n",
    "    # Build the road network and nearest node index\n",
    "    G = build_network_optimized(my_roads)\n",
    "    kd_tree, graph_nodes = build_nearest_node_index(G)\n",
    "\n",
    "    # Run Dijkstra from positive to negative endpoints\n",
    "    dijkstra_results = assign_endpoints_multi_source_dijkstra(\n",
    "        presences_gdf=pos_endpoints,\n",
    "        endpoints_gdf=neg_endpoints,\n",
    "        G=G,\n",
    "        kd_tree=kd_tree,\n",
    "        precision=6\n",
    "    )\n",
    "    dist_df = pd.DataFrame(dijkstra_results)\n",
    "    dist_df = dist_df.loc[dist_df.groupby('epointID')['distance_roads'].idxmin()].reset_index(drop=True)\n",
    "\n",
    "    # Merge distances to endpoints\n",
    "    ramps_w_dist = pd.merge(ramps_epoints, dist_df, on='epointID', how='left')\n",
    "    lakes_w_dist = pd.merge(lakes_epoints, dist_df, on='epointID', how='left')\n",
    "\n",
    "    # Clean and select needed columns\n",
    "    keep_cols = ['geometry', 'target_point_id', 'epointID', 'waterID', 'Present', 'distance_roads', 'distance_to_nearest']\n",
    "    ramps_clean = ramps_w_dist[keep_cols].dropna()\n",
    "    lakes_clean = lakes_w_dist[keep_cols].dropna()\n",
    "\n",
    "    # Downcast numeric columns\n",
    "    for df in [ramps_clean, lakes_clean]:\n",
    "        for col in ['distance_roads', 'distance_to_nearest']:\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "\n",
    "    # Fill missing distances\n",
    "    ramps_clean['adjusted_distance_to_nearest'] = ramps_clean['distance_to_nearest']\n",
    "    lakes_clean['adjusted_distance_to_nearest'] = lakes_clean['distance_to_nearest']\n",
    "\n",
    "    ramps_clean['distance_roads_filled'] = ramps_clean['distance_roads'].fillna(0)\n",
    "    lakes_clean['distance_roads_filled'] = lakes_clean['distance_roads'].fillna(0)\n",
    "\n",
    "    # Compute total distances\n",
    "    ramps_clean['distance_roads_total'] = 0\n",
    "    ramps_clean.loc[ramps_clean['Present'] == 0, 'distance_roads_total'] = (\n",
    "        ramps_clean['distance_roads'] + ramps_clean['adjusted_distance_to_nearest']\n",
    "    )\n",
    "    ramps_clean.loc[ramps_clean['Present'] != 0, 'distance_roads_total'] = ramps_clean['distance_roads_filled']\n",
    "\n",
    "    lakes_clean['distance_roads_total'] = 0\n",
    "    lakes_clean.loc[lakes_clean['Present'] == 0, 'distance_roads_total'] = (\n",
    "        lakes_clean['distance_roads'] + lakes_clean['adjusted_distance_to_nearest']\n",
    "    )\n",
    "    lakes_clean.loc[lakes_clean['Present'] != 0, 'distance_roads_total'] = lakes_clean['distance_roads_filled']\n",
    "\n",
    "    # Clean up intermediate columns\n",
    "    ramps_clean.drop(columns=['distance_roads_filled'], inplace=True)\n",
    "    lakes_clean.drop(columns=['distance_roads_filled'], inplace=True)\n",
    "\n",
    "    # Final merge and assignment\n",
    "    final_dist_df = pd.concat([ramps_clean, lakes_clean], ignore_index=True)\n",
    "    min_dist_by_water = final_dist_df.loc[final_dist_df.groupby(\"waterID\")[\"distance_roads_total\"].idxmin()]\n",
    "    water_w_dist = pd.merge(buffered_water, min_dist_by_water, on=\"waterID\", how=\"left\")\n",
    "    water_w_dist_final = water_w_dist[[\"waterID\", \"geometry_x\", \"distance_roads_total\"]].rename(columns={\"geometry_x\": \"geometry\"}).dropna()\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    water_w_dist_final.plot(\n",
    "        ax=ax, column=\"distance_roads_total\", cmap=\"viridis\", linewidth=0.5, legend=True\n",
    "    )\n",
    "    ax.set_title(\"Water with Distance to Roads\", fontsize=14)\n",
    "    ax.set_xlabel(\"Longitude\")\n",
    "    ax.set_ylabel(\"Latitude\")\n",
    "    plt.show()\n",
    "\n",
    "    # Export raster\n",
    "    output_filename = f\"{my_path}{my_state}_road_distance_{nas_name}.tif\"\n",
    "    vector_to_raster(\n",
    "        gdf=water_w_dist_final,\n",
    "        output_path=output_filename,\n",
    "        nas_name=nas_name,  # must match function signature\n",
    "        value_field=\"distance_roads_total\",\n",
    "        resolution=resolution\n",
    "    )\n",
    "\n",
    "    return water_w_dist_final\n",
    "\n",
    "\n",
    "# Distance to source streams\n",
    "discon_value = 999999.0  # Define a consistent disconnection value\n",
    "@contextmanager\n",
    "def tqdm_joblib(tqdm_object):\n",
    "    class TqdmBatchCompletionCallback(joblib.parallel.BatchCompletionCallBack):\n",
    "        def __call__(self, *args, **kwargs):\n",
    "            tqdm_object.update(n=self.batch_size)\n",
    "            return super().__call__(*args, **kwargs)\n",
    "\n",
    "    old_callback = joblib.parallel.BatchCompletionCallBack\n",
    "    joblib.parallel.BatchCompletionCallBack = TqdmBatchCompletionCallback\n",
    "    try:\n",
    "        yield tqdm_object\n",
    "    finally:\n",
    "        joblib.parallel.BatchCompletionCallBack = old_callback\n",
    "        tqdm_object.close()\n",
    "def build_network(streams_gdf):\n",
    "    G = nx.DiGraph()\n",
    "    for geom in streams_gdf.geometry:\n",
    "        if geom is None or geom.is_empty:\n",
    "            continue\n",
    "\n",
    "        if isinstance(geom, LineString):\n",
    "            coords = list(geom.coords)\n",
    "            edges = zip(coords[:-1], coords[1:])\n",
    "        elif isinstance(geom, MultiLineString):\n",
    "            edges = []\n",
    "            for line in geom:\n",
    "                coords = list(line.coords)\n",
    "                edges.extend(zip(coords[:-1], coords[1:]))\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        for u, v in edges:\n",
    "            u_r = (round(u[0], 6), round(u[1], 6))\n",
    "            v_r = (round(v[0], 6), round(v[1], 6))\n",
    "            if u_r != v_r:\n",
    "                G.add_edge(u_r, v_r, weight=Point(u_r).distance(Point(v_r)))\n",
    "    return G\n",
    "\n",
    "def snap_points_to_vertices(points_gdf, vertices_tree, vertices_coords):\n",
    "    coords = np.array([(geom.x, geom.y) for geom in points_gdf.geometry])\n",
    "    _, idxs = vertices_tree.query(coords, k=1)\n",
    "    snapped_points = vertices_coords[idxs]\n",
    "    return snapped_points, idxs\n",
    "def calculate_dijkstra_for_point(source_node, G, vertices_in_buffer):\n",
    "    if source_node not in G:\n",
    "        print(f\"‚ö†Ô∏è  Source node {source_node} not in graph. Skipping.\")\n",
    "        return np.full(len(vertices_in_buffer), discon_value, dtype=np.float32)\n",
    "\n",
    "    try:\n",
    "        lengths = nx.single_source_dijkstra_path_length(G, source_node, weight='weight')\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error running Dijkstra from {source_node}: {e}\")\n",
    "        return np.full(len(vertices_in_buffer), discon_value, dtype=np.float32)\n",
    "\n",
    "    vertex_distances = np.full(len(vertices_in_buffer), discon_value, dtype=np.float32)\n",
    "    reachable = 0\n",
    "    for j, target_geom in enumerate(vertices_in_buffer.geometry):\n",
    "        target_node = (round(target_geom.x, 6), round(target_geom.y, 6))\n",
    "        if target_node in lengths:\n",
    "            vertex_distances[j] = lengths[target_node]\n",
    "            reachable += 1\n",
    "\n",
    "    print(f\"‚úÖ Source {source_node}: reached {reachable} / {len(vertices_in_buffer)} targets\")\n",
    "    return vertex_distances\n",
    "# --- Main Processing Function ---\n",
    "def process_state(state_abbr, my_path, streams_all, presence_all):\n",
    "    print(f\"\\n--- Processing {state_abbr} ---\")\n",
    "    shp_dir = os.path.join(my_path, f\"{state_abbr}_vertex_distances\")\n",
    "    os.makedirs(shp_dir, exist_ok=True)\n",
    "    shp_path = os.path.join(my_path, f\"{state_abbr}_vertex_distances.gpkg\")\n",
    "\n",
    "    try:\n",
    "        if streams_all.empty:\n",
    "            print(f\"Skipping {state_abbr}: no streams found.\")\n",
    "            return None\n",
    "\n",
    "        # Ensure CRS is consistent\n",
    "        streams = streams_all.copy()\n",
    "        if presence_all.crs != streams.crs:\n",
    "            presence_points = presence_all.to_crs(streams.crs)\n",
    "            print(f\"üìê Reprojected presence points to match stream CRS: {streams.crs}\")\n",
    "        else:\n",
    "            presence_points = presence_all.copy()\n",
    "\n",
    "        if presence_points.empty:\n",
    "            print(f\"{state_abbr}: no presence points found in state. Skipping.\")\n",
    "            return None\n",
    "\n",
    "        print(f\"Extracting vertices for {state_abbr}...\")\n",
    "        all_vertices = []\n",
    "        for geom in tqdm(streams.geometry, desc=f\"{state_abbr} stream vertices\", unit=\"geom\"):\n",
    "            if geom is None or geom.is_empty:\n",
    "                continue\n",
    "            if isinstance(geom, LineString):\n",
    "                all_vertices.extend(geom.coords)\n",
    "            elif isinstance(geom, MultiLineString):\n",
    "                for line in geom.geoms:\n",
    "                    all_vertices.extend(line.coords)\n",
    "\n",
    "        vertices_gdf = gpd.GeoDataFrame(\n",
    "            geometry=gpd.points_from_xy(\n",
    "                [pt[0] for pt in all_vertices],\n",
    "                [pt[1] for pt in all_vertices]\n",
    "            ),\n",
    "            crs=streams.crs\n",
    "        ).drop_duplicates()\n",
    "\n",
    "        vertices_gdf = vertices_gdf[~((vertices_gdf.geometry.x == 0) & (vertices_gdf.geometry.y == 0))]\n",
    "        vertices_coords = np.array([(pt.x, pt.y) for pt in vertices_gdf.geometry])\n",
    "        vertices_tree = cKDTree(vertices_coords)\n",
    "\n",
    "        G = build_network(streams)\n",
    "        print(f\"üìä Graph has {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
    "\n",
    "        snapped_coords, _ = snap_points_to_vertices(presence_points, vertices_tree, vertices_coords)\n",
    "        snapped_coords = np.round(snapped_coords, 6)\n",
    "\n",
    "        # Validate source nodes\n",
    "        valid_sources = [tuple(coord) for coord in snapped_coords if tuple(coord) in G]\n",
    "        print(f\"‚úÖ Valid source nodes in graph: {len(valid_sources)} / {len(snapped_coords)}\")\n",
    "\n",
    "        def is_node_in_graph(geom):\n",
    "            return (round(geom.x, 6), round(geom.y, 6)) in G\n",
    "\n",
    "        vertices_in_graph = vertices_gdf[vertices_gdf.geometry.apply(is_node_in_graph)]\n",
    "\n",
    "        if vertices_in_graph.empty:\n",
    "            print(f\"{state_abbr}: no stream vertices found in graph. Skipping.\")\n",
    "            return None\n",
    "\n",
    "        print(f\"Running parallel Dijkstra for {len(valid_sources)} valid presence-adjacent points in {state_abbr}...\")\n",
    "\n",
    "        presence_tree = cKDTree(snapped_coords)\n",
    "        vertex_coords = np.array([(geom.x, geom.y) for geom in vertices_in_graph.geometry])\n",
    "        _, nearest_presence_idxs = presence_tree.query(vertex_coords, k=1)\n",
    "\n",
    "        presence_to_vertices = defaultdict(list)\n",
    "        for v_idx, p_idx in enumerate(nearest_presence_idxs):\n",
    "            presence_to_vertices[p_idx].append(v_idx)\n",
    "\n",
    "        print(f\"Running optimized Dijkstra for {len(presence_to_vertices)} unique presence points in {state_abbr}...\")\n",
    "\n",
    "        min_distances = np.full(len(vertices_in_graph), discon_value, dtype=np.float32)\n",
    "\n",
    "        with tqdm_joblib(tqdm(desc=f\"{state_abbr} Dijkstra\", total=len(presence_to_vertices), unit=\"pt\")):\n",
    "            results = Parallel(n_jobs=2, backend=\"threading\")(\n",
    "                delayed(calculate_dijkstra_for_point)(\n",
    "                    tuple(snapped_coords[p_idx]),\n",
    "                    G,\n",
    "                    vertices_in_graph.iloc[v_idxs]\n",
    "                )\n",
    "                for p_idx, v_idxs in presence_to_vertices.items()\n",
    "                if tuple(snapped_coords[p_idx]) in G\n",
    "            )\n",
    "\n",
    "        for (p_idx, v_idxs), dist_array in zip(presence_to_vertices.items(), results):\n",
    "            for local_idx, global_idx in enumerate(v_idxs):\n",
    "                min_distances[global_idx] = min(min_distances[global_idx], dist_array[local_idx])\n",
    "\n",
    "        vertices_in_graph[\"distance_r\"] = min_distances.astype(np.float32)\n",
    "\n",
    "        print(f\"üìà distance_r summary: min={np.min(min_distances[min_distances < discon_value]):.2f}, \"\n",
    "              f\"max={np.max(min_distances[min_distances < discon_value]):.2f}, \"\n",
    "              f\"unreachable={(min_distances == discon_value).sum()}\")\n",
    "\n",
    "        vertices_in_graph.to_file(shp_path, driver=\"GPKG\")\n",
    "        print(f\"‚úÖ {state_abbr} vertex shapefile exported to: {shp_path}\")\n",
    "        return shp_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in {state_abbr}: {e}\")\n",
    "        return None\n",
    "# Cell 2: Utility functions\n",
    "\n",
    "def safe_read_file(fp):\n",
    "    try:\n",
    "        return gpd.read_file(fp)\n",
    "    except Exception as e:\n",
    "        raise IOError(f\"Error reading {fp}: {e}\")\n",
    "\n",
    "def build_kdtree(vertices):\n",
    "    coords = np.array(list(zip(vertices.geometry.x, vertices.geometry.y)))\n",
    "    tree = cKDTree(coords)\n",
    "    return tree, coords\n",
    "\n",
    "def query_parallel(tree, points, n_jobs=-1):\n",
    "    results = Parallel(n_jobs=n_jobs, backend=\"threading\")(\n",
    "        delayed(tree.query)(pt) for pt in tqdm(points, desc=\"Querying KDTree\", unit=\"pt\")\n",
    "    )\n",
    "    return zip(*results)\n",
    "\n",
    "def add_line_ids(stream_lines):\n",
    "    stream_lines['line_id'] = stream_lines.index.astype(str)\n",
    "    return stream_lines\n",
    "\n",
    "def assign_vertices_to_lines(vertices, stream_lines):\n",
    "    assert vertices.crs == stream_lines.crs, \"CRS mismatch between vertices and stream lines\"\n",
    "    print(\"Assigning vertices to stream lines via spatial join...\")\n",
    "    joined = gpd.sjoin(vertices, stream_lines[['line_id', 'geometry']], how='left', predicate='within')\n",
    "    if joined['line_id'].isna().all():\n",
    "        raise ValueError(\"No vertices could be matched to stream lines. Check geometries.\")\n",
    "    return joined\n",
    "\n",
    "def filter_disconnected_lines(vertices):\n",
    "    print(\"Filtering disconnected stream segments...\")\n",
    "    bad_line_ids = (\n",
    "        vertices.groupby('line_id')['distance_r']\n",
    "        .apply(lambda x: (x == discon_value).all())\n",
    "        .loc[lambda x: x].index\n",
    "    )\n",
    "    return bad_line_ids\n",
    "\n",
    "def split_waterbodies_by_connection(waterbodies, stream_lines, bad_line_ids):\n",
    "    print(\"Splitting waterbodies into connected and disconnected sets...\")\n",
    "    disconnected = stream_lines[stream_lines['line_id'].isin(bad_line_ids)]\n",
    "    connected = stream_lines[~stream_lines['line_id'].isin(bad_line_ids)]\n",
    "    waterbodies = waterbodies.to_crs(stream_lines.crs)\n",
    "    wb_disconnected = gpd.sjoin(waterbodies, disconnected, how='inner', predicate='intersects')\n",
    "    wb_connected = gpd.sjoin(waterbodies, connected, how='inner', predicate='intersects')\n",
    "    wb_disconnected_ids = set(wb_disconnected.index)\n",
    "    wb_connected_ids = set(wb_connected.index)\n",
    "    purely_disconnected_ids = wb_disconnected_ids - wb_connected_ids\n",
    "    return (\n",
    "        waterbodies[~waterbodies.index.isin(purely_disconnected_ids)],\n",
    "        waterbodies[waterbodies.index.isin(purely_disconnected_ids)]\n",
    "    )\n",
    "# Cell 3: Rasterization and plotting\n",
    "def rasterize_distances(vertices, waterbodies_connected, waterbodies_disconnected, output_raster_fp, nas_name, pixel_size=30):\n",
    "    print(\"Rasterizing distances to source...\")\n",
    "    vertices = vertices[vertices['distance_r'].notna()]\n",
    "    vertices = vertices[vertices['distance_r'] != discon_value]\n",
    "    vertices['distance_r'] = vertices['distance_r'].astype(float)\n",
    "\n",
    "    tree, coords = build_kdtree(vertices)\n",
    "    values = vertices['distance_r'].values\n",
    "\n",
    "    all_waterbodies = gpd.GeoSeries(pd.concat([waterbodies_connected.geometry, waterbodies_disconnected.geometry]))\n",
    "    minx, miny, maxx, maxy = all_waterbodies.total_bounds\n",
    "    width = int(np.ceil((maxx - minx) / pixel_size))\n",
    "    height = int(np.ceil((maxy - miny) / pixel_size))\n",
    "    transform = from_bounds(minx, miny, maxx, maxy, width, height)\n",
    "\n",
    "    x_coords = np.linspace(minx + pixel_size / 2, maxx - pixel_size / 2, width)\n",
    "    y_coords = np.linspace(maxy - pixel_size / 2, miny + pixel_size / 2, height)\n",
    "    xx, yy = np.meshgrid(x_coords, y_coords)\n",
    "    pixel_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    print(\"Rasterizing waterbody masks...\")\n",
    "    all_waterbody_mask = rasterize(\n",
    "        [(geom, 1) for geom in all_waterbodies],\n",
    "        out_shape=(height, width),\n",
    "        transform=transform,\n",
    "        fill=0,\n",
    "        dtype='uint8'\n",
    "    )\n",
    "    connected_mask = rasterize(\n",
    "        [(geom, 1) for geom in waterbodies_connected.geometry],\n",
    "        out_shape=(height, width),\n",
    "        transform=transform,\n",
    "        fill=0,\n",
    "        dtype='uint8'\n",
    "    )\n",
    "\n",
    "    output_array = np.full((height, width), discon_value, dtype='float32')\n",
    "    idx_connected = np.where(connected_mask.ravel() == 1)[0]\n",
    "\n",
    "    print(f\"Querying {len(idx_connected)} connected pixels to KDTree...\")\n",
    "    dist, nn_idx = query_parallel(tree, list(pixel_points[idx_connected]))\n",
    "    output_array.ravel()[idx_connected] = values[list(nn_idx)]\n",
    "    output_array[all_waterbody_mask == 0] = np.nan\n",
    "\n",
    "    valid_mask = output_array != discon_value\n",
    "    if np.any(valid_mask):\n",
    "        max_val = np.nanmax(output_array[valid_mask])\n",
    "        output_array[~valid_mask] = max_val\n",
    "    else:\n",
    "        print(\"Warning: No valid distances found. Raster may be empty.\")\n",
    "\n",
    "    print(f\"Writing output raster to {output_raster_fp}...\")\n",
    "    band_name = f\"distance_river_{nas_name}\"\n",
    "    with rasterio.open(\n",
    "        output_raster_fp,\n",
    "        'w',\n",
    "        driver='GTiff',\n",
    "        height=height,\n",
    "        width=width,\n",
    "        count=1,\n",
    "        dtype='float32',\n",
    "        crs=waterbodies_connected.crs,\n",
    "        transform=transform,\n",
    "        nodata=np.nan\n",
    "    ) as dst:\n",
    "        dst.write(output_array, 1)\n",
    "        dst.set_band_description(1, band_name)\n",
    "\n",
    "    return output_array\n",
    "\n",
    "def preview_raster(array):\n",
    "    plt.imshow(array, cmap='viridis')\n",
    "    plt.colorbar(label='Distance to Source')\n",
    "    plt.title('Rasterized Distance to Stream Source')\n",
    "    plt.show()\n",
    "\n",
    "def log_metadata(output_fp, metadata_dict):\n",
    "    print(f\"Logging metadata to {output_fp}...\")\n",
    "    with open(output_fp, 'w') as f:\n",
    "        json.dump(metadata_dict, f, indent=2)\n",
    "\n",
    "def prepare_stream_network(stream_path, simplify_tolerance, crs):\n",
    "    streams = gpd.read_file(stream_path).to_crs(crs)\n",
    "    streams = streams.drop_duplicates(subset='geometry').reset_index(drop=True)\n",
    "    dissolved = unary_union(streams.geometry)\n",
    "    simplified = gpd.GeoDataFrame(geometry=[dissolved], crs=crs).explode(index_parts=False).reset_index(drop=True)\n",
    "\n",
    "    if simplify_tolerance is not None:\n",
    "        simplified[\"geometry\"] = simplified.simplify(tolerance=simplify_tolerance, preserve_topology=True)\n",
    "\n",
    "    return simplified\n",
    "    \n",
    "def prepare_presence_points(presence_path, crs):\n",
    "    presence = gpd.read_file(presence_path)\n",
    "    return presence.to_crs(crs)\n",
    "def run_stream_distance_model(state_abbr, streams_all, presence_all, output_dir):\n",
    "    return process_state(\n",
    "        state_abbr=state_abbr,\n",
    "        streams_all=streams_all,\n",
    "        presence_all=presence_all,\n",
    "        my_path=output_dir\n",
    "    )    \n",
    "\n",
    "def postprocess_stream_network(vertices_fp, streams_fp, water_fp):\n",
    "    streams = add_line_ids(safe_read_file(streams_fp))\n",
    "    vertices = safe_read_file(vertices_fp)\n",
    "    waterbodies = safe_read_file(water_fp)\n",
    "    \n",
    "    vertices = assign_vertices_to_lines(vertices.to_crs(streams.crs), streams)\n",
    "    bad_line_ids = filter_disconnected_lines(vertices)\n",
    "    wb_connected, wb_disconnected = split_waterbodies_by_connection(waterbodies, streams, bad_line_ids)\n",
    "    \n",
    "    return vertices, wb_connected, wb_disconnected, bad_line_ids\n",
    "\n",
    "def make_stream_distance_raster(vertices, wb_connected, wb_disconnected, output_fp, metadata_fp, nas_name, pixel_size=100, state_abbr=None):\n",
    "    array = rasterize_distances(vertices, wb_connected, wb_disconnected, output_fp, nas_name, pixel_size)\n",
    "    log_metadata(metadata_fp, {\n",
    "        'state': state_abbr,\n",
    "        'pixel_size': pixel_size,\n",
    "        'bad_line_ids': list(filter_disconnected_lines(vertices)),\n",
    "        'vertex_count': len(vertices),\n",
    "        'raster_output': output_fp\n",
    "    })\n",
    "    return array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cb9a35-e950-44cd-8054-d5e6b67d9239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the lake shapefile you just downloaded (It takes a minute)\n",
    "MNDNR_lakes = gpd.read_file(training_path + 'dnr_hydro_features_all.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae54f5b3-4ac1-4cbd-8cbe-344b4fc3d2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_gdf = MNDNR_lakes.dropna(subset=['dowlknum']).to_crs(my_crs)\n",
    "lakes_gdf['dowlknum'] = lakes_gdf['dowlknum'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd1c3ff-b772-4bdd-b0fc-89a6cd78c76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add functionality for EMF and to do WI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c916e2-076c-4c03-8fcf-23cb71d44626",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_df = pd.read_csv(training_path + 'All_risk_table.csv')  # Adjust path and filename accordingly\n",
    "# Example: DOW field may be 'DOWLKNUM' or similar\n",
    "dow_field = 'dowlknum'  # change if needed\n",
    "# Rename risk_df DOW column to match shapefile for join\n",
    "risk_df = risk_df.rename(columns={\"DOW number\": dow_field, 'Boater movement risk score': 'Boater_movement', 'Water connectivity risk score': 'Water_connectivity'})\n",
    "# 3) Merge lakes and risk data on DOW number\n",
    "lakes_risk = lakes_gdf.merge(risk_df[[dow_field, 'Boater_movement', 'Water_connectivity']],\n",
    "                         on=dow_field,\n",
    "                         how=\"inner\")\n",
    "\n",
    "# 4) Project to a metric CRS (e.g., NAD83 UTM Zone 15N EPSG:26915) for 100m pixels\n",
    "lakes_with_risk = lakes_risk[['dowlknum', 'geometry',\n",
    "       'Boater_movement', 'Water_connectivity']].to_crs(my_crs)\n",
    "lakes_with_risk.to_file(training_path + \"lakes_risk.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00c36f9-09cb-4bec-9696-715cc0021b94",
   "metadata": {},
   "source": [
    "# Start of Biological Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea19635-26ab-45b0-b96f-2acef2957114",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download/import water data.\n",
    "water_data_training = get_nhd_waterbodies(\n",
    "    state_name=training_state_name,\n",
    "    local_path=training_path,\n",
    "    output_prefix=training_state_abbr,\n",
    "    save_files=True,\n",
    "    make_background_water=True, # set to false here, unless you want this for future model training within your state\n",
    "    training_path=training_path,\n",
    "    training_state_abbr=training_state_abbr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddde4eae-06be-49ce-bc45-4530a422ff69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buffer individual water layers\n",
    "buffered_streams_training, buffered_lakes_training, buffered_rivers_training, full_streams_training, buffered_water_training = buffer_water_layers(\n",
    "    streams=water_data_training[\"streams\"],\n",
    "    lakes=water_data_training[\"lakes\"],\n",
    "    rivers=water_data_training[\"rivers\"],\n",
    "    filter_streams=False,\n",
    "    save_path= training_path,\n",
    "    use_cached=True,\n",
    "    export_merged_filename=f\"{training_path}{training_state_abbr}_buffered_water.shp\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5849a2a-97b7-4776-8942-13b03d0fce25",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_rich_training = process_nas_occurrences(\n",
    "    state= training_state_abbr,\n",
    "    crs= my_crs,\n",
    "    path= training_path,\n",
    "    target_species_id= nas_id,\n",
    "    target_species_name= nas_name,\n",
    "    make_background=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6496350-c1d0-487f-8e40-4dcb4807a44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_inv_richness(inv_rich_training, training_path + training_state_abbr + '_' + nas_name +'_inv_richness.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953fb947-7162-4dcb-812e-3c068b480730",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_and_export_native_fish_raster(\n",
    "    my_path=training_path, \n",
    "    my_crs=\"EPSG:5070\",\n",
    "    state_abbr=training_state_abbr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b90ed0d-1e64-4c00-a2a1-e654fe721d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the predictors you just created and export a final combined raster \n",
    "# Define input files\n",
    "input_files_bio_training = [\n",
    "    training_path + training_state_abbr +'_' + nas_name + \"_inv_richness.tif\", \n",
    "    training_path + training_state_abbr + \"_Native_Fish_Richness.tif\"]\n",
    "#Combine rasters into a multi-band GeoTIFF\n",
    "output_file_bio_training = training_path + training_state_abbr + '_' + nas_name + \"_combined_bio.tif\" # this will replace the other combined tif you made.\n",
    "combine_geotiffs(input_files_bio_training, output_file_bio_training)\n",
    "\n",
    "print(f\"‚úÖ Multi-band raster saved as {output_file_bio_training}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c5a859-49f9-48ad-a526-a663b10f8f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Note these distance predictors are highly correlated. You should make the predictor that is most relevant to your taxa\n",
    "or if your target does spread by both mechanisms make both and let the model sort it out. This is kind of nuanced as Road Distance may also\n",
    "be informative of AIS which were originally stocked and now spread by stream/river network such as Rainbow and Brook Trout.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92117b4c-5d65-4838-9a2f-eb95d3722505",
   "metadata": {},
   "source": [
    "# Road Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953688f1-bd76-451c-b2a2-577a51dec72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import and format data for road network/distance analysis\n",
    "my_roads, pos_endpoints, neg_endpoints, ramps_in_water_epoints, lakes_no_ramp_epoints = extract_roads_and_endpoints(\n",
    "    local_path=training_path,\n",
    "    state_abbr=training_state_abbr,\n",
    "    state_fips=training_fip,\n",
    "    state_name=training_state_abbr,\n",
    "    buffered_water = buffered_water_training,\n",
    "    nas_id=nas_id,\n",
    "    my_crs=my_crs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a84bab-686d-4aee-8ed4-afc7f6d7ce3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_with_dist = compute_and_export_road_distances(\n",
    "    my_roads=my_roads,\n",
    "    pos_endpoints=pos_endpoints,\n",
    "    neg_endpoints=neg_endpoints,\n",
    "    ramps_epoints=ramps_in_water_epoints,\n",
    "    lakes_epoints=lakes_no_ramp_epoints,\n",
    "    buffered_water=buffered_water_training,\n",
    "    my_crs=my_crs,\n",
    "    my_path= training_path,\n",
    "    my_state=training_state_abbr,\n",
    "    nas_name= nas_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4ac656-ef2a-4b07-80f2-a47082fff9e0",
   "metadata": {},
   "source": [
    "# Stream/River Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22bdbf4-1a25-42f6-a5ca-89ea3cd8b447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "stream_path = os.path.join(training_path, f\"full_streams.shp\")\n",
    "presence_path = os.path.join(training_path, f\"{training_state_abbr}_{nas_name}_pos_data.shp\")\n",
    "output_raster_fp = os.path.join(training_path, f\"{training_state_abbr}_dist_to_src_river_{nas_name}.tif\")\n",
    "metadata_fp = output_raster_fp.replace('.tif', '_metadata.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac67c9a-a217-42ff-a20c-e501af7a8f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare inputs\n",
    "streams_all = prepare_stream_network(stream_path, simplify_tolerance=1000, crs=my_crs)\n",
    "presence_all = prepare_presence_points(presence_path, crs=my_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797c1bc3-50a0-41e6-828a-0f3147056837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Run Dijkstra model\n",
    "run_stream_distance_model(training_state_abbr, streams_all, presence_all, training_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d74554-55c4-4448-b6fa-a46997b7241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Postprocess results\n",
    "vertices_fp = os.path.join(training_path, f\"{training_state_abbr}_vertex_distances.gpkg\")\n",
    "streams_fp = stream_path\n",
    "water_fp = os.path.join(training_path, f\"{training_state_abbr}_buffered_water.shp\")\n",
    "vertices, wb_conn, wb_disc, _ = postprocess_stream_network(vertices_fp, streams_fp, water_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09ab9da-3756-4146-a470-65d2aff38cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Rasterize\n",
    "raster_array = make_stream_distance_raster(vertices, wb_conn, wb_disc, output_raster_fp, metadata_fp, nas_name, pixel_size=100, state_abbr=training_state_abbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf31ded-47b0-4e3f-a13b-a18f2bd8b350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the predictors you just created and export a final combined raster \n",
    "# Define input files\n",
    "input_files_dist_training = [\n",
    "    training_path + training_state_abbr +'_road_distance_' + nas_name + \".tif\", \n",
    "    training_path + training_state_abbr + '_dist_to_src_river_' + nas_name + '.tif']\n",
    "#Combine rasters into a multi-band GeoTIFF\n",
    "output_file_dist_training = training_path + training_state_abbr + '_' + nas_name + \"_combined_dist.tif\" # this will replace the other combined tif you made.\n",
    "combine_geotiffs(input_files_dist_training, output_file_dist_training)\n",
    "\n",
    "print(f\"‚úÖ Multi-band raster saved as {output_file_dist_training}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249553ef-b341-45e8-ae99-47d94447f86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Homerange similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b627c1e-d186-46d8-a59e-d9f6d03b4d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***You need to run this GEE script for your state and homerange first***\n",
    "# https://code.earthengine.google.com/b5d49bb675cc2d6583e866dc1dfb440b\n",
    "#get taxon key for your AIS from gbif\n",
    "#species.name_backbone(name='Dreissena polymorpha', kingdom='animal')\n",
    "my_training_state = 'MN' # should be the postal code abbreviation for the state you created the environmental raster for....\n",
    "my_nas_id = 5 # go to USGS NAS database for species_ids (e.g., 5 = Zebra Mussels; 237 = Eurasian watermilfoil; 551 = Bighead carp)\n",
    "my_path = 'data/' + my_training_state + '/'\n",
    "homerange_raster = my_path + \"homerange_2003_2022.tif\"\n",
    "invaded_raster = my_path + \"inv_rsd_2003_2022.tif\"\n",
    "my_countries = [\"RU\", \"UA\", \"BG\", \"RO\", \"GE\", \"AZ\", \"TM\", \"KZ\"] # Endemic range countries for your taxa\n",
    "my_taxon = 2287072  # gbif taxon id ; Eurasian watermilfoil = 2362486; Zebra mussels = 2362486\n",
    "limit = 10000 # This is for the gbif function so you don't blow up your computer... Just kidding that shouldn't happen : )\n",
    "my_scale = 1000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f680bb5-e796-487e-9837-54a155328219",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import requests\n",
    "import glob\n",
    "from pygbif import occurrences as occ \n",
    "from pygbif import species\n",
    "import rasterio\n",
    "from matplotlib import pyplot as plt\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.mask import mask\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Functions\n",
    "def gbif_api_call(taxon, country, limit):\n",
    "    \"\"\"Fetch GBIF occurrences for a given taxon and country.\"\"\"\n",
    "    URL_BASE = 'https://api.gbif.org/v1/'\n",
    "    url_request = f\"{URL_BASE}occurrence/search?taxonKey={taxon}&country={country}&limit={limit}\"  \n",
    "    response = requests.get(url_request, timeout=30)\n",
    "    return response.json()  # Return the JSON response directly\n",
    "\n",
    "def nas_api_call(nas_id, state):\n",
    "    URL_BASE = 'http://nas.er.usgs.gov/api/v2/'\n",
    "    url_request = f\"{URL_BASE}/occurrence/search?species_ID={nas_id}&state={my_training_state}\"\n",
    "    response = requests.get(url_request, timeout=None).json()\n",
    "    results = pd.json_normalize(response, 'results')\n",
    "    return results\n",
    "\n",
    "\n",
    "def sample_multiband_geotiff_with_names(raster_path, gdf):\n",
    "    \"\"\"\n",
    "    Samples a multi-band GeoTIFF at specified point locations from a GeoDataFrame,\n",
    "    using band names from the raster.\n",
    "\n",
    "    Parameters:\n",
    "    - raster_path (str): Path to the GeoTIFF file.\n",
    "    - gdf (GeoDataFrame): GeoDataFrame containing point geometries.\n",
    "\n",
    "    Returns:\n",
    "    - GeoDataFrame with additional columns for each band, using raster band names.\n",
    "    \"\"\"\n",
    "\n",
    "    # Open the raster file\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        # Reproject GeoDataFrame to match raster CRS if needed\n",
    "        if gdf.crs != src.crs:\n",
    "            gdf = gdf.to_crs(src.crs)\n",
    "\n",
    "        # Convert point geometries to raster pixel coordinates\n",
    "        coords = [(geom.x, geom.y) for geom in gdf.geometry]\n",
    "\n",
    "        # Sample raster at point locations (returns a list of tuples with values per band)\n",
    "        sampled_values = list(src.sample(coords))\n",
    "\n",
    "        # Get band names (if available, otherwise use default names)\n",
    "        band_names = src.descriptions if all(src.descriptions) else [f\"band_{i+1}\" for i in range(src.count)]\n",
    "\n",
    "        # Create new columns in the GeoDataFrame with the corresponding band names\n",
    "        for band_idx, band_name in enumerate(band_names):\n",
    "            gdf[band_name] = [val[band_idx] for val in sampled_values]\n",
    "\n",
    "    return gdf\n",
    "\n",
    "def filter_dataframe_columns(df, feature_choices):\n",
    "    return df[[col for col in df.columns if col in feature_choices or col == \"geometry\"]]\n",
    "\n",
    "def extract_fields(data):\n",
    "    \"\"\"Extract relevant fields from GBIF response.\"\"\"\n",
    "    extracted_data = []\n",
    "    for record in data:\n",
    "        entry = {\n",
    "            'key': record.get('key'),\n",
    "            'species': record.get('species'),\n",
    "            'decimalLatitude': record.get('decimalLatitude'),\n",
    "            'decimalLongitude': record.get('decimalLongitude'),\n",
    "            'countryCode': record.get('countryCode'),\n",
    "            'year': record.get('year')\n",
    "        }\n",
    "        extracted_data.append(entry)\n",
    "    return extracted_data\n",
    "\n",
    "def MESS(ref_df, pred_df):\n",
    "    # Extract geometry before dropping it\n",
    "    geometry = None\n",
    "    if \"geometry\" in pred_df.columns:\n",
    "        geometry = pred_df[\"geometry\"].copy()  # Save geometry separately\n",
    "        pred_df = pred_df.drop(columns=[\"geometry\", \"predID\"])  # Drop before calculations\n",
    "\n",
    "    # Ensure reference DataFrame does not include geometry\n",
    "    ref_numeric = ref_df.drop(columns=[\"geometry\"], errors=\"ignore\")  # Avoid geometry errors\n",
    "\n",
    "    # Compute min and max values for each variable\n",
    "    mins = dict(ref_numeric.min())\n",
    "    maxs = dict(ref_numeric.max())\n",
    "\n",
    "    def calculate_s(column):\n",
    "        values = ref_numeric[column]  # Reference values\n",
    "        sims = []\n",
    "\n",
    "        for element in np.array(pred_df[column]):\n",
    "            f = np.count_nonzero((values < element)) / values.size\n",
    "\n",
    "            if f == 0:\n",
    "                sim = ((element - mins[column]) / (maxs[column] - mins[column]))\n",
    "            elif 0 < f <= 50:\n",
    "                sim = 2 * f\n",
    "            elif 50 < f < 100:\n",
    "                sim = 2 * (1 - f)\n",
    "            elif f == 100:\n",
    "                sim = ((maxs[column] - element) / (maxs[column] - mins[column]))\n",
    "\n",
    "            sims.append(sim)\n",
    "\n",
    "        return sims\n",
    "\n",
    "    # Compute similarity scores for each predictor\n",
    "    sim_df = pd.DataFrame()\n",
    "    for c in pred_df.columns:\n",
    "        sim_df[c] = calculate_s(c)\n",
    "\n",
    "    # Compute MESS values\n",
    "    min_similarity = sim_df.min(axis=1)  # Least similar predictor's score\n",
    "    MoD = sim_df.idxmin(axis=1)  # Least similar predictor's name\n",
    "\n",
    "    # Combine results\n",
    "    MESS = pd.concat([min_similarity, MoD], axis=1)\n",
    "    MESS.columns = [\"MESS_Score\", \"Least_Similar_Variable\"]\n",
    "\n",
    "    # Reattach geometry if it was present\n",
    "    if geometry is not None:\n",
    "        print(\"Before reattaching geometry:\", MESS.dtypes)  # Debug print\n",
    "    \n",
    "        MESS[\"geometry\"] = geometry  # Re-add geometry\n",
    "        MESS = gpd.GeoDataFrame(MESS, geometry=\"geometry\", crs=4269)  # Convert back to GeoDataFrame\n",
    "        \n",
    "        print(\"After reattaching geometry:\", MESS.dtypes)  # Debug print\n",
    "        print(\"Geometry column exists?\", \"geometry\" in MESS.columns)\n",
    "    \n",
    "    return MESS\n",
    "\n",
    "def export_mess(joined_gdf: gpd.GeoDataFrame, resolution: int = my_scale):\n",
    "    # Ensure CRS is projected (use EPSG:5070 or appropriate for your region)\n",
    "    if joined_gdf.crs.to_epsg() != 5070:\n",
    "        joined_gdf = joined_gdf.to_crs(epsg=5070)\n",
    "\n",
    "    # Get bounds\n",
    "    bounds = joined_gdf.total_bounds  # [minx, miny, maxx, maxy]\n",
    "    print(f\"Bounds in projected CRS: {bounds}\")\n",
    "\n",
    "    # Compute raster size\n",
    "    width = int(np.ceil((bounds[2] - bounds[0]) / resolution))\n",
    "    height = int(np.ceil((bounds[3] - bounds[1]) / resolution))\n",
    "\n",
    "    if width <= 0 or height <= 0:\n",
    "        raise ValueError(f\"Invalid raster dimensions: width={width}, height={height}\")\n",
    "\n",
    "    # Define transform\n",
    "    transform = rasterio.transform.from_origin(bounds[0], bounds[3], resolution, resolution)\n",
    "\n",
    "    # Ensure \"mess\" column exists and is numeric\n",
    "    column_name = \"MESS_Score\"\n",
    "    if column_name not in joined_gdf.columns:\n",
    "        raise KeyError(f\"Column '{column_name}' is missing from the GeoDataFrame!\")\n",
    "\n",
    "    joined_gdf[column_name] = joined_gdf[column_name].fillna(0).astype(float)\n",
    "\n",
    "    # Prepare shapes for rasterization\n",
    "    shapes = [(geom, value) for geom, value in zip(joined_gdf.geometry, joined_gdf[column_name]) if not np.isnan(value)]\n",
    "\n",
    "    # Create raster\n",
    "    raster = rasterize(\n",
    "        shapes=shapes,\n",
    "        out_shape=(height, width),\n",
    "        transform=transform,\n",
    "        fill=0,\n",
    "        dtype=np.float32\n",
    "    )\n",
    "\n",
    "    # Save to file\n",
    "    output_filename = f\"{my_path}{my_training_state}_{column_name}.tif\"\n",
    "    with rasterio.open(\n",
    "        output_filename, \"w\",\n",
    "        driver=\"GTiff\",\n",
    "        height=height,\n",
    "        width=width,\n",
    "        count=1,\n",
    "        dtype=rasterio.float32,\n",
    "        crs=joined_gdf.crs,  # Use the same projected CRS\n",
    "        transform=transform\n",
    "    ) as dst:\n",
    "        dst.write(raster, 1)\n",
    "        dst.set_band_description(1, column_name)\n",
    "\n",
    "    # Check raster output\n",
    "    #print(f\"Raster saved as: {output_filename}\")\n",
    "    #print(f\"Unique raster values: {np.unique(raster)}\")  # Ensure non-zero values exist\n",
    "\n",
    "    # Plot the raster\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(raster, cmap=\"viridis\", extent=[bounds[0], bounds[2], bounds[1], bounds[3]])\n",
    "    plt.colorbar(label=f'{column_name}')\n",
    "    plt.title('Rasterized MESS')\n",
    "    plt.xlabel('X (meters)')\n",
    "    plt.ylabel('Y (meters)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cb3fc8-3387-422a-9106-187619853ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbif_result = []\n",
    "for country in my_countries:\n",
    "    result = gbif_api_call(my_taxon, country, limit)\n",
    "    gbif_result.extend(result.get(\"results\", []))  # Append results directly\n",
    "# Extract fields from all collected results\n",
    "homerange_points = pd.DataFrame(extract_fields(gbif_result))\n",
    "homerange_points = gpd.GeoDataFrame(\n",
    "    homerange_points, geometry=gpd.points_from_xy(homerange_points.decimalLongitude, homerange_points.decimalLatitude)).dropna().set_crs(4269).to_crs(5070)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1803dc-9311-4bce-a1ed-da9dbbafac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raster dataset (assume multiband raster where each band is a predictor)\n",
    "raster_path = invaded_raster\n",
    "with rasterio.open(raster_path) as src:\n",
    "    out_image = src.read()  # Read all bands without masking\n",
    "    meta = src.meta  # Store metadata for later use\n",
    "    transform = src.transform  # Affine transform for georeferencing\n",
    "\n",
    "    # Extract band names or fallback to generic names\n",
    "    band_names = [src.descriptions[i] if src.descriptions and src.descriptions[i] else f\"Band_{i+1}\" \n",
    "                  for i in range(src.count)]\n",
    "    print(\"Extracted Band Names:\", band_names)  # Debugging step\n",
    "\n",
    "# Convert extracted raster data to a DataFrame\n",
    "bands, height, width = out_image.shape\n",
    "pixels = out_image.reshape(bands, -1).T  # Flatten to (num_pixels, num_bands)\n",
    "pred_data = pd.DataFrame(pixels, columns=band_names)\n",
    "\n",
    "# Handle NoData values (if applicable)\n",
    "if meta.get(\"nodata\") is not None:\n",
    "    pred_data.replace(meta[\"nodata\"], np.nan, inplace=True)\n",
    "\n",
    "# Generate coordinates for each pixel\n",
    "row_indices, col_indices = np.indices((height, width))\n",
    "x_coords, y_coords = rasterio.transform.xy(transform, row_indices.flatten(), col_indices.flatten())\n",
    "\n",
    "# Create geometries (Point objects)\n",
    "geometries = [Point(x, y) for x, y in zip(x_coords, y_coords)]\n",
    "\n",
    "# Convert DataFrame to GeoDataFrame\n",
    "my_pred_data = gpd.GeoDataFrame(pred_data, geometry=geometries, crs=4269).reset_index().rename(columns ={'index':'predID'})\n",
    "\n",
    "print(my_pred_data.head())  # Check first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257c4fa9-4fef-4260-9851-ed8424b2e72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_choices = ['NDBI', 'NDTI', 'NDSI', 'NDCI', 'GPP_Summer', 'gHM', \n",
    "            'Heat_Insolation', 'Topo_Diversity', 'Flashiness', 'LST_Summer',\n",
    "            'LST_Winter','NDVI','LST_Spring','LST_Fall', 'Precip_Winter', \n",
    "            'Precip_Spring', 'Precip_Summer', 'Precip_Fall', 'Drawdown', 'Runoff', 'geometry', 'predID']\n",
    "ref_data = sample_multiband_geotiff_with_names(homerange_raster, homerange_points)\n",
    "my_ref_data = filter_dataframe_columns(ref_data, feature_choices).dropna()\n",
    "my_pred_data = filter_dataframe_columns(my_pred_data, feature_choices).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cf78e4-3181-42be-a5ba-0cd62f6132c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_mess = MESS(my_ref_data, my_pred_data)\n",
    "my_mess_clean = my_mess.dropna()\n",
    "export_mess(my_mess_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008ee61e-c11c-4755-bab1-7a5b9d61a573",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = gpd.read_file(\"eda_surfacewater_stations.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896dd63e-13a3-4f11-9566-d5cde7e1d05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wid = stations['wid'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14544969-3dbd-4460-9fb3-e173e5affb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_results(my_wid_list, my_format):\n",
    "    responses = []\n",
    "    URL_BASE = 'https://services.pca.state.mn.us/api/v1/'\n",
    "    for station in stations:\n",
    "            url_request = f\"{URL_BASE}surfacewater/water-units/trophic-statuses?wid={my_wid_list}&format={my_format}\"  \n",
    "            response = requests.get(url_request, timeout=None)\n",
    "            responses.append(response.json())\n",
    "            return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281f306e-1214-414f-bb10-d34b93023956",
   "metadata": {},
   "outputs": [],
   "source": [
    "return_list = []\n",
    "for i in wid:\n",
    "    return_list.append(api_results(batch_list, 'json'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ae84b3-1d23-478b-b343-a6a07e2dbe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsi_df  = pd.DataFrame()\n",
    "\n",
    "for d in return_list:\n",
    "    df_tmp = pd.json_normalize(d, record_path='data')\n",
    "    row = pd.DataFrame(df_tmp)\n",
    "    tsi_df  = pd.concat([tsi_df, row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7358459-999e-43d4-8117-3f17171fef6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsi_df.to_csv('tsi_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f399d0-14ae-4d0a-98fd-6492ee0ca03f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
