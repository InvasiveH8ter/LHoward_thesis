{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f571cc5-36da-4c2f-ae7f-1935516b9131",
   "metadata": {},
   "source": [
    "# User defined variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffc18a25-f28e-419e-adb0-99f9ea9048ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_state_abbr = 'MN' # State USPS abbreviation\n",
    "training_state_name = 'Minnesota' \n",
    "nas_id = 5 # European Frogbit = 1110; Eurasian watermilfoil = 237; Hydrilla verticillata = 6 (get these identifiers from https://nas.er.usgs.gov/api/v2/species)\n",
    "nas_name = 'ZM'\n",
    "# IA = 19; ID = 16; IL = 17; MN = 27; MO = 29; MT = 30; OR = 41;  WA = 53; WI = 55\n",
    "training_fip = 27\n",
    "# Replace last 2 digits with your state's FIP code\n",
    "training_path = 'my_data/' + training_state_abbr + '/' # leave this alone \n",
    "my_crs = \"EPSG:5070\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9b3dba6-25a5-4217-82a9-be1f5f8cf03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This version allows you to create predictors for in-state models where an AIS exist as well as a second set of predictors for \n",
    "predicting to your state if your AIS is not present. You only need to make the training predictors if you are training and predicting \n",
    "to the same state.  To predict to a different state you need to make predictors for that state too.\n",
    "'''\n",
    "#Import required Python packages. \n",
    "'''\n",
    "If you get an error here, search for the package name online + conda installation; open a new Anaconda PowerShell window;\n",
    "activate your environment and paste in the code you found online into the prompt.  It will most likely be: pip install \"package-name\" \n",
    "for Python packages or conda install conda-forge:: \"package-name\" for cross-platform packages (i.e., Scikit Learn).  \n",
    "'''\n",
    "\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import zipfile\n",
    "import random\n",
    "import io\n",
    "from io import StringIO, BytesIO\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "# Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from typing import Set, Tuple\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.mask import mask\n",
    "import rasterio.plot\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from affine import Affine\n",
    "import networkx as nx\n",
    "from shapely.geometry import Point, LineString, MultiLineString, MultiPoint\n",
    "from shapely.ops import snap, nearest_points\n",
    "from rasterio.transform import from_bounds\n",
    "from shapely.geometry import box\n",
    "from shapely.ops import unary_union\n",
    "from rasterio.merge import merge\n",
    "from rasterio.errors import RasterioIOError\n",
    "from joblib import Parallel, delayed\n",
    "# Machine Learning & Statistical Modeling\n",
    "import sklearn as skl\n",
    "from sklearn.neighbors import KDTree, KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import gaussian_process\n",
    "\n",
    "# Geostatistics & Interpolation\n",
    "import skgstat as skg\n",
    "import gstools as gs\n",
    "from skgstat import models\n",
    "from skgstat.util.likelihood import get_likelihood\n",
    "import scipy\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.spatial import distance_matrix\n",
    "from scipy.optimize import minimize\n",
    "from scipy.spatial import cKDTree\n",
    "from shapely.strtree import STRtree\n",
    "from scipy.interpolate import NearestNDInterpolator\n",
    "import scipy.ndimage\n",
    "import pykrige.kriging_tools as kt\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# APIs & Requests\n",
    "import requests\n",
    "from pygbif import occurrences as occ\n",
    "from pygbif import species\n",
    "from shapely import union_all\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from contextlib import contextmanager\n",
    "import joblib\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from multiprocessing import Manager, Lock\n",
    "import tempfile\n",
    "from shapely.geometry import CAP_STYLE\n",
    "from shapely.ops import unary_union, polygonize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cef442c4-0f8e-4b9f-8783-e00e153a5c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Water quality helper functions\n",
    "def get_usgs_stations_in_state(state_fip, state_abbr, my_path):\n",
    "    \"\"\"\n",
    "    Downloads USGS water monitoring station data and filters it to the specified state boundary.\n",
    "\n",
    "    Parameters:\n",
    "    - state_fip (str): State FIPS code in the form 'US:55'\n",
    "    - state_abbr (str): Two-letter state abbreviation (e.g., 'WI')\n",
    "    - my_path (str): Directory to save shapefile (must end with '/')\n",
    "\n",
    "    Returns:\n",
    "    - GeoDataFrame: Filtered monitoring stations within the state\n",
    "    \"\"\"\n",
    "    # === Step 1: Download USGS station data ===\n",
    "    print(\"üì• Downloading USGS station data...\")\n",
    "    url_base = 'https://www.waterqualitydata.us/data/Station/search?'\n",
    "    request_url = f\"{url_base}countrycode=US&statecode=US:{state_fip}\"\n",
    "    station_df = pd.read_csv(request_url)\n",
    "\n",
    "    stations = station_df[['MonitoringLocationIdentifier', 'LatitudeMeasure', 'LongitudeMeasure']].rename(columns={\n",
    "        'MonitoringLocationIdentifier': 'station_id',\n",
    "        'LatitudeMeasure': 'latitude',\n",
    "        'LongitudeMeasure': 'longitude'\n",
    "    })\n",
    "\n",
    "    stations_gdf = gpd.GeoDataFrame(\n",
    "        stations,\n",
    "        geometry=gpd.points_from_xy(stations.longitude, stations.latitude),\n",
    "        crs=\"EPSG:4326\"\n",
    "    ).to_crs(\"EPSG:5070\")\n",
    "\n",
    "    # === Step 2: Download state boundaries shapefile ===\n",
    "    print(\"üó∫Ô∏è  Downloading state boundary shapefile...\")\n",
    "    state_boundary_url = 'http://www2.census.gov/geo/tiger/TIGER2012/STATE/tl_2012_us_state.zip'\n",
    "    r = requests.get(state_boundary_url)\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    z.extractall(path=my_path)\n",
    "    \n",
    "    # Load shapefile\n",
    "    shp_path = os.path.join(my_path, [f for f in z.namelist() if f.endswith('.shp')][0])\n",
    "    state_boundary = gpd.read_file(shp_path).to_crs(\"EPSG:5070\")\n",
    "\n",
    "    # === Step 3: Filter to selected state ===\n",
    "    state = state_boundary[state_boundary['STUSPS'] == state_abbr]\n",
    "    if state.empty:\n",
    "        raise ValueError(f\"No state found for abbreviation '{state_abbr}'\")\n",
    "\n",
    "    # Match CRS and filter points within state\n",
    "    stations_gdf = stations_gdf.to_crs(state.crs)\n",
    "    filtered_stations = gpd.sjoin(\n",
    "        stations_gdf, state, how=\"inner\", predicate=\"within\"\n",
    "    ).drop(columns=[\"index_right\"])\n",
    "\n",
    "    # === Step 4: Save to file and return ===\n",
    "    out_file = os.path.join(my_path, f\"usgs_stations_{state_abbr}.shp\")\n",
    "    filtered_stations.to_file(out_file)\n",
    "    print(f\"‚úÖ Filtered station shapefile saved to: {out_file}\")\n",
    "\n",
    "    return filtered_stations\n",
    "\n",
    "\n",
    "\n",
    "def find_nearest(row, other_gdf):\n",
    "    # Find the index of the nearest geometry\n",
    "    nearest_idx = other_gdf.distance(row.geometry).idxmin()\n",
    "    return other_gdf.loc[nearest_idx]\n",
    "def add_nearest(gdf1, gdf2):\n",
    "    nearest_neighbors = gdf1.apply(lambda row: find_nearest(row, gdf2), axis=1)\n",
    "    # Add nearest neighbor information to the first GeoDataFrame\n",
    "    gdf1['nearest_id'] = nearest_neighbors['station_id']\n",
    "    gdf1['median'] = nearest_neighbors['median']\n",
    "    parameter_added = gdf1\n",
    "    return parameter_added\n",
    "def thin_geodataframe(gdf, min_dist=100):\n",
    "    \"\"\"\n",
    "    Spatially thin a GeoDataFrame using a minimum distance (in meters).\n",
    "    Ensures points are at least `min_dist` apart.\n",
    "    \"\"\"\n",
    "    if gdf.crs.to_epsg() != 5070:\n",
    "        gdf = gdf.to_crs(\"EPSG:5070\")\n",
    "\n",
    "    # Create mapping from geometry to index\n",
    "    geom_to_index = {id(geom): idx for idx, geom in zip(gdf.index, gdf.geometry)}\n",
    "    strtree = STRtree(gdf.geometry.values)\n",
    "\n",
    "    kept_geoms = []\n",
    "    taken_ids = set()\n",
    "\n",
    "    for geom in gdf.geometry:\n",
    "        if id(geom) in taken_ids:\n",
    "            continue\n",
    "\n",
    "        kept_geoms.append(geom)\n",
    "        buffered = geom.buffer(min_dist)\n",
    "        for neighbor in strtree.query(buffered):\n",
    "            taken_ids.add(id(neighbor))\n",
    "\n",
    "    return gpd.GeoDataFrame(geometry=kept_geoms, crs=gdf.crs)\n",
    "\n",
    "# Data acquisition & formating\n",
    "def download_wqp_csv(url, max_retries=3, backoff=5):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            with requests.get(url, stream=True, timeout=60) as r:\n",
    "                r.raise_for_status()\n",
    "                content = r.content  # read all bytes\n",
    "            return io.BytesIO(content)\n",
    "        except (requests.exceptions.RequestException, requests.exceptions.ChunkedEncodingError) as e:\n",
    "            print(f\"Download failed (attempt {attempt+1}/{max_retries}): {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Retrying in {backoff} seconds...\")\n",
    "                time.sleep(backoff)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "\n",
    "\n",
    "def get_water_quality(characteristic, state_fip, stations_gdf, my_path):\n",
    "    \"\"\"\n",
    "    Download, clean, summarize, and spatially join water quality data from WQP.\n",
    "\n",
    "    Parameters:\n",
    "    - characteristic (str): Water quality variable (e.g., 'pH', 'Calcium', 'Nitrogen', 'Phosphorus', 'Oxygen', 'Salinity', 'Temperature, water')\n",
    "    - state_fip (str): State FIPS code (e.g., '55' for Wisconsin)\n",
    "    - stations_gdf (GeoDataFrame): Monitoring stations to match to data\n",
    "    - my_path (str): Path to save output shapefile (include trailing slash)\n",
    "\n",
    "    Returns:\n",
    "    - GeoDataFrame: Stations with median value for the specified characteristic\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    \n",
    "    # === Step 1: Download data ===\n",
    "    print(f\"üì• Downloading {characteristic} data from WQP...\")\n",
    "    url_base = 'https://www.waterqualitydata.us/data/Result/search?'\n",
    "    request_url = f\"{url_base}countrycode=US&statecode=US:{state_fip}&characteristicName={characteristic}&mimeType=csv\"\n",
    "    csv_buffer = download_wqp_csv(request_url)\n",
    "    df = pd.read_csv(csv_buffer)\n",
    "    print(f\"‚úÖ Retrieved {len(df)} records\")\n",
    "\n",
    "    # === Step 2: Clean and rename ===\n",
    "    df = df.dropna(subset=['ResultMeasureValue'])\n",
    "    df['ResultMeasureValue'] = pd.to_numeric(df['ResultMeasureValue'], errors='coerce')\n",
    "    \n",
    "    df_clean = df[['CharacteristicName', 'ActivityStartDate', 'MonitoringLocationIdentifier',\n",
    "                   'ResultMeasureValue', 'ResultMeasure/MeasureUnitCode']].rename(columns={\n",
    "        \"CharacteristicName\": \"Characteristic\",\n",
    "        \"ActivityStartDate\": \"date\",\n",
    "        \"MonitoringLocationIdentifier\": \"station_id\",\n",
    "        \"ResultMeasureValue\": \"value\",\n",
    "        \"ResultMeasure/MeasureUnitCode\": \"unit\"\n",
    "    })\n",
    "\n",
    "    # === Step 3: Standardize units ===\n",
    "    df_clean['unit'] = df_clean['unit'].astype(str).str.strip().str.lower()\n",
    "\n",
    "    char_lower = characteristic.lower()\n",
    "    # Process Calcium, Nitrogen, Phosphorus as before\n",
    "    if char_lower in ['calcium', 'nitrogen', 'phosphorus']:\n",
    "        df_clean.loc[df_clean['unit'].isin(['ug/l', 'ppb', 'mg/g']), 'value'] /= 1000\n",
    "        df_clean['unit'] = df_clean['unit'].replace({\n",
    "            'mg/l': 'mg/l', 'mg/l as p': 'mg/l', 'mg/l po4': 'mg/l',\n",
    "            'mg/kg': 'mg/l', 'mg/kg as p': 'mg/l',\n",
    "            'ug/l': 'mg/l', 'ppb': 'mg/l', 'mg/g': 'mg/l'\n",
    "        })\n",
    "\n",
    "    elif char_lower == 'ph':\n",
    "        df_clean = df_clean[(df_clean['unit'].isna()) | (df_clean['unit'].isin(['std units', '', 'na']))]\n",
    "        df_clean['unit'] = 'std units'\n",
    "\n",
    "    elif char_lower == 'oxygen':\n",
    "        # Convert mg/L oxygen to % saturation using multiplier 12.67 (approximate)\n",
    "        df_clean.loc[df_clean['unit'] == 'mg/l', 'value'] *= 12.67\n",
    "        df_clean['unit'] = df_clean['unit'].replace({'mg/l': '% saturatn', '% by vol': '% saturatn'})\n",
    "\n",
    "    elif char_lower == 'salinity':\n",
    "        # Convert salinity to PSU (practical salinity units)\n",
    "        # Common units include ppt, PSU, mg/L (rare, then convert to ppt)\n",
    "        # 1 ppt ‚âà 1 PSU\n",
    "        # Convert mg/L to ppt by dividing by 1000 (assuming 1 ppt = 1000 mg/L)\n",
    "        # For simplicity, unify all to 'ppt'\n",
    "        # Handle common unit variants\n",
    "        def convert_salinity(row):\n",
    "            u = row['unit']\n",
    "            v = row['value']\n",
    "            if u in ['psu', 'ppt', 'parts per thousand', '‚Ä∞']:\n",
    "                return v\n",
    "            elif u in ['mg/l', 'mg/l as cl', 'mg/l as nacl']:\n",
    "                return v / 1000  # convert mg/L to ppt approx\n",
    "            elif u in ['g/l']:\n",
    "                return v * 1000  # g/L to ppt (1000 ppt per g/L)\n",
    "            else:\n",
    "                # unknown units, keep as is but warn\n",
    "                return v\n",
    "\n",
    "        df_clean['value'] = df_clean.apply(convert_salinity, axis=1)\n",
    "        df_clean['unit'] = 'ppt'\n",
    "\n",
    "    elif char_lower == 'temperature, water':\n",
    "        # Convert temperature to Celsius if needed\n",
    "        def convert_temp(row):\n",
    "            u = row['unit']\n",
    "            v = row['value']\n",
    "            if u in ['c', 'deg c', '¬∞c', 'celsius']:\n",
    "                return v\n",
    "            elif u in ['f', 'deg f', '¬∞f', 'fahrenheit']:\n",
    "                return (v - 32) * 5.0 / 9.0\n",
    "            elif u in ['k', 'kelvin']:\n",
    "                return v - 273.15\n",
    "            else:\n",
    "                # unknown units, keep as is but warn\n",
    "                return v\n",
    "\n",
    "        df_clean['value'] = df_clean.apply(convert_temp, axis=1)\n",
    "        df_clean['unit'] = 'c'\n",
    "\n",
    "    # === Step 4: Filter implausible values ===\n",
    "    if char_lower == 'calcium':\n",
    "        df_clean = df_clean[(df_clean['unit'] == 'mg/l') & (df_clean['value'].between(0, 300))]\n",
    "    elif char_lower == 'ph':\n",
    "        df_clean = df_clean[(df_clean['value'].between(4, 14))]\n",
    "    elif char_lower == 'nitrogen':\n",
    "        df_clean = df_clean[(df_clean['unit'] == 'mg/l') & (df_clean['value'].between(0, 500))]\n",
    "    elif char_lower == 'phosphorus':\n",
    "        df_clean = df_clean[(df_clean['unit'] == 'mg/l') & (df_clean['value'].between(0, 1))]\n",
    "    elif char_lower == 'oxygen':\n",
    "        df_clean = df_clean[(df_clean['unit'] == '% saturatn') & (df_clean['value'].between(0, 200))]\n",
    "    elif char_lower == 'salinity':\n",
    "        # plausible range for salinity in ppt (freshwater to ocean)\n",
    "        df_clean = df_clean[(df_clean['unit'] == 'ppt') & (df_clean['value'].between(0, 42))]\n",
    "    elif char_lower == 'temperature, water':\n",
    "        # plausible range in Celsius (e.g., -5 to 40 C)\n",
    "        df_clean = df_clean[(df_clean['unit'] == 'c') & (df_clean['value'].between(-5, 40))]\n",
    "\n",
    "    # === Step 5: Aggregate by station ===\n",
    "    summary = df_clean.groupby('station_id')['value'].median().reset_index()\n",
    "    summary.rename(columns={'value': 'median'}, inplace=True)\n",
    "\n",
    "    # === Step 6: Spatial join with training stations ===\n",
    "    stations_with = pd.merge(stations_gdf, summary, on='station_id', how='inner')\n",
    "    stations_missing = stations_gdf[~stations_gdf['station_id'].isin(stations_with['station_id'])]\n",
    "\n",
    "    # === Step 7: Fill missing via nearest station ===\n",
    "    print(f\"üîÑ Imputing missing {characteristic} values...\")\n",
    "    stations_filled = add_nearest(stations_missing, stations_with)\n",
    "    final_gdf = pd.concat([stations_with, stations_filled], axis=0).drop(columns=[\"nearest_id\", \"latitude\", \"longitude\"], errors='ignore')\n",
    "\n",
    "    # === Step 8: Save shapefile ===\n",
    "    out_name = f\"usgs_{characteristic.lower().replace(' ', '_').replace(',', '').replace('-', '_')}.shp\"\n",
    "    out_path = os.path.join(my_path, out_name)\n",
    "    final_gdf.to_file(out_path)\n",
    "    print(f\"‚úÖ {characteristic} data saved to: {out_path}\")\n",
    "\n",
    "    return final_gdf\n",
    "\n",
    "\n",
    "# Functions for making predictors\n",
    "def interpolate(stations_w_data, filename, bandname=\"Interpolated_Band\", pixel_size=100, use_kriging='auto'):\n",
    "    # Ensure CRS and extract coordinates\n",
    "    stations_w_data = stations_w_data.copy()\n",
    "    stations_w_data = stations_w_data.set_crs(\"EPSG:5070\")\n",
    "    stations_w_data['x'] = stations_w_data.geometry.x\n",
    "    stations_w_data['y'] = stations_w_data.geometry.y\n",
    "    samples_df = stations_w_data[['x', 'y', 'median']]\n",
    "    coords = np.column_stack((samples_df['x'], samples_df['y']))\n",
    "    vals = samples_df['median']\n",
    "\n",
    "    # Scatter plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    art = ax.scatter(samples_df['x'], samples_df['y'], s=10, c=vals, cmap='plasma', vmin=0, vmax=vals.max())\n",
    "    plt.colorbar(art)\n",
    "    plt.title(\"Original Data Points\")\n",
    "    plt.show()\n",
    "\n",
    "    # Define bounds and grid size\n",
    "    xmin, xmax = samples_df['x'].min(), samples_df['x'].max()\n",
    "    ymin, ymax = samples_df['y'].min(), samples_df['y'].max()\n",
    "\n",
    "    # Define grid size based on pixel resolution\n",
    "    ncols = int(np.ceil((xmax - xmin) / pixel_size))\n",
    "    nrows = int(np.ceil((ymax - ymin) / pixel_size))\n",
    "    xres = pixel_size\n",
    "    yres = pixel_size\n",
    "    \n",
    "    print(f\"üìè Pixel size: {xres} m x {yres} m\")\n",
    "    print(f\"üì¶ Grid dimensions: {ncols} cols √ó {nrows} rows\")\n",
    "    \n",
    "    # Smart fallback if 'auto' is used\n",
    "    if use_kriging == 'auto':\n",
    "        use_kriging = (ncols * nrows <= 1e6)\n",
    "    \n",
    "    # Interpolation block\n",
    "    if use_kriging:\n",
    "        try:\n",
    "            print(\"üîπ Attempting Ordinary Kriging interpolation...\")\n",
    "            maxlag = np.median(pdist(coords))\n",
    "            n_lags = min(30, max(10, int(len(coords) / 10)))\n",
    "            V = skg.Variogram(coords, vals, model='exponential', maxlag=maxlag, n_lags=n_lags, normalize=False)\n",
    "            ok = skg.OrdinaryKriging(V, min_points=1, max_points=8, mode='exact')\n",
    "    \n",
    "            grid_x = np.linspace(xmin, xmax, ncols)\n",
    "            grid_y = np.linspace(ymin, ymax, nrows)\n",
    "            xx, yy = np.meshgrid(grid_x, grid_y)\n",
    "    \n",
    "            field = ok.transform(xx.flatten(), yy.flatten()).reshape(xx.shape)\n",
    "            print(\"‚úÖ Kriging completed successfully.\")\n",
    "    \n",
    "        except (MemoryError, ValueError, np.linalg.LinAlgError):\n",
    "            print(\"‚ö†Ô∏è Kriging failed, switching to Nearest Neighbor interpolation...\")\n",
    "            use_kriging = False  # Fallback to KNN\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Skipping Kriging; using Nearest Neighbor interpolation...\")\n",
    "    \n",
    "    # KNN Interpolation (used either by default or after Kriging fails)\n",
    "    if not use_kriging:\n",
    "        grid_x = np.linspace(xmin, xmax, ncols)\n",
    "        grid_y = np.linspace(ymin, ymax, nrows)\n",
    "        xx, yy = np.meshgrid(grid_x, grid_y)\n",
    "    \n",
    "        knn = KNeighborsRegressor(n_neighbors=min(8, len(coords)), weights='distance', algorithm='kd_tree', n_jobs=-1)\n",
    "        knn.fit(coords, vals)\n",
    "        query_points = np.column_stack((xx.ravel(), yy.ravel()))\n",
    "        field = knn.predict(query_points).reshape(xx.shape)\n",
    "    \n",
    "        print(\"‚úÖ Nearest Neighbor interpolation completed.\")\n",
    "\n",
    "\n",
    "    # Fill NaNs\n",
    "    def fill_missing_values(field):\n",
    "        mask = np.isnan(field)\n",
    "        if np.all(mask):\n",
    "            raise ValueError(\"All values are NaN; interpolation cannot be performed.\")\n",
    "        nearest_indices = scipy.ndimage.distance_transform_edt(mask, return_distances=False, return_indices=True)\n",
    "        field[mask] = field[tuple(nearest_indices[i][mask] for i in range(field.ndim))]\n",
    "        return field\n",
    "\n",
    "    field = fill_missing_values(field)\n",
    "\n",
    "    # Prepare raster\n",
    "    arr = np.flip(field, axis=0).astype(np.float32)\n",
    "    transform = from_origin(xmin, ymax, xres, yres)\n",
    "\n",
    "    with rasterio.open(filename, 'w', driver='GTiff',\n",
    "                       height=arr.shape[0], width=arr.shape[1],\n",
    "                       count=1, dtype=arr.dtype,\n",
    "                       crs=\"EPSG:5070\", transform=transform) as dst:\n",
    "        dst.write(arr, 1)\n",
    "        dst.set_band_description(1, bandname)\n",
    "\n",
    "    print(f\"‚úÖ Raster saved: {filename}\")\n",
    "\n",
    "    # Plot final raster\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(arr, extent=[xmin, xmax, ymin, ymax], cmap='plasma', origin=\"upper\")\n",
    "    plt.colorbar(label=bandname)\n",
    "    plt.title(\"Interpolated Raster\")\n",
    "    plt.xlabel(\"X Coordinate\")\n",
    "    plt.ylabel(\"Y Coordinate\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def combine_geotiffs(input_files, output_file):\n",
    "    \"\"\"\n",
    "    Combine single- or multi-band GeoTIFFs into one multi-band GeoTIFF,\n",
    "    preserving band names where available.\n",
    "\n",
    "    Parameters:\n",
    "        input_files (list): List of input raster file paths.\n",
    "        output_file (str): Path to output multi-band GeoTIFF.\n",
    "        state (str): Optional metadata tag (not used in core logic).\n",
    "    \"\"\"\n",
    "    all_bands = []\n",
    "    band_descriptions = []\n",
    "    meta = None\n",
    "\n",
    "    for file in input_files:\n",
    "        try:\n",
    "            with rasterio.open(file) as src:\n",
    "                if meta is None:\n",
    "                    meta = src.meta.copy()\n",
    "                    meta.update({\n",
    "                        'count': 0,  # Will be updated after reading all bands\n",
    "                        'dtype': src.dtypes[0]  # Assumes consistent dtype\n",
    "                    })\n",
    "\n",
    "                for bidx in range(1, src.count + 1):\n",
    "                    band_data = src.read(bidx)\n",
    "                    all_bands.append(band_data)\n",
    "\n",
    "                    # Try to get band description (e.g., band name)\n",
    "                    desc = src.descriptions[bidx - 1] or f'band_{bidx}'\n",
    "                    band_descriptions.append(desc)\n",
    "\n",
    "        except RasterioIOError:\n",
    "            print(f\"‚ö†Ô∏è Warning: Could not read {file}, skipping...\")\n",
    "\n",
    "    if not all_bands:\n",
    "        raise RuntimeError(\"‚ùå No valid input rasters found.\")\n",
    "\n",
    "    meta['count'] = len(all_bands)\n",
    "\n",
    "    with rasterio.open(output_file, 'w', **meta) as dst:\n",
    "        for idx, band in enumerate(all_bands, start=1):\n",
    "            dst.write(band, idx)\n",
    "            if band_descriptions[idx - 1]:\n",
    "                dst.set_band_description(idx, band_descriptions[idx - 1])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b791c42-4834-470c-8822-f566d061f157",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "‚ö†Ô∏è  Note on Warnings:\n",
    "Some warnings may appear during this script. These warnings do not indicate pipeline failure and can be safely ignored unless followed by an error.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca726f14-f463-46f8-9222-44cf017372f8",
   "metadata": {},
   "source": [
    "# Start of Water Quality predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0ec64d6-9d31-4ac3-8e7c-932c56fda9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Downloading USGS station data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leif.howard\\AppData\\Local\\Temp\\ipykernel_18108\\1878486312.py:18: DtypeWarning: Columns (5,8,10,14,15,21,27,28,29,30,31,33) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  station_df = pd.read_csv(request_url)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üó∫Ô∏è  Downloading state boundary shapefile...\n",
      "‚úÖ Filtered station shapefile saved to: my_data/MN/usgs_stations_MN.shp\n"
     ]
    }
   ],
   "source": [
    "training_stations_gdf = get_usgs_stations_in_state(\n",
    "    state_fip= training_fip,      \n",
    "    state_abbr= training_state_abbr,\n",
    "    my_path= training_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "266ef9a8-ee47-48e7-b706-09fe339c1812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Downloading Calcium data from WQP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leif.howard\\AppData\\Local\\Temp\\ipykernel_18108\\1878486312.py:139: DtypeWarning: Columns (23,24,25,33,35,42,57,61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_buffer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Retrieved 54752 records\n",
      "üîÑ Imputing missing Calcium values...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Calcium\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m training_ca \u001b[38;5;241m=\u001b[39m get_water_quality(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalcium\u001b[39m\u001b[38;5;124m\"\u001b[39m, state_fip\u001b[38;5;241m=\u001b[39mtraining_fip, stations_gdf\u001b[38;5;241m=\u001b[39mtraining_stations_gdf, my_path\u001b[38;5;241m=\u001b[39mtraining_path)\n",
      "Cell \u001b[1;32mIn[6], line 246\u001b[0m, in \u001b[0;36mget_water_quality\u001b[1;34m(characteristic, state_fip, stations_gdf, my_path)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;66;03m# === Step 7: Fill missing via nearest station ===\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müîÑ Imputing missing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcharacteristic\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m values...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 246\u001b[0m stations_filled \u001b[38;5;241m=\u001b[39m add_nearest(stations_missing, stations_with)\n\u001b[0;32m    247\u001b[0m final_gdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([stations_with, stations_filled], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnearest_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m\"\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# === Step 8: Save shapefile ===\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 68\u001b[0m, in \u001b[0;36madd_nearest\u001b[1;34m(gdf1, gdf2)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_nearest\u001b[39m(gdf1, gdf2):\n\u001b[1;32m---> 68\u001b[0m     nearest_neighbors \u001b[38;5;241m=\u001b[39m gdf1\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: find_nearest(row, gdf2), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# Add nearest neighbor information to the first GeoDataFrame\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     gdf1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnearest_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m nearest_neighbors[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstation_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\masters\\Lib\\site-packages\\geopandas\\geodataframe.py:1568\u001b[0m, in \u001b[0;36mGeoDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[0;32m   1566\u001b[0m \u001b[38;5;129m@doc\u001b[39m(pd\u001b[38;5;241m.\u001b[39mDataFrame)\n\u001b[0;32m   1567\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, raw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, result_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, args\u001b[38;5;241m=\u001b[39m(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m-> 1568\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m   1569\u001b[0m         func, axis\u001b[38;5;241m=\u001b[39maxis, raw\u001b[38;5;241m=\u001b[39mraw, result_type\u001b[38;5;241m=\u001b[39mresult_type, args\u001b[38;5;241m=\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   1570\u001b[0m     )\n\u001b[0;32m   1571\u001b[0m     \u001b[38;5;66;03m# pandas <1.4 re-attach last geometry col if lost\u001b[39;00m\n\u001b[0;32m   1572\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1573\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m compat\u001b[38;5;241m.\u001b[39mPANDAS_GE_14\n\u001b[0;32m   1574\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, GeoDataFrame)\n\u001b[0;32m   1575\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m result\u001b[38;5;241m.\u001b[39m_geometry_column_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1576\u001b[0m     ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\masters\\Lib\\site-packages\\pandas\\core\\frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m  10373\u001b[0m )\n\u001b[1;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mapply()\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\masters\\Lib\\site-packages\\pandas\\core\\apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[1;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\masters\\Lib\\site-packages\\pandas\\core\\apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_generator()\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\masters\\Lib\\site-packages\\pandas\\core\\apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(v, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[6], line 68\u001b[0m, in \u001b[0;36madd_nearest.<locals>.<lambda>\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_nearest\u001b[39m(gdf1, gdf2):\n\u001b[1;32m---> 68\u001b[0m     nearest_neighbors \u001b[38;5;241m=\u001b[39m gdf1\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: find_nearest(row, gdf2), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# Add nearest neighbor information to the first GeoDataFrame\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     gdf1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnearest_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m nearest_neighbors[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstation_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[1;32mIn[6], line 65\u001b[0m, in \u001b[0;36mfind_nearest\u001b[1;34m(row, other_gdf)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_nearest\u001b[39m(row, other_gdf):\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Find the index of the nearest geometry\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m     nearest_idx \u001b[38;5;241m=\u001b[39m other_gdf\u001b[38;5;241m.\u001b[39mdistance(row\u001b[38;5;241m.\u001b[39mgeometry)\u001b[38;5;241m.\u001b[39midxmin()\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m other_gdf\u001b[38;5;241m.\u001b[39mloc[nearest_idx]\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\masters\\Lib\\site-packages\\geopandas\\base.py:2577\u001b[0m, in \u001b[0;36mGeoPandasBase.distance\u001b[1;34m(self, other, align)\u001b[0m\n\u001b[0;32m   2483\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdistance\u001b[39m(\u001b[38;5;28mself\u001b[39m, other, align\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   2484\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a ``Series`` containing the distance to aligned `other`.\u001b[39;00m\n\u001b[0;32m   2485\u001b[0m \n\u001b[0;32m   2486\u001b[0m \u001b[38;5;124;03m    The operation works on a 1-to-1 row-wise manner:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2575\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   2576\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _binary_op(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistance\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, other, align)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\masters\\Lib\\site-packages\\geopandas\\base.py:63\u001b[0m, in \u001b[0;36m_binary_op\u001b[1;34m(op, this, other, align, *args, **kwargs)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_binary_op\u001b[39m(op, this, other, align, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# type: (str, GeoSeries, GeoSeries, args/kwargs) -> Series[bool/float]\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Binary operation on GeoSeries objects that returns a Series\"\"\"\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m     data, index \u001b[38;5;241m=\u001b[39m _delegate_binary_method(op, this, other, align, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Series(data, index\u001b[38;5;241m=\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\masters\\Lib\\site-packages\\geopandas\\base.py:47\u001b[0m, in \u001b[0;36m_delegate_binary_method\u001b[1;34m(op, this, other, align, *args, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m(this), \u001b[38;5;28mtype\u001b[39m(other))\n\u001b[1;32m---> 47\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(a_this, op)(other, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data, this\u001b[38;5;241m.\u001b[39mindex\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\masters\\Lib\\site-packages\\geopandas\\array.py:693\u001b[0m, in \u001b[0;36mGeometryArray.distance\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdistance\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_geographic_crs(stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m)\n\u001b[1;32m--> 693\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_binary_method(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistance\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, other)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\masters\\Lib\\site-packages\\geopandas\\array.py:611\u001b[0m, in \u001b[0;36mGeometryArray._binary_method\u001b[1;34m(op, left, right, **kwargs)\u001b[0m\n\u001b[0;32m    608\u001b[0m         _crs_mismatch_warn(left, right, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m)\n\u001b[0;32m    609\u001b[0m     right \u001b[38;5;241m=\u001b[39m right\u001b[38;5;241m.\u001b[39m_data\n\u001b[1;32m--> 611\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(vectorized, op)(left\u001b[38;5;241m.\u001b[39m_data, right, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\masters\\Lib\\site-packages\\geopandas\\_vectorized.py:982\u001b[0m, in \u001b[0;36mdistance\u001b[1;34m(data, other)\u001b[0m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdistance\u001b[39m(data, other):\n\u001b[0;32m    981\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compat\u001b[38;5;241m.\u001b[39mUSE_SHAPELY_20:\n\u001b[1;32m--> 982\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m shapely\u001b[38;5;241m.\u001b[39mdistance(data, other)\n\u001b[0;32m    983\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m compat\u001b[38;5;241m.\u001b[39mUSE_PYGEOS:\n\u001b[0;32m    984\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _binary_method(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistance\u001b[39m\u001b[38;5;124m\"\u001b[39m, data, other)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\masters\\Lib\\site-packages\\shapely\\decorators.py:77\u001b[0m, in \u001b[0;36mmultithreading_enabled.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m array_args:\n\u001b[0;32m     76\u001b[0m         arr\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m arr, old_flag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(array_args, old_flags):\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\masters\\Lib\\site-packages\\shapely\\measurement.py:74\u001b[0m, in \u001b[0;36mdistance\u001b[1;34m(a, b, **kwargs)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;129m@multithreading_enabled\u001b[39m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdistance\u001b[39m(a, b, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     50\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Computes the Cartesian distance between two geometries.\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m    nan\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mdistance(a, b, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Calcium\n",
    "training_ca = get_water_quality(\"Calcium\", state_fip=training_fip, stations_gdf=training_stations_gdf, my_path=training_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f991fd1a-3fdb-46e1-917e-3a6be419f53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pH\n",
    "training_pH= get_water_quality(\"pH\", state_fip=training_fip, stations_gdf=training_stations_gdf, my_path=training_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fa2053-0ad3-41be-9496-f6d05b036e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dissolved Oxygen\n",
    "training_DO = get_water_quality('Oxygen', state_fip=training_fip, stations_gdf=training_stations_gdf, my_path=training_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e549808a-df07-4d91-8210-874023bf2b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nitrogen\n",
    "training_N = get_water_quality(\"Nitrogen\", state_fip=training_fip, stations_gdf=training_stations_gdf, my_path=training_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60b64d5-1d3b-419b-8017-5f8501b7ffff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phosphorus\n",
    "training_Phos = get_water_quality(\"Phosphorus\", state_fip=training_fip, stations_gdf=training_stations_gdf, my_path=training_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea2a8304-f4bf-49d8-9151-9016b33782ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Downloading Salinity data from WQP...\n",
      "Download failed (attempt 1/3): (\"Connection broken: InvalidChunkLength(got length b'', 0 bytes read)\", InvalidChunkLength(got length b'', 0 bytes read))\n",
      "Retrying in 5 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leif.howard\\AppData\\Local\\Temp\\ipykernel_18108\\1878486312.py:139: DtypeWarning: Columns (9,10,11,14,16,18,20,30,35,37,41,44,47,58,60) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_buffer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Retrieved 59959 records\n",
      "üîÑ Imputing missing Salinity values...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leif.howard\\AppData\\Local\\anaconda3\\envs\\masters\\Lib\\site-packages\\geopandas\\geodataframe.py:1525: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n",
      "C:\\Users\\leif.howard\\AppData\\Local\\anaconda3\\envs\\masters\\Lib\\site-packages\\geopandas\\geodataframe.py:1525: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Salinity data saved to: my_data/MN/usgs_salinity.shp\n"
     ]
    }
   ],
   "source": [
    "# Salinity\n",
    "training_Salinity = get_water_quality(\"Salinity\", state_fip=training_fip, stations_gdf=training_stations_gdf, my_path=training_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7d0682e-0d92-4867-89bc-eb919b22946b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Downloading Temperature, water data from WQP...\n",
      "Download failed (attempt 1/3): 400 Client Error: Bad Request for url: https://www.waterqualitydata.us/data/Result/search?countrycode=US&statecode=US:27&characteristicName=Temperature%2C%20water&startDateLo=2000-01-01&mimeType=csv\n",
      "Retrying in 5 seconds...\n",
      "Download failed (attempt 2/3): 400 Client Error: Bad Request for url: https://www.waterqualitydata.us/data/Result/search?countrycode=US&statecode=US:27&characteristicName=Temperature%2C%20water&startDateLo=2000-01-01&mimeType=csv\n",
      "Retrying in 5 seconds...\n",
      "Download failed (attempt 3/3): 400 Client Error: Bad Request for url: https://www.waterqualitydata.us/data/Result/search?countrycode=US&statecode=US:27&characteristicName=Temperature%2C%20water&startDateLo=2000-01-01&mimeType=csv\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "400 Client Error: Bad Request for url: https://www.waterqualitydata.us/data/Result/search?countrycode=US&statecode=US:27&characteristicName=Temperature%2C%20water&startDateLo=2000-01-01&mimeType=csv",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Water Temperature\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m training_Temp \u001b[38;5;241m=\u001b[39m get_water_quality(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTemperature\u001b[39m\u001b[38;5;124m'\u001b[39m, state_fip\u001b[38;5;241m=\u001b[39mtraining_fip, stations_gdf\u001b[38;5;241m=\u001b[39mtraining_stations_gdf, my_path\u001b[38;5;241m=\u001b[39mtraining_path)\n",
      "Cell \u001b[1;32mIn[17], line 154\u001b[0m, in \u001b[0;36mget_water_quality\u001b[1;34m(characteristic, state_fip, stations_gdf, my_path, start_date)\u001b[0m\n\u001b[0;32m    147\u001b[0m request_url \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl_base\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcountrycode=US&statecode=US:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate_fip\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&characteristicName=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchar_encoded\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m&startDateLo=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&mimeType=csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    151\u001b[0m )\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müì• Downloading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcharacteristic_wqp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m data from WQP...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 154\u001b[0m csv_buffer \u001b[38;5;241m=\u001b[39m download_wqp_csv(request_url)\n\u001b[0;32m    155\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(csv_buffer)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Retrieved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m records\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[17], line 105\u001b[0m, in \u001b[0;36mdownload_wqp_csv\u001b[1;34m(url, max_retries, backoff)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mget(url, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m r:\n\u001b[1;32m--> 105\u001b[0m         r\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m    106\u001b[0m         content \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mcontent  \u001b[38;5;66;03m# read all bytes\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m io\u001b[38;5;241m.\u001b[39mBytesIO(content)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\masters\\Lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1016\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1018\u001b[0m     )\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://www.waterqualitydata.us/data/Result/search?countrycode=US&statecode=US:27&characteristicName=Temperature%2C%20water&startDateLo=2000-01-01&mimeType=csv"
     ]
    }
   ],
   "source": [
    "# Water Temperature\n",
    "training_Temp = get_water_quality('Temperature', state_fip=training_fip, stations_gdf=training_stations_gdf, my_path=training_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d10750f-473e-4d16-9335-13702b340562",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate(training_ca, training_path + training_state_abbr + '_ca.tif', 'Ca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f62f2b-8f7a-4595-b90a-9b9c39cc5331",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate(training_pH, training_path + training_state_abbr + '_pH.tif', 'pH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ebd38d-ab62-4dfd-b26d-013b984c8324",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate(training_N, training_path + training_state_abbr + '_N.tif', 'Nitrogen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6833a24d-7038-4b02-9d3b-94f40d9b7771",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate(training_DO, training_path + training_state_abbr + '_do.tif', 'DO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cce07c-fcda-4652-befb-ad4f1cd729a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate(training_Phos, training_path + training_state_abbr + '_phos.tif', 'Phos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5219e3-db6c-49a2-95ed-b25ac40fd1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate(training_Salinity, training_path + training_state_abbr + '_salinity.tif', 'Salinity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59f4d9e-fa54-4abd-97be-4952c6b4ef46",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate(training_Temp, training_path + training_state_abbr + '_temp.tif', 'Temperature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d5fd19-d150-4034-9825-641a393420f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the predictors you just created and export a final combined raster \n",
    "# Define input files\n",
    "input_files_wq_training = [\n",
    "    training_path + training_state_abbr + \"_ca.tif\", training_path + training_state_abbr + \"_pH.tif\", training_path + training_state_abbr + \"_N.tif\",\n",
    "    training_path + training_state_abbr + \"_do.tif\", training_path + training_state_abbr + \"_phos.tif\"]\n",
    "#Combine rasters into a multi-band GeoTIFF\n",
    "output_file_wq_training = training_path + training_state_abbr + \"_combined_wq.tif\" # this will replace the other combined tif you made.\n",
    "combine_geotiffs(input_files_wq_training, output_file_wq_training)\n",
    "\n",
    "print(f\"‚úÖ Multi-band raster saved as {output_file_wq_training}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa11a9b7-cb73-4d86-a2da-a4ac7938067f",
   "metadata": {},
   "source": [
    "# Create Minnesota specific predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c13a2da-3989-412c-a3a8-fc4948c84adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def TSI_api(my_wid_list, my_format):\n",
    "    responses = []\n",
    "    URL_BASE = 'https://services.pca.state.mn.us/api/v1/'\n",
    "    for wid in my_wid_list:\n",
    "            url_request = f\"{URL_BASE}surfacewater/water-units/trophic-statuses?wid={wid}&format={my_format}\"  \n",
    "            response = requests.get(url_request, timeout=None)\n",
    "            responses.append(response.json())\n",
    "            return responses\n",
    "\n",
    "def download_ramps(local_path):\n",
    "        url = \"https://www.sciencebase.gov/catalog/file/get/63b81b50d34e92aad3cc004d?facet=Boatramps_United_States_final_20230104\"\n",
    "        extensions = {'.dbf', '.prj', '.shp', '.shx'}\n",
    "        response = requests.get(url, stream=True, timeout=60)\n",
    "        response.raise_for_status()\n",
    "        with zipfile.ZipFile(BytesIO(response.content)) as zip_ref:\n",
    "            os.makedirs(local_path, exist_ok=True)\n",
    "            for f in zip_ref.namelist():\n",
    "                if os.path.splitext(f)[1].lower() in extensions:\n",
    "                    zip_ref.extract(f, local_path)\n",
    "\n",
    "def pick_awetype(values):\n",
    "    values = list(pd.Series(values).dropna().unique())\n",
    "    if not values:\n",
    "        return None\n",
    "    if 1 in values: return 1\n",
    "    if 3 in values: return 3\n",
    "    if 2 in values: return 2\n",
    "    return values[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gdf_to_multiband_tif(gdf, columns, output_path, pixel_size=100, crs_epsg=5070):\n",
    "    \"\"\"\n",
    "    Rasterize selected columns of a GeoDataFrame into a single multi-band GeoTIFF.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    gdf : GeoDataFrame\n",
    "        Must contain geometry and numeric columns to rasterize.\n",
    "    columns : list of str\n",
    "        Column names to rasterize into separate bands.\n",
    "    output_path : str\n",
    "        Path to save the GeoTIFF.\n",
    "    pixel_size : float\n",
    "        Output raster resolution in CRS units (default 100 for UTM meters).\n",
    "    crs_epsg : int\n",
    "        EPSG code for raster CRS (default 26915 for MN).\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure numeric columns (replace NaN with 0 so rasterize doesn't crash)\n",
    "    gdf = gdf.copy()\n",
    "    for col in columns:\n",
    "        gdf[col] = gdf[col].fillna(0).astype(float)\n",
    "\n",
    "    # Reproject to target CRS\n",
    "    gdf = gdf.to_crs(epsg=crs_epsg)\n",
    "\n",
    "    # Get bounds and create transform\n",
    "    minx, miny, maxx, maxy = gdf.total_bounds\n",
    "    width = int(np.ceil((maxx - minx) / pixel_size))\n",
    "    height = int(np.ceil((maxy - miny) / pixel_size))\n",
    "    transform = from_origin(minx, maxy, pixel_size, pixel_size)\n",
    "\n",
    "    # Prepare bands\n",
    "    bands = []\n",
    "    for col in columns:\n",
    "        shapes = ((geom, value) for geom, value in zip(gdf.geometry, gdf[col]))\n",
    "        band_array = rasterize(\n",
    "            shapes=shapes,\n",
    "            out_shape=(height, width),\n",
    "            transform=transform,\n",
    "            fill=0,\n",
    "            dtype=\"float32\"\n",
    "        )\n",
    "        bands.append(band_array)\n",
    "\n",
    "    # Stack into (bands, height, width)\n",
    "    bands_stack = np.stack(bands, axis=0)\n",
    "\n",
    "    # Write to multi-band GeoTIFF\n",
    "    with rasterio.open(\n",
    "        output_path,\n",
    "        'w',\n",
    "        driver='GTiff',\n",
    "        height=height,\n",
    "        width=width,\n",
    "        count=len(columns),\n",
    "        dtype='float32',\n",
    "        crs=f\"EPSG:{crs_epsg}\",\n",
    "        transform=transform\n",
    "    ) as dst:\n",
    "        for i in range(len(columns)):\n",
    "            dst.write(bands_stack[i], i + 1)\n",
    "\n",
    "    print(f\"‚úÖ Multi-band raster saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b769aa-9b60-427b-864a-3b8372bbde5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Minnesota waterbody data\n",
    "MN_water_url = \"https://resources.gisdata.mn.gov/pub/gdrs/data/pub/us_mn_state_dnr/water_dnr_hydrography/shp_water_dnr_hydrography.zip\"\n",
    "# Send a GET request to fetch the ZIP file\n",
    "MN_water_response = requests.get(MN_water_url)\n",
    "MN_water_response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "# Open the ZIP file from the response content\n",
    "with zipfile.ZipFile(io.BytesIO(MN_water_response.content)) as zip_ref:\n",
    "    # Extract all contents into the current directory\n",
    "    zip_ref.extractall(training_path)\n",
    "\n",
    "print(\"mn_dnr_hydrography Shapefile extracted to directory.\")\n",
    "\n",
    "# Download Watercourse alteration and use risk lakes to create rasters \n",
    "watercourse_alteration_url= 'https://resources.gisdata.mn.gov/pub/gdrs/data/pub/us_mn_state_pca/water_altered_watercourse/shp_water_altered_watercourse.zip'\n",
    "# Send a GET request to fetch the ZIP file\n",
    "watercourse_alteration_response = requests.get(watercourse_alteration_url)\n",
    "watercourse_alteration_response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "\n",
    "# Open the ZIP file from the response content\n",
    "with zipfile.ZipFile(io.BytesIO(watercourse_alteration_response.content)) as zip_ref:\n",
    "    # Extract all contents into the current directory\n",
    "    zip_ref.extractall(training_path)\n",
    "\n",
    "print(\"watercourse alteration Shapefile extracted to directory.\")\n",
    "\n",
    "sf_water_station_url = \"https://resources.gisdata.mn.gov/pub/gdrs/data/pub/us_mn_state_pca/env_eda_surfacewater_stations/shp_env_eda_surfacewater_stations.zip\"\n",
    "# Send a GET request to fetch the ZIP file\n",
    "sf_water_station_response = requests.get(sf_water_station_url)\n",
    "sf_water_station_response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "\n",
    "# Open the ZIP file from the response content\n",
    "with zipfile.ZipFile(io.BytesIO(sf_water_station_response.content)) as zip_ref:\n",
    "    # Extract all contents into the current directory\n",
    "    zip_ref.extractall(training_path)\n",
    "\n",
    "print(\"surface water stations Shapefile extracted to directory.\")\n",
    "\n",
    "#Spatially join ramps to risk lakes \n",
    "\n",
    "download_ramps(training_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13cb9a35-e950-44cd-8054-d5e6b67d9239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the lake shapefile you just downloaded (It takes a minute)\n",
    "MNDNR_lakes = gpd.read_file(training_path + 'dnr_hydro_features_all.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae54f5b3-4ac1-4cbd-8cbe-344b4fc3d2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_gdf = MNDNR_lakes.dropna(subset=['dowlknum']).to_crs(my_crs)\n",
    "lakes_gdf['dowlknum'] = lakes_gdf['dowlknum'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73c916e2-076c-4c03-8fcf-23cb71d44626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leif.howard\\AppData\\Local\\Temp\\ipykernel_12500\\2898377978.py:15: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  lakes_with_risk.to_file(training_path + \"lakes_risk.shp\")\n",
      "WARNING:Normalized/laundered field name: 'Boater_movement' to 'Boater_mov'\n",
      "WARNING:Normalized/laundered field name: 'Water_connectivity' to 'Water_conn'\n"
     ]
    }
   ],
   "source": [
    "# You need to go to https://www.aisexplorer.umn.edu/ and download this data and save the csv in your training path directory\n",
    "risk_df = pd.read_csv(training_path + 'All_risk_table.csv')  # Adjust path and filename accordingly\n",
    "# Example: DOW field may be 'DOWLKNUM' or similar\n",
    "dow_field = 'dowlknum'  # change if needed\n",
    "# Rename risk_df DOW column to match shapefile for join\n",
    "risk_df = risk_df.rename(columns={\"DOW number\": dow_field, 'Boater movement risk score': 'Boater_movement', 'Water connectivity risk score': 'Water_connectivity'})\n",
    "# 3) Merge lakes and risk data on DOW number\n",
    "lakes_risk = lakes_gdf.merge(risk_df[[dow_field, 'Boater_movement', 'Water_connectivity']],\n",
    "                         on=dow_field,\n",
    "                         how=\"inner\")\n",
    "\n",
    "# 4) Project to a metric CRS (e.g., NAD83 UTM Zone 15N EPSG:26915) for 100m pixels\n",
    "lakes_with_risk = lakes_risk[['dowlknum', 'geometry',\n",
    "       'Boater_movement', 'Water_connectivity']].to_crs(my_crs)\n",
    "lakes_with_risk.to_file(training_path + \"lakes_risk.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cac0a6-89b2-4521-bbdb-76db454774a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "AWEvtType_gdf =  gpd.read_file(training_path + 'Altered_Watercourse.shp').to_crs(my_crs)# has AWEvtType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e56e244-5bac-4628-9c73-ae0342bb4dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-pass nearest join (distance=0 means intersect)\n",
    "nearest_all = gpd.sjoin_nearest(\n",
    "    lakes_with_risk,\n",
    "    AWEvtType_gdf[[\"AWEvtType\", \"geometry\"]],\n",
    "    how=\"left\",\n",
    "    distance_col=\"nearest_dist\",\n",
    "    max_distance=2000  # limit in meters\n",
    ")\n",
    "\n",
    "# Apply priority logic in one groupby\n",
    "final_df = (\n",
    "    nearest_all.groupby(\"dowlknum\", as_index=False)\n",
    "    .agg({\"AWEvtType\": pick_awetype})\n",
    ")\n",
    "\n",
    "# This is now a plain DataFrame\n",
    "AWEvtType_for_merge = final_df[[\"dowlknum\", \"AWEvtType\"]]\n",
    "AWEvtType_for_merge[\"AWEvtType\"] = AWEvtType_for_merge[\"AWEvtType\"].fillna(2)\n",
    "AWEvtType_for_merge.to_csv(training_path + \"AWEvtType.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b447ed40-cd4d-4849-8a8a-a3dd88f7a051",
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_gdf = gpd.read_file(training_path + \"eda_surfacewater_stations.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "28dea87c-0ad0-47ee-9128-4446c636cb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wid = eda_gdf['wid'].unique()\n",
    "batch_list = []\n",
    "return_list = []\n",
    "for i in wid:\n",
    "    batch_list.append(i)\n",
    "    if len(batch_list)  == 100:\n",
    "        return_list.append(TSI_api(batch_list, 'json'))\n",
    "        batch_list.clear()\n",
    "\n",
    "if batch_list:\n",
    "    return_list.append(TSI_api(batch_list,  'json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8f284140-fad1-4eb1-9c5c-b4a913dc60c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten all 'data' lists from each API result\n",
    "all_data = []\n",
    "for api_result_group in return_list:   # outer list\n",
    "    for api_result in api_result_group:  # inner list of dicts\n",
    "        if api_result.get(\"data\"):  # only if not empty\n",
    "            all_data.extend(api_result[\"data\"])\n",
    "\n",
    "# Create DataFrame if there's data\n",
    "if all_data:\n",
    "    tsi_df = pd.DataFrame(all_data)\n",
    "    # Keep only wid + tpTsi, ensure numeric\n",
    "    tsi_df = tsi_df[['wid', 'tpTsi']].copy()\n",
    "    tsi_df['tpTsi'] = pd.to_numeric(tsi_df['tpTsi'], errors='coerce')\n",
    "    tsi_df = tsi_df.dropna(subset=[\"tpTsi\"]).reset_index(drop=True)\n",
    "else:\n",
    "    tsi_df = pd.DataFrame(columns=['wid', 'tpTsi'])\n",
    "    tsi_df = tsi_df.dropna(subset=[\"tpTsi\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "572b8871-ba74-46c0-bac7-a84c1e279ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ensure wid has matching types ---\n",
    "eda_gdf[\"wid\"] = eda_gdf[\"wid\"].astype(str)\n",
    "tsi_df[\"wid\"] = tsi_df[\"wid\"].astype(str)\n",
    "# --- Merge on wid ---\n",
    "eda_w_tsi = eda_gdf.merge(tsi_df, on=\"wid\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "77f8fc53-4ff6-4a20-94b8-e5f4a8c47f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsi_df_clean = eda_w_tsi.dropna(subset=[\"tpTsi\"]).reset_index(drop=True)\n",
    "# Ensure both GeoDataFrames are in the same CRS\n",
    "if tsi_df_clean.crs != lakes_gdf.crs:\n",
    "    tsi_df_clean = tsi_df_clean.to_crs(lakes_gdf.crs)\n",
    "\n",
    "# Nearest spatial join: each lake gets nearest station's tpTSI\n",
    "lakes_with_tsi = gpd.sjoin_nearest(\n",
    "    lakes_gdf, \n",
    "    tsi_df_clean,\n",
    "    how=\"left\",\n",
    "    distance_col=\"nearest_dist\"\n",
    ")\n",
    "tsi_for_merge = lakes_with_tsi[[\"dowlknum\", \"tpTsi\"]]\n",
    "tsi_for_merge.to_csv(training_path + \"tsi.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b65aa67e-a5b1-44e7-a41c-f52165dedc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ramps = gpd.read_file(training_path + 'Boatramps_United_States_final_20230104.shp').set_crs(my_crs, allow_override=True)\n",
    "ramp_geo = ramps.loc[ramps['State'] == training_state_abbr, ['geometry']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a41c2301-5146-4abe-ad1a-2327f760aaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ramps_gdf = ramp_geo.to_crs(my_crs)\n",
    "# Spatial join to get each ramp assigned to a lake polygon\n",
    "ramps_with_lake = gpd.sjoin(ramps_gdf, lakes_gdf, how=\"inner\", predicate=\"within\")\n",
    "# Count ramps per lake\n",
    "ramps_count = ramps_with_lake.groupby(\"dowlknum\").size().reset_index(name=\"n_boat_ramps\")\n",
    "\n",
    "# Merge back to lakes\n",
    "lakes_gdf[\"dowlknum\"] = lakes_gdf[\"dowlknum\"].astype(int)\n",
    "ramps_count[\"dowlknum\"] = ramps_count[\"dowlknum\"].astype(int)\n",
    "\n",
    "ramps_gdf = lakes_gdf.merge(ramps_count, on=\"dowlknum\", how=\"left\")\n",
    "\n",
    "# Fill NaN for lakes with no ramps\n",
    "ramps_gdf[\"n_boat_ramps\"] = ramps_gdf[\"n_boat_ramps\"].fillna(0).astype(int)\n",
    "ramps_for_merge = ramps_gdf[[\"dowlknum\", \"n_boat_ramps\"]]\n",
    "ramps_for_merge.to_csv(training_path + \"boat_ramps.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ee8c169-d143-4c62-bbac-4b7ee2562bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get records from database API\n",
    "def api_lakefinder(lake_id):\n",
    "    URL_BASE_lakefinder = 'https://maps.dnr.state.mn.us/cgi-bin/lakefinder/detail.cgi'\n",
    "    url_request_lakefinder = f\"{URL_BASE_lakefinder}?id={lake_id}&type=lake_survey\"\n",
    "    response_lakefinder = requests.get(url_request_lakefinder, timeout=30)\n",
    "    return response_lakefinder.json()  # no need for a list here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "029a8ae6-1604-4abb-ae79-afb30efa148c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MN native diversity predictors\n",
    "# Load your lake IDs\n",
    "my_lakes = lakes_with_risk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a785e9d7-1520-42c0-8fd3-ba384b25897b",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_codes = my_lakes['dowlknum'].astype(str)\n",
    "dow_list = id_codes.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e69ccec-7933-4b37-b0ec-be93c2a592b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching lake 56051000: Expecting value: line 1 column 1 (char 0)\n"
     ]
    }
   ],
   "source": [
    "api_result = []\n",
    "batch_size = 100\n",
    "for i in range(0, len(dow_list), batch_size):\n",
    "    batch = dow_list[i:i + batch_size]\n",
    "    for lake_id in batch:\n",
    "        try:\n",
    "            data = api_lakefinder(lake_id)\n",
    "            if data:\n",
    "                api_result.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching lake {lake_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3c62620-b795-450d-9d4c-f0a9d1687b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_success_entries(api_data):\n",
    "    \"\"\"\n",
    "    Extracts all dictionary entries with status == 'SUCCESS' from nested API data.\n",
    "    \n",
    "    Handles mixed nesting of lists/dicts in api_data.\n",
    "    \"\"\"\n",
    "    success_entries = []\n",
    "\n",
    "    def extract_from(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            if obj.get('status') == 'SUCCESS':\n",
    "                success_entries.append(obj)\n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                extract_from(item)\n",
    "\n",
    "    extract_from(api_data)\n",
    "    return success_entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f764bbec-4ad4-4158-8bfd-2635c82c1ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_api_data = get_success_entries(api_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d2ede157-c9d9-4117-b5a7-96cdffe150f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract DOW, survey date and cpue by species and return seperate records for each survey\n",
    "def extract_fish_data(data):\n",
    "    results = []\n",
    "    \n",
    "    # Loop through the data\n",
    "    for lake in data:\n",
    "        dow_number = lake['result']['DOWNumber']\n",
    "        \n",
    "        for survey in lake['result']['surveys']:\n",
    "            survey_date = survey['surveyDate']\n",
    "            \n",
    "            # Initialize total fish count and CPUE dictionary\n",
    "            total_fish_count = 0\n",
    "            cpue_by_species = {}\n",
    "            \n",
    "            # Extract fish catch summaries\n",
    "            for fish in survey['fishCatchSummaries']:\n",
    "                species = fish['species']\n",
    "                total_catch = fish['totalCatch']\n",
    "                cpue = fish['CPUE']\n",
    "                \n",
    "                # Add to the total fish count\n",
    "                total_fish_count += total_catch\n",
    "                \n",
    "                # Store CPUE by species\n",
    "                cpue_by_species[species] = cpue\n",
    "            \n",
    "            # Append the result to the list\n",
    "            results.append({\n",
    "                'DOWNumber': dow_number,\n",
    "                'surveyDate': survey_date,\n",
    "                'totalFishCount': total_fish_count,\n",
    "                'CPUEBySpecies': cpue_by_species\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "from datetime import datetime\n",
    "# Function to filter records with surveyDate > 2002\n",
    "def filter_records_by_date(data):\n",
    "    filtered_data = []\n",
    "    for record in data:\n",
    "        survey_date = datetime.strptime(record['surveyDate'], '%Y-%m-%d')\n",
    "        if survey_date.year > 2002:\n",
    "            filtered_data.append(record)\n",
    "    return filtered_data\n",
    "\n",
    "from collections import defaultdict\n",
    "# Function to check if all values in CPUEBySpecies can be converted to float\n",
    "def can_convert_to_float(cpue_by_species):\n",
    "    try:\n",
    "        return all(float(value) for value in cpue_by_species.values())\n",
    "    except (ValueError, TypeError):\n",
    "        return False\n",
    "\n",
    "# Function to remove or fix invalid CPUE values before calculations\n",
    "def remove_invalid_cpue(data_list):\n",
    "    valid_data = []\n",
    "    for item in data_list:\n",
    "        cleaned_cpue = {}\n",
    "        for species, cpue in item['CPUEBySpecies'].items():\n",
    "            try:\n",
    "                cleaned_cpue[species] = float(cpue)\n",
    "            except (ValueError, TypeError):\n",
    "                # skip values that can't be converted to float (e.g. '?', '‚àû', None)\n",
    "                continue\n",
    "        item['CPUEBySpecies'] = cleaned_cpue\n",
    "        valid_data.append(item)\n",
    "    return valid_data\n",
    "\n",
    "# Function to calculate mean CPUEBySpecies for each DOWNumber\n",
    "def calculate_mean_cpue(data):\n",
    "    species_totals = defaultdict(lambda: defaultdict(float))\n",
    "    species_counts = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    for record in data:\n",
    "        dow = record['DOWNumber']\n",
    "        for species, cpue in record['CPUEBySpecies'].items():\n",
    "            try:\n",
    "                cpue_value = float(cpue)\n",
    "                species_totals[dow][species] += cpue_value\n",
    "                species_counts[dow][species] += 1\n",
    "            except (ValueError, TypeError):\n",
    "                # If a bad value slips through, just skip it\n",
    "                continue\n",
    "\n",
    "    species_means = {}\n",
    "    for dow, species_data in species_totals.items():\n",
    "        species_means[dow] = {}\n",
    "        for species, total_cpue in species_data.items():\n",
    "            count = species_counts[dow][species]\n",
    "            species_means[dow][species] = total_cpue / count if count > 0 else 0\n",
    "    \n",
    "    return species_means\n",
    "def shannon_wiener_index(row):\n",
    "    # Remove NaN values from the row\n",
    "    row = row.dropna()\n",
    "    \n",
    "    # Total abundance in the row (sum of all species abundances)\n",
    "    total_abundance = row.sum()\n",
    "    \n",
    "    # If total abundance is zero, return 0 (no diversity)\n",
    "    if total_abundance == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Proportions of each species\n",
    "    proportions = row / total_abundance\n",
    "    \n",
    "    # Calculate the Shannon-Wiener index\n",
    "    shannon_index = -np.sum(proportions * np.log(proportions))\n",
    "    \n",
    "    return shannon_index\n",
    "\n",
    "def calculate_shannon_index(df):\n",
    "    # Apply the Shannon-Wiener index function to each row (excluding the first column)\n",
    "    shannon_indices = df.iloc[:, 1:].apply(shannon_wiener_index, axis=1)\n",
    "    \n",
    "    # Create a new DataFrame with the row identifier and Shannon index\n",
    "    result_df = pd.DataFrame({\n",
    "        'DOWNumber': df.iloc[:, 0],\n",
    "        'Shannon_Index': shannon_indices\n",
    "    })\n",
    "    \n",
    "    return result_df\n",
    "def calculate_richness(df):\n",
    "    # Count the number of non-NaN cells in each row, excluding the first column (identifier)\n",
    "    richness = df.iloc[:, 1:].notna().sum(axis=1)\n",
    "    \n",
    "    # Create a new DataFrame with the row identifier and richness\n",
    "    result_df = pd.DataFrame({\n",
    "        'DOWNumber': df.iloc[:, 0],\n",
    "        'Richness': richness\n",
    "    })\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2afb20a0-9a94-47e5-bb90-83602c878717",
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_data = extract_fish_data(fish_api_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "69861738-9738-45dd-8d9c-2a92f255f729",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_fish_data = filter_records_by_date(fish_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a30c5648-5e08-4032-b76d-3bc31b1081cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_fish_data = remove_invalid_cpue(filtered_fish_data)\n",
    "mean_cpue_by_species = calculate_mean_cpue(cleaned_fish_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0737f33b-5728-4a37-8555-9636e9ac3fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpue_df = pd.DataFrame.from_dict(mean_cpue_by_species, orient='index').reset_index()\n",
    "fish_result_df = cpue_df.rename(columns={'index': 'DOWNumber'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c996a526-11d7-4ba1-9d61-4ec153052c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_result_df.to_csv(training_path + 'fish_survey_all_lakes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b17691dc-8424-4f3c-8a29-62129678a337",
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_result_df = pd.read_csv(training_path +\"fish_survey_all_lakes.csv\").drop(columns=[\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1c1fcfef-45d0-432f-b081-68978d820b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DOWNumber', 'BRB', 'YEP', 'YEB', 'NOP', 'LMB', 'WAE', 'HSF', 'BLG',\n",
       "       'PMK',\n",
       "       ...\n",
       "       'SKJ', 'WAM', 'PGM', 'GOF', 'BRH', 'LXB', 'MCP', 'CMS', 'SJC', 'FTD'],\n",
       "      dtype='object', length=136)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fish_result_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "21888e8c-6305-4ff0-bb0f-637d5d7c72fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leif.howard\\AppData\\Local\\anaconda3\\envs\\masters\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "my_wiener = calculate_shannon_index(fish_result_df).rename(columns={'DOWNumber': 'dowlknum', 'Shannon_Index': 'Shannon_Index_Native_Fish'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9ba60d24-b014-4b4d-81ab-d16c4aa6fa01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dowlknum</th>\n",
       "      <th>Shannon_Index_Native_Fish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69062700</td>\n",
       "      <td>2.137422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77000700</td>\n",
       "      <td>2.333884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11037100</td>\n",
       "      <td>2.521285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29015700</td>\n",
       "      <td>1.885471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61012800</td>\n",
       "      <td>1.602303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dowlknum  Shannon_Index_Native_Fish\n",
       "0  69062700                   2.137422\n",
       "1  77000700                   2.333884\n",
       "2  11037100                   2.521285\n",
       "3  29015700                   1.885471\n",
       "4  61012800                   1.602303"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_wiener.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a993fef8-0d32-458c-a348-a925fdc9dad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_richness = calculate_richness(fish_result_df).rename(columns={'DOWNumber': 'dowlknum', 'Richness': 'Richness_Native_Fish'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "da9bd8b8-2426-42da-b168-5cea349e9c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dowlknum</th>\n",
       "      <th>Richness_Native_Fish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69062700</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77000700</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11037100</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29015700</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61012800</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dowlknum  Richness_Native_Fish\n",
       "0  69062700                    16\n",
       "1  77000700                    25\n",
       "2  11037100                    23\n",
       "3  29015700                    19\n",
       "4  61012800                    12"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_richness.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6fcf38dd-2e79-4273-b360-624a5cdad593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dowlknum</th>\n",
       "      <th>Shannon_Index_gamefish</th>\n",
       "      <th>Richness_gamefish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69062700</td>\n",
       "      <td>1.202243</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77000700</td>\n",
       "      <td>1.010407</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11037100</td>\n",
       "      <td>1.448585</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29015700</td>\n",
       "      <td>0.426604</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61012800</td>\n",
       "      <td>0.084448</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dowlknum  Shannon_Index_gamefish  Richness_gamefish\n",
       "0  69062700                1.202243                  4\n",
       "1  77000700                1.010407                  5\n",
       "2  11037100                1.448585                  5\n",
       "3  29015700                0.426604                  5\n",
       "4  61012800                0.084448                  4"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamefish_comp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "538c09e7-43a2-4a8d-8414-861487dc1f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamefish = ['DOWNumber', 'LMB', 'SMB', 'CRP', 'SUN', 'MUE', 'NOP', 'WAE','YEP']\n",
    "gamefish_df = fish_result_df[gamefish]\n",
    "gamefish_wiener_df = calculate_shannon_index(gamefish_df).rename(columns={'Shannon_Index': 'Shannon_Index_gamefish'})\n",
    "gamefish_richness_df = calculate_richness(gamefish_df).rename(columns={'Richness': 'Richness_gamefish'})\n",
    "gamefish_comp = gamefish_wiener_df.merge(gamefish_richness_df, on = 'DOWNumber').rename(columns={'DOWNumber': 'dowlknum'})\n",
    "gamefish_comp.to_csv(training_path + 'gamefish_comp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "abb0da02-d346-4ad7-8cb1-d42dc57235db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leif.howard\\AppData\\Local\\anaconda3\\envs\\masters\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\leif.howard\\AppData\\Local\\anaconda3\\envs\\masters\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "CAT = ['DOWNumber', 'BLB', 'BRB', 'BLH', 'CCF', 'FCF', 'STC', 'TPM','YEB']\n",
    "CAT_df = fish_result_df[CAT]\n",
    "CAT_wiener_df = calculate_shannon_index(CAT_df).rename(columns={'Shannon_Index': 'Shannon_Index_CAT'})\n",
    "CAT_richness_df = calculate_richness(CAT_df).rename(columns={'Richness': 'Richness_CAT'})\n",
    "CENT = ['DOWNumber','BLC','BLG','CRP','SUN','GSF','HSF','LMB','OSS',\n",
    "'PMK','RKB','SMB','WAM','WHC']\n",
    "CENT_df = fish_result_df[CENT]\n",
    "CENT_wiener_df = calculate_shannon_index(CENT_df).rename(columns={'Shannon_Index': 'Shannon_Index_CENT'})\n",
    "CENT_richness_df = calculate_richness(CENT_df).rename(columns={'Richness': 'Richness_CENT'})\n",
    "DAR = ['DOWNumber','BSD','FTD','DAR','IOD','JND','LED',\n",
    "'MDD','DAR','RVD','SHD','WSD']\n",
    "DAR_df = fish_result_df[DAR]\n",
    "DAR_wiener_df = calculate_shannon_index(DAR_df).rename(columns={'Shannon_Index': 'Shannon_Index_DAR'})\n",
    "DAR_richness_df = calculate_richness(DAR_df).rename(columns={'Richness': 'Richness_DAR'})\n",
    "ESX = ['DOWNumber','MUE','NOP','TME']\n",
    "ESX_df = fish_result_df[ESX]\n",
    "ESX_wiener_df = calculate_shannon_index(ESX_df).rename(columns={'Shannon_Index': 'Shannon_Index_ESX'})\n",
    "ESX_richness_df = calculate_richness(ESX_df).rename(columns={'Richness': 'Richness_ESX'})\n",
    "OTM = ['DOWNumber','CRC','HHC','SLC','BKF','GOF','BND',\n",
    "'FND','LND','NRD','PRD','BNM','BRM','BKS',\n",
    "'BHM','CNM','CSR','OTM','FHM','PGM','QBS','BMS','BCS','BNS','CSH',\n",
    "'EMS','SHI','GOS','MMS','PGS','RVS','SDS','SFS','SPO','WDS']\n",
    "OTM_df = fish_result_df[OTM]\n",
    "OTM_wiener_df = calculate_shannon_index(OTM_df).rename(columns={'Shannon_Index': 'Shannon_Index_OTM'})\n",
    "OTM_richness_df = calculate_richness(OTM_df).rename(columns={'Richness': 'Richness_OTM'})\n",
    "PRCH = ['DOWNumber','SAR','WAE','WAS','YEP']\n",
    "PRCH_df = fish_result_df[PRCH]\n",
    "PRCH_wiener_df = calculate_shannon_index(PRCH_df).rename(columns={'Shannon_Index': 'Shannon_Index_PRCH'})\n",
    "PRCH_richness_df = calculate_richness(PRCH_df).rename(columns={'Richness': 'Richness_PRCH'})\n",
    "SAL_Other = ['DOWNumber','LKW','SJC','TLC']\n",
    "SAL_Other_df = fish_result_df[SAL_Other]\n",
    "SAL_Other_wiener_df = calculate_shannon_index(SAL_Other_df).rename(columns={'Shannon_Index': 'Shannon_Index_SAL_Other'})\n",
    "SAL_Other_richness_df = calculate_richness(SAL_Other_df).rename(columns={'Richness': 'Richness_SAL_Other'})\n",
    "SCUL = ['DOWNumber','DWS','SCU','MTS','SMS']\n",
    "SCUL_df = fish_result_df[SCUL]\n",
    "SCUL_wiener_df = calculate_shannon_index(SCUL_df).rename(columns={'Shannon_Index': 'Shannon_Index_SCUL'})\n",
    "SCUL_richness_df = calculate_richness(SCUL_df).rename(columns={'Richness': 'Richness_SCUL'})\n",
    "SUCK = ['DOWNumber','BIB','SAB','CAP','OTS','CPS','HFS','RCS','BLS',\n",
    "'WTS','RHS','GLR','GRR','RRH','SHR','SLR','LNS','NHS','SPS']\n",
    "SUCK_df = fish_result_df[SUCK]\n",
    "SUCK_wiener_df = calculate_shannon_index(SUCK_df).rename(columns={'Shannon_Index': 'Shannon_Index_SUCK'})\n",
    "SUCK_richness_df = calculate_richness(SUCK_df).rename(columns={'Richness': 'Richness_SUCK'})\n",
    "\n",
    "\n",
    "alpha_list = [CAT_wiener_df, CENT_wiener_df, DAR_wiener_df, ESX_wiener_df, OTM_wiener_df, PRCH_wiener_df,\n",
    "             SAL_Other_wiener_df, SCUL_wiener_df, SUCK_wiener_df]\n",
    "merged_alpha_df = reduce(lambda left, right: pd.merge(left, right, on='DOWNumber', how='outer'), alpha_list).rename(columns={'DOWNumber': 'dowlknum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6dce1999-4df9-4c7d-8324-fdb0662ce332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 8908 entries, 0 to 8907\n",
      "Data columns (total 4 columns):\n",
      " #   Column              Non-Null Count  Dtype   \n",
      "---  ------              --------------  -----   \n",
      " 0   dowlknum            8908 non-null   int64   \n",
      " 1   geometry            8908 non-null   geometry\n",
      " 2   Boater_movement     8908 non-null   float64 \n",
      " 3   Water_connectivity  8908 non-null   float64 \n",
      "dtypes: float64(2), geometry(1), int64(1)\n",
      "memory usage: 278.5 KB\n"
     ]
    }
   ],
   "source": [
    "lakes_with_risk.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3eaae188-51b6-4f05-90d1-c231a9d36aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "AWEvtType_for_merge = pd.read_csv(training_path + \"AWEvtType.csv\").drop(columns=[\"Unnamed: 0\"])\n",
    "tsi_for_merge = pd.read_csv(training_path + \"tsi.csv\").drop(columns=[\"Unnamed: 0\"])\n",
    "ramps_for_merge = pd.read_csv(training_path + \"boat_ramps.csv\").drop(columns=[\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b0531541-9ca8-4dd8-984e-3933c96e89ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = (\n",
    "    lakes_with_risk\n",
    "    .merge(tsi_for_merge, on=\"dowlknum\", how=\"left\")\n",
    "    .merge(ramps_for_merge, on=\"dowlknum\", how=\"left\")\n",
    "    .merge(AWEvtType_for_merge, on=\"dowlknum\", how=\"left\")\n",
    "    .merge(merged_alpha_df, on=\"dowlknum\", how=\"left\")\n",
    "    .merge(my_wiener, on=\"dowlknum\", how=\"left\")\n",
    "    .merge(my_richness, on=\"dowlknum\", how=\"left\")\n",
    "    .merge(gamefish_comp, on=\"dowlknum\", how=\"left\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c88db408-6b29-4ab8-b9d9-269482591a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_clean = merged_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a7fab501-4719-4e9c-adcc-5e860d194215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "Index: 2421 entries, 0 to 9492\n",
      "Data columns (total 20 columns):\n",
      " #   Column                     Non-Null Count  Dtype   \n",
      "---  ------                     --------------  -----   \n",
      " 0   dowlknum                   2421 non-null   int64   \n",
      " 1   geometry                   2421 non-null   geometry\n",
      " 2   Boater_movement            2421 non-null   float64 \n",
      " 3   Water_connectivity         2421 non-null   float64 \n",
      " 4   tpTsi                      2421 non-null   float64 \n",
      " 5   n_boat_ramps               2421 non-null   int64   \n",
      " 6   AWEvtType                  2421 non-null   float64 \n",
      " 7   Shannon_Index_CAT          2421 non-null   float64 \n",
      " 8   Shannon_Index_CENT         2421 non-null   float64 \n",
      " 9   Shannon_Index_DAR          2421 non-null   float64 \n",
      " 10  Shannon_Index_ESX          2421 non-null   float64 \n",
      " 11  Shannon_Index_OTM          2421 non-null   float64 \n",
      " 12  Shannon_Index_PRCH         2421 non-null   float64 \n",
      " 13  Shannon_Index_SAL_Other    2421 non-null   float64 \n",
      " 14  Shannon_Index_SCUL         2421 non-null   float64 \n",
      " 15  Shannon_Index_SUCK         2421 non-null   float64 \n",
      " 16  Shannon_Index_Native_Fish  2421 non-null   float64 \n",
      " 17  Richness_Native_Fish       2421 non-null   float64 \n",
      " 18  Shannon_Index_gamefish     2421 non-null   float64 \n",
      " 19  Richness_gamefish          2421 non-null   float64 \n",
      "dtypes: float64(17), geometry(1), int64(2)\n",
      "memory usage: 397.2 KB\n"
     ]
    }
   ],
   "source": [
    "merged_df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "05d0966b-15c4-489e-98ef-6579f7f6134e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Multi-band raster saved to: my_data/MN/mn_predictors_multiband.tif\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "columns_to_raster = [\n",
    "    \"Boater_movement\",\n",
    "    \"Water_connectivity\",\n",
    "    \"tpTsi\",\n",
    "    \"n_boat_ramps\",\n",
    "    \"AWEvtType\",\n",
    "    \"Shannon_Index_CAT\",\n",
    "    \"Shannon_Index_CENT\",\n",
    "    \"Shannon_Index_DAR\",\n",
    "    \"Shannon_Index_ESX\",\n",
    "    \"Shannon_Index_OTM\",\n",
    "    \"Shannon_Index_PRCH\",\n",
    "    \"Shannon_Index_SAL_Other\",\n",
    "    \"Shannon_Index_SCUL\",\n",
    "    \"Shannon_Index_SUCK\",\n",
    "    \"Shannon_Index_Native_Fish\",\n",
    "    \"Richness_Native_Fish\",\n",
    "    \"Shannon_Index_gamefish\",\n",
    "    \"Richness_gamefish\"\n",
    "]\n",
    "\n",
    "gdf_to_multiband_tif(\n",
    "    gdf=merged_df_clean,\n",
    "    columns=columns_to_raster,\n",
    "    output_path=training_path + \"mn_predictors_multiband.tif\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce44e34-839b-4a1c-b68f-0ef4b41d2e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download positive training data, create background points and biological predictors from USGS NAS database and Native Fish Presence/Absence dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87268ef0-72c8-4d02-b20e-ecc5330e0953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nhd_waterbodies(state_name, local_path, output_prefix=None, save_files=True,\n",
    "                         make_background_water=False, training_path=None, training_state_abbr=None):\n",
    "    \"\"\"\n",
    "    Downloads, extracts, and loads NHD shapefiles for a U.S. state, returning lakes, rivers, and flowlines.\n",
    "    \"\"\"\n",
    "    if output_prefix is None:\n",
    "        output_prefix = state_name.lower()\n",
    "\n",
    "    os.makedirs(local_path, exist_ok=True)\n",
    "\n",
    "    # Use GPKG filepaths for caching\n",
    "    lakes_gpkg = os.path.join(local_path, f\"{output_prefix}_lakes.gpkg\")\n",
    "    rivers_gpkg = os.path.join(local_path, f\"{output_prefix}_rivers.gpkg\")\n",
    "    streams_gpkg = os.path.join(local_path, f\"{output_prefix}_streams.gpkg\")\n",
    "\n",
    "    if save_files and all(os.path.exists(f) for f in [lakes_gpkg, rivers_gpkg, streams_gpkg]):\n",
    "        print(\"üìÇ Found existing GPKG files. Loading from disk...\")\n",
    "        lakes = gpd.read_file(lakes_gpkg)\n",
    "        rivers = gpd.read_file(rivers_gpkg)\n",
    "        streams = gpd.read_file(streams_gpkg)\n",
    "    else:\n",
    "        # === Step 1‚Äì5: Download, extract, load ===\n",
    "        url = f\"https://prd-tnm.s3.amazonaws.com/StagedProducts/Hydrography/NHD/State/Shape/NHD_H_{state_name}_State_Shape.zip\"\n",
    "        print(f\"üì• Downloading NHD data for {state_name}...\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"‚ùå Failed to download NHD data for {state_name}\")\n",
    "\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".zip\") as tmp_file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                tmp_file.write(chunk)\n",
    "\n",
    "        with zipfile.ZipFile(tmp_file.name, 'r') as z:\n",
    "            z.extractall(path=local_path)\n",
    "        print(\"‚úÖ Extraction complete.\")\n",
    "\n",
    "        shp_files = glob.glob(os.path.join(local_path, \"**\", \"*.shp\"), recursive=True)\n",
    "        if not shp_files:\n",
    "            raise FileNotFoundError(\"‚ùå No shapefiles found in extracted directory.\")\n",
    "\n",
    "        lakes_path = next((f for f in shp_files if \"nhdwaterbody\" in f.lower()), None)\n",
    "        if not lakes_path:\n",
    "            raise FileNotFoundError(\"‚ùå No NHDWaterbody shapefile found.\")\n",
    "        lakes = gpd.read_file(lakes_path)\n",
    "\n",
    "        rivers_path = next((f for f in shp_files if \"nhdarea\" in f.lower()), None)\n",
    "        if not rivers_path:\n",
    "            raise FileNotFoundError(\"‚ùå No NHDArea shapefile found.\")\n",
    "        rivers = gpd.read_file(rivers_path)\n",
    "\n",
    "        print(\"üìÑ Loading and combining stream flowlines...\")\n",
    "        flowline_paths = [f for f in shp_files if \"nhdflowline\" in f.lower()]\n",
    "        if not flowline_paths:\n",
    "            raise FileNotFoundError(\"‚ùå No NHDFlowline shapefiles found.\")\n",
    "        stream_dfs = [gpd.read_file(fp) for fp in flowline_paths]\n",
    "        streams = gpd.GeoDataFrame(pd.concat(stream_dfs, ignore_index=True), crs=stream_dfs[0].crs)\n",
    "\n",
    "        if save_files:\n",
    "            print(\"üíæ Saving as GeoPackage files...\")\n",
    "            lakes.to_file(lakes_gpkg, driver=\"GPKG\")\n",
    "            rivers.to_file(rivers_gpkg, driver=\"GPKG\")\n",
    "            streams.to_file(streams_gpkg, driver=\"GPKG\")\n",
    "\n",
    "    # ‚úÖ Now return regardless of source (cached or new)\n",
    "    result = {\n",
    "        \"lakes\": lakes,\n",
    "        \"rivers\": rivers,\n",
    "        \"streams\": streams\n",
    "    }\n",
    "\n",
    "    if make_background_water and training_path and training_state_abbr:\n",
    "        bg_water = pd.concat([lakes, rivers], ignore_index=True)\n",
    "        output_file = os.path.join(training_path, f\"{training_state_abbr}_bg_water.shp\")\n",
    "        bg_water.to_file(output_file)\n",
    "        result[\"bg_water\"] = bg_water\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def buffer_water_layers(\n",
    "    streams: gpd.GeoDataFrame = None,\n",
    "    lakes: gpd.GeoDataFrame = None,\n",
    "    rivers: gpd.GeoDataFrame = None,\n",
    "    simplify_tolerance: float = None,\n",
    "    filter_streams: bool = True,\n",
    "    save_path: str = None,\n",
    "    use_cached: bool = True,\n",
    "    export_merged_filename: str = None\n",
    ") -> tuple[gpd.GeoDataFrame, gpd.GeoDataFrame, gpd.GeoDataFrame, gpd.GeoDataFrame, gpd.GeoDataFrame]:\n",
    "    \"\"\"\n",
    "    Buffer and merge water layers, optionally using cached files.\n",
    "    \"\"\"\n",
    "\n",
    "    crs = \"EPSG:5070\"\n",
    "\n",
    "    # Define cache paths\n",
    "    if save_path:\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        stream_file = os.path.join(save_path, \"buffered_streams.gpkg\")\n",
    "        lake_file = os.path.join(save_path, \"buffered_lakes.gpkg\")\n",
    "        river_file = os.path.join(save_path, \"buffered_rivers.gpkg\")\n",
    "        full_stream_file = os.path.join(save_path, \"full_streams.shp\")\n",
    "\n",
    "        cache_exists = all(os.path.exists(f) for f in [stream_file, lake_file, river_file, full_stream_file])\n",
    "        if use_cached and cache_exists:\n",
    "            print(\"üìÇ Loading buffered layers from cache...\")\n",
    "            buffered_streams = gpd.read_file(stream_file)\n",
    "            buffered_lakes = gpd.read_file(lake_file)\n",
    "            buffered_rivers = gpd.read_file(river_file)\n",
    "            full_streams = gpd.read_file(full_stream_file)\n",
    "\n",
    "            buffered_water = pd.concat([buffered_streams, buffered_lakes, buffered_rivers], ignore_index=True)\n",
    "            buffered_water = buffered_water[buffered_water.geometry.notnull()]\n",
    "            buffered_water[\"waterID\"] = range(1, len(buffered_water) + 1)\n",
    "\n",
    "            if export_merged_filename and not os.path.exists(export_merged_filename):\n",
    "                print(f\"üíæ Merging and exporting combined buffered layer to {export_merged_filename}...\")\n",
    "                buffered_water.to_file(export_merged_filename)\n",
    "\n",
    "            return buffered_streams, buffered_lakes, buffered_rivers, full_streams, buffered_water\n",
    "\n",
    "    # If no cache or not using cache, we require streams/lakes/rivers\n",
    "    if streams is None or lakes is None or rivers is None:\n",
    "        raise ValueError(\"streams, lakes, and rivers must be provided if not using cached files.\")\n",
    "\n",
    "    # === Process from scratch ===\n",
    "    print(\"[1/6] Projecting all layers...\")\n",
    "    streams = streams.to_crs(crs)\n",
    "    lakes = lakes.to_crs(crs)\n",
    "    rivers = rivers.to_crs(crs)\n",
    "    full_streams = streams.copy()\n",
    "\n",
    "    print(\"[2/6] Filtering lakes and rivers with areasqkm ‚â• 0.25...\")\n",
    "    lakes = lakes[lakes.is_valid & lakes.geometry.notnull() & (lakes[\"areasqkm\"] >= 0.25)]\n",
    "    rivers = rivers[rivers.is_valid & rivers.geometry.notnull() & (rivers[\"areasqkm\"] >= 0.25)]\n",
    "\n",
    "    print(\"[3/6] Preparing stream geometry...\")\n",
    "    # Drop invalid or missing geometries before simplification\n",
    "    filtered_streams = streams.copy()\n",
    "    filtered_streams = filtered_streams[filtered_streams.geometry.notnull()].copy()\n",
    "    filtered_streams = filtered_streams[filtered_streams.is_valid].copy()\n",
    "    \n",
    "    # Now apply simplification\n",
    "    if simplify_tolerance is not None:\n",
    "        print(f\"[4/6] Simplifying stream geometries with tolerance {simplify_tolerance}...\")\n",
    "        filtered_streams[\"geometry\"] = filtered_streams.geometry.simplify(\n",
    "            tolerance=simplify_tolerance, preserve_topology=True)\n",
    "    else:\n",
    "        print(\"[4/6] Skipping simplification.\")\n",
    "\n",
    "    print(\"[5/6] Buffering lakes and rivers...\")\n",
    "    lakes[\"geometry\"] = lakes.geometry.buffer(50)\n",
    "    rivers[\"geometry\"] = rivers.geometry.buffer(50)\n",
    "\n",
    "    print(\"[6/6] Buffering stream geometries...\")\n",
    "    filtered_streams[\"geometry\"] = filtered_streams.geometry.buffer(50, cap_style=CAP_STYLE.flat)\n",
    "\n",
    "    # Optional save\n",
    "    if save_path:\n",
    "        print(\"üíæ Saving buffered layers...\")\n",
    "        filtered_streams.to_file(stream_file, driver=\"GPKG\")\n",
    "        lakes.to_file(lake_file, driver=\"GPKG\")\n",
    "        rivers.to_file(river_file, driver=\"GPKG\")\n",
    "        full_streams.to_file(full_stream_file)\n",
    "\n",
    "    buffered_water = pd.concat([filtered_streams, lakes, rivers], ignore_index=True)\n",
    "    buffered_water = buffered_water[buffered_water.geometry.notnull()]\n",
    "    buffered_water['waterID'] = range(1, len(buffered_water) + 1)\n",
    "\n",
    "    if export_merged_filename:\n",
    "        print(f\"üß± Merging and exporting combined buffered layer to {export_merged_filename}...\")\n",
    "        buffered_water.to_file(export_merged_filename)\n",
    "\n",
    "    return filtered_streams, lakes, rivers, full_streams, buffered_water\n",
    "# Biological predictor and training/background data helper functions\n",
    "def make_bg_data(\n",
    "    waterbody_gdf,\n",
    "    training_path,\n",
    "    training_state_abbr,\n",
    "    nas_name,\n",
    "    seed=42,\n",
    "    max_attempts=100,\n",
    "    thin=True,\n",
    "    min_dist=1000\n",
    "):\n",
    "    target_crs = \"EPSG:5070\"\n",
    "    \n",
    "    # Auto-load presence data\n",
    "    pos_data_path = f\"{training_path}{training_state_abbr}_{nas_name}_pos_data.shp\"\n",
    "    species_gdf_thin = gpd.read_file(pos_data_path)\n",
    "\n",
    "    # Reproject everything if needed\n",
    "    if waterbody_gdf.crs != target_crs:\n",
    "        waterbody_gdf = waterbody_gdf.to_crs(target_crs)\n",
    "    if species_gdf_thin.crs != target_crs:\n",
    "        species_gdf_thin = species_gdf_thin.to_crs(target_crs)\n",
    "\n",
    "    # Filter waterbodies by area\n",
    "    if 'areasqkm' not in waterbody_gdf.columns:\n",
    "        raise ValueError(\"Expected 'areasqkm' column in waterbody_gdf\")\n",
    "    waterbody_gdf = waterbody_gdf[waterbody_gdf['areasqkm'] > 0.25].copy()\n",
    "\n",
    "    presence_points = species_gdf_thin[species_gdf_thin['Present'] == 1]\n",
    "    sindex = presence_points.sindex\n",
    "\n",
    "    def has_presence(geom):\n",
    "        if geom.is_empty or not geom.is_valid:\n",
    "            return True\n",
    "        bounds = geom.bounds\n",
    "        candidates = list(sindex.intersection(bounds))\n",
    "        return any(presence_points.iloc[i].geometry.intersects(geom) for i in candidates)\n",
    "\n",
    "    non_ais_waterbodies = waterbody_gdf[~waterbody_gdf['geometry'].apply(has_presence)].copy()\n",
    "\n",
    "    random.seed(seed)\n",
    "    bg_points = []\n",
    "    for geom in non_ais_waterbodies.geometry.values:\n",
    "        minx, miny, maxx, maxy = geom.bounds\n",
    "        for _ in range(max_attempts):\n",
    "            x = random.uniform(minx, maxx)\n",
    "            y = random.uniform(miny, maxy)\n",
    "            pt = Point(x, y)\n",
    "            if geom.contains(pt):\n",
    "                bg_points.append(pt)\n",
    "                break\n",
    "\n",
    "    bg_gdf = gpd.GeoDataFrame(geometry=bg_points, crs=target_crs)\n",
    "\n",
    "    if thin and not bg_gdf.empty:\n",
    "        bg_gdf = thin_geodataframe(bg_gdf, min_dist=min_dist)\n",
    "\n",
    "    bg_gdf['Present'] = 0\n",
    "\n",
    "    # Save to shapefile\n",
    "    output_path = f\"{training_path}{training_state_abbr}_{nas_name}_bg_data.shp\"\n",
    "    bg_gdf.to_file(output_path)\n",
    "\n",
    "    return bg_gdf\n",
    "\n",
    "def process_nas_occurrences(\n",
    "    state,\n",
    "    crs,\n",
    "    path,\n",
    "    target_species_id=None,\n",
    "    target_species_name=None,\n",
    "    buffer_shapefile_suffix=\"_buffered_water.shp\",\n",
    "    species_columns=None,\n",
    "    snap_tolerance=100,\n",
    "    make_background=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Downloads, filters, spatially joins, and pivots NAS invasive species data.\n",
    "\n",
    "    Parameters:\n",
    "        state (str): US state abbreviation (e.g., 'MN')\n",
    "        crs (str or int): Target CRS (e.g., 'EPSG:5070')\n",
    "        path (str): Path to save/load shapefiles (must end with '/')\n",
    "        target_species_id (str, optional): SpeciesID to filter for a single species\n",
    "        target_species_name (str, optional): Common name to include in filename if target_species_id is used\n",
    "        buffer_shapefile_suffix (str): Suffix for the buffered waterbodies shapefile\n",
    "        species_columns (list): List of invasive group column names\n",
    "        snap_tolerance (int): Max distance for nearest spatial join in meters\n",
    "        make_background (bool): Whether to generate and save background points\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame: Final dataframe with invasive species richness per lake\n",
    "    \"\"\"\n",
    "    if species_columns is None:\n",
    "        species_columns = ['Inv_Algae', 'Inv_Crustaceans', 'Inv_Fish', 'Inv_Mollusks', 'Inv_Plants']\n",
    "\n",
    "    # Step 1: Call NAS API\n",
    "    url = f\"http://nas.er.usgs.gov/api/v2/occurrence/search?state={state}\"\n",
    "    response = requests.get(url, timeout=None).json()\n",
    "    results = pd.json_normalize(response, 'results')\n",
    "\n",
    "    # Step 2: Filter and clean\n",
    "    all_nas_data = results[results['status'] == 'established'].dropna()\n",
    "    all_nas_data = all_nas_data[[\"speciesID\", \"commonName\", \"group\", \"decimalLatitude\", \"decimalLongitude\"]]\n",
    "\n",
    "    # Step 3: Create GeoDataFrame and save\n",
    "    nas_gdf = gpd.GeoDataFrame(\n",
    "        all_nas_data,\n",
    "        geometry=gpd.points_from_xy(all_nas_data.decimalLongitude, all_nas_data.decimalLatitude),\n",
    "        crs=\"EPSG:4326\"\n",
    "    ).to_crs(crs).drop(columns=[\"decimalLatitude\", \"decimalLongitude\"])\n",
    "\n",
    "    species_gdf = nas_gdf[nas_gdf[\"speciesID\"] == target_species_id]\n",
    "    species_gdf_thin = thin_geodataframe(species_gdf, min_dist=100)\n",
    "    species_gdf_thin['Present'] = 1\n",
    "    species_filename = f\"{path}{state}_{target_species_name}_pos_data.shp\"\n",
    "    species_gdf_thin.to_file(species_filename)\n",
    "\n",
    "    nas_gdf = nas_gdf[nas_gdf[\"speciesID\"] != target_species_id]\n",
    "    nas_gdf.to_file(f\"{path}{state}_nas.shp\")\n",
    "\n",
    "    nas_gdf = gpd.read_file(f\"{path}{state}_nas.shp\").to_crs(crs)\n",
    "    buffered_water = gpd.read_file(f\"{path}{state}{buffer_shapefile_suffix}\").to_crs(crs)\n",
    "\n",
    "    NAS_ais_obs_df = gpd.sjoin_nearest(nas_gdf[['speciesID', 'commonName', 'group', 'geometry']],\n",
    "                                       buffered_water, how=\"inner\", max_distance=snap_tolerance)\n",
    "\n",
    "    NAS_ais_obs_df['group'] = NAS_ais_obs_df['group'].replace({\n",
    "        'Algae': 'Inv_Algae',\n",
    "        'Plants': 'Inv_Plants',\n",
    "        'Fishes': 'Inv_Fish',\n",
    "        'Crustaceans-Cladocerans': 'Inv_Crustaceans',\n",
    "        'Crustaceans-Amphipods': 'Inv_Crustaceans',\n",
    "        'Mollusks-Bivalves': 'Inv_Mollusks',\n",
    "        'Mollusks-Gastropods': 'Inv_Mollusks'\n",
    "    })\n",
    "\n",
    "    grouped = NAS_ais_obs_df[['waterID', 'commonName', 'group']].groupby(['waterID', 'group'])['commonName'].nunique().reset_index()\n",
    "    pivot_df = grouped.pivot(index='waterID', columns='group', values='commonName').reset_index().fillna(0)\n",
    "    lakes_w_invasives = pd.merge(buffered_water, pivot_df, on='waterID', how='left')\n",
    "\n",
    "    for col in species_columns:\n",
    "        if col not in lakes_w_invasives.columns:\n",
    "            lakes_w_invasives[col] = 0.0\n",
    "        else:\n",
    "            lakes_w_invasives[col] = lakes_w_invasives[col].astype('float64')\n",
    "\n",
    "    inv_rich = lakes_w_invasives[species_columns + ['geometry']].copy()\n",
    "    inv_gdf = gpd.GeoDataFrame(inv_rich, geometry='geometry', crs=lakes_w_invasives.crs)\n",
    "\n",
    "    # Step 11: Optional background generation\n",
    "    if make_background:\n",
    "        make_bg_data(\n",
    "            waterbody_gdf=buffered_water,\n",
    "            training_path=path,\n",
    "            training_state_abbr=state,\n",
    "            nas_name=target_species_name,\n",
    "            seed=42,\n",
    "            max_attempts=100,\n",
    "            thin=True,\n",
    "            min_dist=1000\n",
    "        )\n",
    "\n",
    "    return inv_gdf\n",
    "\n",
    "\n",
    "def export_inv_richness(inv_rich_gdf, output_path):\n",
    "    # Convert input data to EPSG:5070 BEFORE getting bounds\n",
    "    inv_rich_gdf = inv_rich_gdf\n",
    "\n",
    "    data_columns = ['Inv_Algae', 'Inv_Crustaceans', 'Inv_Fish', 'Inv_Mollusks', 'Inv_Plants']\n",
    "\n",
    "    # Ensure valid geometry\n",
    "    inv_rich_gdf = inv_rich_gdf[inv_rich_gdf.geometry.notnull() & inv_rich_gdf.geometry.is_valid]\n",
    "\n",
    "    # Ensure all required columns exist\n",
    "    for col in data_columns:\n",
    "        if col not in inv_rich_gdf.columns:\n",
    "            inv_rich_gdf[col] = 0\n",
    "    \n",
    "    # Warn if file exists\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"Warning: {output_path} already exists and will be overwritten.\")\n",
    "\n",
    "    if inv_rich_gdf.empty:\n",
    "        raise ValueError(\"Error: The input GeoDataFrame is empty!\")\n",
    "\n",
    "    # Get bounding box in CRS 5070\n",
    "    xmin, ymin, xmax, ymax = inv_rich_gdf.total_bounds  \n",
    "    pixel_size = 100 \n",
    "\n",
    "    # Ensure bounds are valid\n",
    "    if xmin == xmax:\n",
    "        xmin -= pixel_size\n",
    "        xmax += pixel_size\n",
    "    if ymin == ymax:\n",
    "        ymin -= pixel_size\n",
    "        ymax += pixel_size\n",
    "\n",
    "    # Recalculate width and height\n",
    "    width = max(1, int((xmax - xmin) / pixel_size))\n",
    "    height = max(1, int((ymax - ymin) / pixel_size))\n",
    "\n",
    "    # Correct transform in CRS 5070\n",
    "    transform = Affine(pixel_size, 0, xmin, 0, -pixel_size, ymax)\n",
    "    num_bands = len(data_columns) + 1  # Extra band for sum\n",
    "    raster = np.zeros((num_bands, height, width), dtype=np.float32)\n",
    "\n",
    "    # Debug: Check valid geometries\n",
    "    valid_geom_count = inv_rich_gdf.geometry.notnull().sum()\n",
    "    print(f\"Valid geometries count: {valid_geom_count}\")\n",
    "\n",
    "    # Rasterize each data column\n",
    "    for i, column in enumerate(data_columns):\n",
    "        shapes = [(geom, value) for geom, value in zip(inv_rich_gdf.geometry, inv_rich_gdf[column]) if geom is not None and not np.isnan(value)]\n",
    "        if not shapes:\n",
    "            print(f\"Skipping {column}: No valid geometries!\")\n",
    "            continue  # Skip empty bands\n",
    "        raster_band = rasterize(\n",
    "            shapes,\n",
    "            out_shape=(height, width),\n",
    "            transform=transform,\n",
    "            fill=0,\n",
    "            dtype='float32',\n",
    "            default_value=1,  # Ensure full coverage of waterbody\n",
    "            all_touched=True  # Ensures every pixel within the geometry gets a value\n",
    "        )\n",
    "        shapes = [(geom, value) for geom, value in zip(inv_rich_gdf.geometry, inv_rich_gdf[column])]\n",
    "        raster[i] = raster_band  # Keep original values\n",
    "        print(f\"Rasterized {column}: Min={raster_band.min()}, Max={raster_band.max()}, Non-zero pixels={np.count_nonzero(raster_band)}\")\n",
    "\n",
    "    # Compute the sum band\n",
    "    raster[-1] = np.sum(raster[:-1], axis=0)\n",
    "    print(f\"Final Sum Band: Min={raster[-1].min()}, Max={raster[-1].max()}, Non-zero pixels={np.count_nonzero(raster[-1])}\")\n",
    "\n",
    "    # Save as a multi-band GeoTIFF in EPSG:5070\n",
    "    with rasterio.open(\n",
    "        output_path, 'w',\n",
    "        driver='GTiff',\n",
    "        height=height,\n",
    "        width=width,\n",
    "        count=num_bands,\n",
    "        dtype=raster.dtype,\n",
    "        crs=\"EPSG:5070\",\n",
    "        transform=transform\n",
    "    ) as dst:\n",
    "        for band in range(num_bands):\n",
    "            dst.write(raster[band], band + 1)\n",
    "        band_names = data_columns + [\"Inv_Richness\"]\n",
    "        for band, name in enumerate(band_names, start=1):\n",
    "            dst.set_band_description(band, name)\n",
    "\n",
    "    # Debug: Plot overlay of geometries on raster\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.imshow(raster[-1], cmap='viridis', extent=[xmin, xmax, ymin, ymax], origin=\"upper\")\n",
    "    gpd.GeoSeries(inv_rich_gdf.geometry).plot(ax=ax, facecolor=\"none\", edgecolor=\"red\")\n",
    "    plt.title(\"Overlaying Geometries on Raster\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot bands\n",
    "    fig, axes = plt.subplots(1, num_bands, figsize=(15, 5))\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(raster[i], cmap='viridis')\n",
    "        ax.set_title(band_names[i])\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Raster file saved as '{output_path}' in CRS {\"EPSG:5070\"} with band names: {band_names}\")\n",
    "\n",
    "def process_and_export_native_fish_raster(\n",
    "    my_path: str,\n",
    "    my_crs: str,\n",
    "    state_abbr: str,\n",
    "    resolution: int = 100,\n",
    "    fish_shapefile: str = 'usgs_native_fish_rich.shp',\n",
    "    state_shapefile: str = 'tl_2012_us_state.shp',\n",
    "    buffer_shapefile_suffix: str = '_buffered_water.shp'\n",
    "):\n",
    "    def clip_points_by_polygon(points_gdf, polygon_gdf):\n",
    "        if points_gdf.crs != polygon_gdf.crs:\n",
    "            points_gdf = points_gdf.to_crs(polygon_gdf.crs)\n",
    "        clipped_points = gpd.sjoin(points_gdf, polygon_gdf, how='inner')\n",
    "        return clipped_points.drop(columns=polygon_gdf.columns.difference(['geometry']))\n",
    "\n",
    "    def sum_numeric_columns(gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "        sum_columns = [col for col in gdf.columns if col.isdigit() and len(col) == 6]\n",
    "        gdf[\"Native_Fish_Richness\"] = gdf[sum_columns].sum(axis=1)\n",
    "        return gdf[[\"Native_Fish_Richness\", \"geometry\"]]\n",
    "\n",
    "    def spatial_join_with_nearest(poly_gdf: gpd.GeoDataFrame, point_gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "        if poly_gdf.crs != point_gdf.crs:\n",
    "            poly_gdf = poly_gdf.to_crs(point_gdf.crs)\n",
    "        return gpd.sjoin_nearest(poly_gdf, point_gdf, how=\"left\", distance_col=\"distance\")\n",
    "\n",
    "    def export_native_raster(joined_gdf: gpd.GeoDataFrame, my_path, state_abbr, column_name: str = \"Native_Fish_Richness\"):\n",
    "        bounds = joined_gdf.total_bounds\n",
    "        transform = rasterio.transform.from_origin(bounds[0], bounds[3], resolution, resolution)\n",
    "        out_shape = (\n",
    "            int(np.ceil((bounds[3] - bounds[1]) / resolution)),\n",
    "            int(np.ceil((bounds[2] - bounds[0]) / resolution))\n",
    "        )\n",
    "\n",
    "        raster = rasterize(\n",
    "            [(geom, value) for geom, value in zip(joined_gdf.geometry, joined_gdf[column_name])],\n",
    "            out_shape=out_shape,\n",
    "            transform=transform,\n",
    "            fill=0,\n",
    "            dtype=rasterio.float32\n",
    "        )\n",
    "\n",
    "        parent_dir = os.path.abspath(os.path.join(my_path, os.pardir))\n",
    "        output_filename = os.path.join(my_path, f\"{state_abbr}_{column_name}.tif\")\n",
    "\n",
    "        with rasterio.open(\n",
    "            output_filename, \"w\",\n",
    "            driver=\"GTiff\",\n",
    "            height=out_shape[0],\n",
    "            width=out_shape[1],\n",
    "            count=1,\n",
    "            dtype=rasterio.float32,\n",
    "            crs=\"EPSG:5070\",\n",
    "            transform=transform\n",
    "        ) as dst:\n",
    "            dst.write(raster, 1)\n",
    "            dst.set_band_description(1, column_name)\n",
    "\n",
    "        # Optional plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(raster, cmap='viridis', extent=[bounds[0], bounds[2], bounds[1], bounds[3]])\n",
    "        plt.colorbar(label=f'{column_name} Richness')\n",
    "        plt.title('Rasterized Native Fish Richness')\n",
    "        plt.xlabel('Longitude')\n",
    "        plt.ylabel('Latitude')\n",
    "        plt.show()\n",
    "\n",
    "    # Load or download native fish data\n",
    "    fish_path = os.path.join(my_path, fish_shapefile)\n",
    "    if os.path.exists(fish_path):\n",
    "        fish_gdf = gpd.read_file(fish_path).to_crs(my_crs)\n",
    "    else:\n",
    "        url = \"https://www.sciencebase.gov/catalog/file/get/6086df60d34eadd49d31b04a?f=__disk__ad%2F21%2Ffc%2Fad21fc677379f4e45caa4bd506ca1c587d5f01f7\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            csv_data = StringIO(response.text)\n",
    "            fish_df = pd.read_csv(csv_data)\n",
    "            fish_gdf = gpd.GeoDataFrame(\n",
    "                fish_df,\n",
    "                geometry=gpd.points_from_xy(fish_df.longitude, fish_df.latitude),\n",
    "                crs=\"EPSG:4326\"\n",
    "            ).to_crs(my_crs)\n",
    "        else:\n",
    "            raise RuntimeError(f\"Failed to download native fish data: {response.status_code}\")\n",
    "\n",
    "    # Load state boundary and buffered water\n",
    "    state_boundary = gpd.read_file(os.path.join(my_path, state_shapefile)).dropna().to_crs(my_crs)\n",
    "    state_boundary = state_boundary[state_boundary['STUSPS'] == state_abbr]\n",
    "    buffered_water_path = os.path.join(my_path, state_abbr + buffer_shapefile_suffix)\n",
    "    buffered_water = gpd.read_file(buffered_water_path).to_crs(my_crs)\n",
    "\n",
    "    # Process native fish richness\n",
    "    clipped_fish = clip_points_by_polygon(fish_gdf, state_boundary)\n",
    "    native_fish_gdf = sum_numeric_columns(clipped_fish)\n",
    "    water_with_fish = spatial_join_with_nearest(buffered_water, native_fish_gdf)\n",
    "\n",
    "    # Convert int columns to float\n",
    "    for col in water_with_fish.select_dtypes(include=['int64']).columns:\n",
    "        water_with_fish[col] = water_with_fish[col].astype('float64')\n",
    "\n",
    "    export_native_raster(water_with_fish, my_path, state_abbr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea19635-26ab-45b0-b96f-2acef2957114",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download/import water data.\n",
    "water_data_training = get_nhd_waterbodies(\n",
    "    state_name=training_state_name,\n",
    "    local_path=training_path,\n",
    "    output_prefix=training_state_abbr,\n",
    "    save_files=True,\n",
    "    make_background_water=True, # set to false here, unless you want this for future model training within your state\n",
    "    training_path=training_path,\n",
    "    training_state_abbr=training_state_abbr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddde4eae-06be-49ce-bc45-4530a422ff69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buffer individual water layers\n",
    "buffered_streams_training, buffered_lakes_training, buffered_rivers_training, full_streams_training, buffered_water_training = buffer_water_layers(\n",
    "    streams=water_data_training[\"streams\"],\n",
    "    lakes=water_data_training[\"lakes\"],\n",
    "    rivers=water_data_training[\"rivers\"],\n",
    "    filter_streams=False,\n",
    "    save_path= training_path,\n",
    "    use_cached=True,\n",
    "    export_merged_filename=f\"{training_path}{training_state_abbr}_buffered_water.shp\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5849a2a-97b7-4776-8942-13b03d0fce25",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_rich_training = process_nas_occurrences(\n",
    "    state= training_state_abbr,\n",
    "    crs= my_crs,\n",
    "    path= training_path,\n",
    "    target_species_id= nas_id,\n",
    "    target_species_name= nas_name,\n",
    "    make_background=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6496350-c1d0-487f-8e40-4dcb4807a44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_inv_richness(inv_rich_training, training_path + training_state_abbr + '_' + nas_name +'_inv_richness.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953fb947-7162-4dcb-812e-3c068b480730",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_and_export_native_fish_raster(\n",
    "    my_path=training_path, \n",
    "    my_crs=\"EPSG:5070\",\n",
    "    state_abbr=training_state_abbr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b90ed0d-1e64-4c00-a2a1-e654fe721d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the predictors you just created and export a final combined raster \n",
    "# Define input files\n",
    "input_files_bio_training = [\n",
    "    training_path + training_state_abbr +'_' + nas_name + \"_inv_richness.tif\", \n",
    "    training_path + training_state_abbr + \"_Native_Fish_Richness.tif\"]\n",
    "#Combine rasters into a multi-band GeoTIFF\n",
    "output_file_bio_training = training_path + training_state_abbr + '_' + nas_name + \"_combined_bio.tif\" # this will replace the other combined tif you made.\n",
    "combine_geotiffs(input_files_bio_training, output_file_bio_training)\n",
    "\n",
    "print(f\"‚úÖ Multi-band raster saved as {output_file_bio_training}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c5a859-49f9-48ad-a526-a663b10f8f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Note these distance predictors are highly correlated. You should make the predictor that is most relevant to your taxa\n",
    "or if your target does spread by both mechanisms make both and let the model sort it out. This is kind of nuanced as Road Distance may also\n",
    "be informative of AIS which were originally stocked and now spread by stream/river network such as Rainbow and Brook Trout.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92117b4c-5d65-4838-9a2f-eb95d3722505",
   "metadata": {},
   "source": [
    "# Road Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45e7eda-b7e0-4f36-bb6b-5970a0ea885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_roads_and_endpoints(\n",
    "    local_path,\n",
    "    state_abbr,\n",
    "    state_fips,\n",
    "    state_name,\n",
    "    buffered_water,\n",
    "    nas_id,\n",
    "    my_crs,\n",
    "    snap_dist=500\n",
    "):\n",
    "    def download_tiger_roads(state_fips, local_path):\n",
    "        url = f'https://www2.census.gov/geo/tiger/TIGER2022/PRISECROADS/tl_2022_{state_fips}_prisecroads.zip'\n",
    "        r = requests.get(url)\n",
    "        if r.status_code != 200:\n",
    "            raise Exception(f\"‚ùå Failed to download from {url}\")\n",
    "        z = zipfile.ZipFile(BytesIO(r.content))\n",
    "        z.extractall(path=local_path)\n",
    "\n",
    "    def download_ramps(local_path):\n",
    "        url = \"https://www.sciencebase.gov/catalog/file/get/63b81b50d34e92aad3cc004d?facet=Boatramps_United_States_final_20230104\"\n",
    "        extensions = {'.dbf', '.prj', '.shp', '.shx'}\n",
    "        response = requests.get(url, stream=True, timeout=60)\n",
    "        response.raise_for_status()\n",
    "        with zipfile.ZipFile(BytesIO(response.content)) as zip_ref:\n",
    "            os.makedirs(local_path, exist_ok=True)\n",
    "            for f in zip_ref.namelist():\n",
    "                if os.path.splitext(f)[1].lower() in extensions:\n",
    "                    zip_ref.extract(f, local_path)\n",
    "\n",
    "    def get_endpoints(geometry):\n",
    "        if geometry.geom_type == 'LineString':\n",
    "            return [geometry.coords[0], geometry.coords[-1]]\n",
    "        return []\n",
    "\n",
    "    def nas_api_call(nas_id, state_abbr):\n",
    "        url = f\"http://nas.er.usgs.gov/api/v2/occurrence/search?species_ID={nas_id}&state={state_abbr}\"\n",
    "        response = requests.get(url).json()\n",
    "        return pd.json_normalize(response, 'results')\n",
    "\n",
    "    def sjoin_nearest_to_centroid_replace_geom(left_gdf, right_gdf, **kwargs):\n",
    "        left_centroids = left_gdf.copy()\n",
    "        left_centroids[\"geometry_centroid\"] = left_centroids.geometry.centroid\n",
    "        left_centroids = left_centroids.set_geometry(\"geometry_centroid\")\n",
    "        right_temp = right_gdf.copy()\n",
    "        right_temp[\"geometry_right\"] = right_temp.geometry\n",
    "        right_temp = right_temp.set_geometry(\"geometry_right\")\n",
    "        joined = gpd.sjoin_nearest(left_centroids, right_temp, how=\"left\", **kwargs)\n",
    "        joined[\"distance\"] = joined.geometry_centroid.distance(joined[\"geometry_right\"])\n",
    "        result = left_gdf.copy()\n",
    "        result[\"geometry\"] = joined[\"geometry_right\"]\n",
    "        result[\"epointID\"] = joined[\"epointID\"]\n",
    "        result[\"distance_to_nearest\"] = joined[\"distance\"]\n",
    "        return result\n",
    "\n",
    "    # Step 1: Download ramps and roads\n",
    "    download_ramps(os.path.abspath(os.path.join(local_path)))\n",
    "    download_tiger_roads(state_fips, local_path)\n",
    "\n",
    "    # Step 2: Load data\n",
    "    ramps = gpd.read_file(local_path + 'Boatramps_United_States_final_20230104.shp').set_crs(my_crs, allow_override=True)\n",
    "    ramp_geo = ramps.loc[ramps['State'] == state_name, ['geometry']].copy()\n",
    "    ramp_geo['ramp_ID'] = range(1, len(ramp_geo) + 1)\n",
    "\n",
    "    buffered_water = buffered_water.to_crs(my_crs)\n",
    "    pos_data = nas_api_call(nas_id, state_abbr)\n",
    "    pos_gdf = gpd.GeoDataFrame(\n",
    "        pos_data[[\"decimalLatitude\", \"decimalLongitude\"]],\n",
    "        geometry=gpd.points_from_xy(pos_data.decimalLongitude, pos_data.decimalLatitude),\n",
    "        crs=\"EPSG:4326\"\n",
    "    ).to_crs(my_crs)\n",
    "\n",
    "    # Step 3: Identify water presence/absence\n",
    "    water_check = buffered_water.sjoin(pos_gdf, how=\"left\", predicate=\"contains\")\n",
    "    water_check = water_check.drop_duplicates(subset=\"waterID\")\n",
    "    neg_water = water_check[water_check['index_right'].isna()].drop(columns=[\"index_right\", \"decimalLatitude\", \"decimalLongitude\"], errors=\"ignore\")\n",
    "    pos_water = water_check.dropna(subset=[\"index_right\"]).drop(columns=[\"index_right\", \"decimalLatitude\", \"decimalLongitude\"], errors=\"ignore\")\n",
    "    pos_water[\"Present\"], neg_water[\"Present\"] = 1.0, 0.0\n",
    "    water_with_presence = pd.concat([pos_water, neg_water])\n",
    "\n",
    "    # Step 4: Match ramps to water\n",
    "    ramps_in_water = ramp_geo.sjoin(water_with_presence, how=\"left\", predicate=\"within\")\n",
    "    ramps_not_in_water = ramps_in_water[ramps_in_water['index_right'].isna()].drop(columns=[\"index_right\"], errors=\"ignore\").copy()\n",
    "    ramps_in_water = ramps_in_water.dropna(subset=[\"index_right\"]).drop(columns=[\"index_right\"], errors=\"ignore\")\n",
    "    ramps_in_water[\"waterID\"] = ramps_in_water[\"waterID\"].astype(\"int64\")\n",
    "\n",
    "    water_ids_with_ramps = ramps_in_water['waterID'].tolist()\n",
    "    water_no_ramps = water_with_presence[~water_with_presence['waterID'].isin(water_ids_with_ramps)]\n",
    "\n",
    "    # Step 5: Extract and deduplicate road endpoints\n",
    "    my_roads = gpd.read_file(os.path.join(local_path, f\"tl_2022_{state_fips}_prisecroads.shp\")).to_crs(my_crs)\n",
    "    endpoints = my_roads['geometry'].apply(get_endpoints).explode()\n",
    "    endpoints_df = pd.DataFrame(endpoints.tolist(), columns=['x', 'y']).drop_duplicates()\n",
    "    endpoints_gdf = gpd.GeoDataFrame(endpoints_df, geometry=gpd.points_from_xy(endpoints_df['x'], endpoints_df['y']), crs=my_crs).drop(columns=['x', 'y'])\n",
    "    endpoints_gdf['epointID'] = range(1, len(endpoints_gdf) + 1)\n",
    "\n",
    "    # Step 6: Snap ramps and lakes to endpoints\n",
    "    ramps_in_water_sj = sjoin_nearest_to_centroid_replace_geom(ramps_in_water, endpoints_gdf)\n",
    "    lakes_no_ramp_sj = sjoin_nearest_to_centroid_replace_geom(water_no_ramps, endpoints_gdf)\n",
    "    lakes_no_ramp_epoints = lakes_no_ramp_sj[['waterID', 'geometry', 'Present', 'epointID', 'distance_to_nearest']].dropna()\n",
    "    ramps_in_water_epoints = ramps_in_water_sj[['waterID', 'geometry', 'Present', 'epointID', 'distance_to_nearest']].dropna()\n",
    "    all_endpoints = pd.concat([ramps_in_water_epoints, lakes_no_ramp_epoints])\n",
    "    pos_endpoints = all_endpoints.loc[all_endpoints['Present'] == 1.0]\n",
    "    neg_endpoints = all_endpoints.loc[all_endpoints['Present'] == 0.0]\n",
    "\n",
    "    return my_roads, pos_endpoints, neg_endpoints, ramps_in_water_epoints, lakes_no_ramp_epoints\n",
    "\n",
    "def build_network_optimized(road_network_gdf, precision=6):\n",
    "    \"\"\"\n",
    "    Build a NetworkX graph from a GeoDataFrame of road LineStrings.\n",
    "\n",
    "    Nodes are rounded to a given precision to reduce floating-point redundancy.\n",
    "\n",
    "    Args:\n",
    "        road_network_gdf (GeoDataFrame): Must contain LineString or MultiLineString geometries.\n",
    "        precision (int): Number of decimal places to round coordinates to for node deduplication.\n",
    "\n",
    "    Returns:\n",
    "        networkx.Graph: Graph with nodes as (x, y) tuples and edges weighted by length.\n",
    "    \"\"\"\n",
    "    import networkx as nx\n",
    "    from shapely.geometry import LineString\n",
    "\n",
    "    G = nx.Graph()\n",
    "\n",
    "    for geom in road_network_gdf.geometry:\n",
    "        if geom is None:\n",
    "            continue\n",
    "\n",
    "        # Handle both LineString and MultiLineString\n",
    "        if isinstance(geom, LineString):\n",
    "            lines = [geom]\n",
    "        elif hasattr(geom, 'geoms'):  # MultiLineString\n",
    "            lines = list(geom.geoms)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        for line in lines:\n",
    "            coords = list(line.coords)\n",
    "            for u, v in zip(coords[:-1], coords[1:]):\n",
    "                if len(u) < 2 or len(v) < 2:\n",
    "                    continue  # skip invalid coordinate pairs\n",
    "\n",
    "                u_rounded = tuple(round(c, precision) for c in u[:2])\n",
    "                v_rounded = tuple(round(c, precision) for c in v[:2])\n",
    "                dist = LineString([u, v]).length\n",
    "\n",
    "                # Add undirected edge with distance as weight\n",
    "                G.add_edge(u_rounded, v_rounded, weight=dist)\n",
    "\n",
    "    return G\n",
    "\n",
    "def build_nearest_node_index(graph):\n",
    "    \"\"\"\n",
    "    Build a KDTree for efficient nearest-node lookup in a NetworkX graph.\n",
    "\n",
    "    Args:\n",
    "        graph (networkx.Graph): A graph with 2D tuple coordinates as nodes.\n",
    "\n",
    "    Returns:\n",
    "        cKDTree: KD-tree built from graph node coordinates.\n",
    "        list: List of node tuples in the same order as the KD-tree.\n",
    "    \"\"\"\n",
    "    # Ensure all nodes are 2D points (x, y)\n",
    "    nodes = [node for node in graph.nodes if len(node) == 2]\n",
    "    coords = [list(node) for node in nodes]  # convert tuples to lists for KDTree\n",
    "\n",
    "    tree = cKDTree(coords)\n",
    "    return tree, nodes\n",
    "    \n",
    "def find_nearest_node_kd_tree(kd_tree, point, precision=6):\n",
    "    \"\"\"\n",
    "    Find the nearest graph node to a given point using a KD-tree.\n",
    "\n",
    "    Args:\n",
    "        kd_tree (cKDTree): KDTree built from graph node coordinates.\n",
    "        point (shapely.geometry.Point): Point to find the nearest node to.\n",
    "        precision (int): Decimal places to round the coordinates.\n",
    "\n",
    "    Returns:\n",
    "        tuple: The (x, y) coordinates of the nearest node, rounded.\n",
    "    \"\"\"\n",
    "    _, index = kd_tree.query([point.x, point.y])\n",
    "    nearest_coords = kd_tree.data[index]\n",
    "    return tuple(round(coord, precision) for coord in nearest_coords)\n",
    "    \n",
    "def assign_endpoints_multi_source_dijkstra(presences_gdf, endpoints_gdf, G, kd_tree, precision=6):\n",
    "    \"\"\"\n",
    "    Assign each endpoint to its nearest presence based on road network distance.\n",
    "\n",
    "    Args:\n",
    "        presences_gdf (GeoDataFrame): GeoDataFrame of presence points with 'epointID'.\n",
    "        endpoints_gdf (GeoDataFrame): GeoDataFrame of negative endpoints with 'epointID'.\n",
    "        G (networkx.Graph): Graph of road network.\n",
    "        kd_tree (cKDTree): KDTree built from graph nodes.\n",
    "        precision (int): Decimal precision for rounding coordinates.\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: Each dict contains 'epointID', 'target_point_id', and 'distance_roads'.\n",
    "    \"\"\"\n",
    "    # Step 1: Snap presence points to graph nodes\n",
    "    presence_nodes = {}\n",
    "    node_to_presence_id = {}\n",
    "\n",
    "    for _, row in tqdm(presences_gdf.iterrows(), total=len(presences_gdf), desc=\"Snapping presences\"):\n",
    "        node = find_nearest_node_kd_tree(kd_tree, row.geometry.centroid, precision)\n",
    "        if not G.has_node(node):\n",
    "            continue  # Skip if the node isn't actually in the graph\n",
    "        presence_nodes[row[\"epointID\"]] = node\n",
    "        node_to_presence_id[node] = row[\"epointID\"]\n",
    "\n",
    "    if not presence_nodes:\n",
    "        raise ValueError(\"No presence points could be snapped to graph nodes.\")\n",
    "\n",
    "    # Step 2: Multi-source Dijkstra from all presence nodes\n",
    "    source_nodes = list(presence_nodes.values())\n",
    "    distances, predecessors = nx.multi_source_dijkstra(G, sources=source_nodes, weight='weight')\n",
    "\n",
    "    # Step 3: For each negative endpoint, find the closest presence\n",
    "    results = []\n",
    "\n",
    "    for _, row in tqdm(endpoints_gdf.iterrows(), total=len(endpoints_gdf), desc=\"Processing negatives\"):\n",
    "        neg_id = row[\"epointID\"]\n",
    "        neg_point = row.geometry.centroid\n",
    "        neg_node = find_nearest_node_kd_tree(kd_tree, neg_point, precision)\n",
    "\n",
    "        if not G.has_node(neg_node):\n",
    "            results.append({\n",
    "                \"epointID\": neg_id,\n",
    "                \"target_point_id\": None,\n",
    "                \"distance_roads\": np.inf\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        if neg_node in source_nodes:\n",
    "            # Direct match to a presence node\n",
    "            dist = 0.0\n",
    "            target_id = node_to_presence_id[neg_node]\n",
    "        elif neg_node in distances:\n",
    "            # Trace back the path to the nearest source\n",
    "            path = []\n",
    "            current = neg_node\n",
    "            while current not in source_nodes:\n",
    "                prev = predecessors.get(current)\n",
    "                if not prev:\n",
    "                    break\n",
    "                current = prev[0]  # Multi-source returns list of predecessors\n",
    "                path.append(current)\n",
    "\n",
    "            source_node = current if current in source_nodes else None\n",
    "            target_id = node_to_presence_id.get(source_node, None)\n",
    "            dist = distances[neg_node]\n",
    "        else:\n",
    "            # Not reachable\n",
    "            dist = np.inf\n",
    "            target_id = None\n",
    "\n",
    "        results.append({\n",
    "            \"epointID\": neg_id,\n",
    "            \"target_point_id\": target_id,\n",
    "            \"distance_roads\": dist\n",
    "        })\n",
    "\n",
    "    return results\n",
    "    \n",
    "def vector_to_raster(gdf, output_path, nas_name, value_field=None, resolution=100):\n",
    "    # Ensure geometries are valid\n",
    "    gdf = gdf[~gdf.geometry.isna()]\n",
    "    gdf = gdf[gdf.geometry.is_valid]\n",
    "\n",
    "    # Ensure it has a CRS\n",
    "    if gdf.crs is None:\n",
    "        raise ValueError(\"GeoDataFrame must have a CRS.\")\n",
    "\n",
    "    # Reproject to projected CRS if necessary\n",
    "    if gdf.crs.is_geographic:\n",
    "        gdf = gdf.to_crs(\"EPSG:5070\")  # NAD83 / Conus Albers (meters)\n",
    "\n",
    "    # Get bounds\n",
    "    minx, miny, maxx, maxy = gdf.total_bounds\n",
    "    width = int(np.ceil((maxx - minx) / resolution))\n",
    "    height = int(np.ceil((maxy - miny) / resolution))\n",
    "\n",
    "    # Check again\n",
    "    if width <= 0 or height <= 0:\n",
    "        raise ValueError(\"Calculated width or height is <= 0. Check your resolution and geometry bounds.\")\n",
    "\n",
    "    # Create transform\n",
    "    transform = from_origin(minx, maxy, resolution, resolution)\n",
    "\n",
    "    # Prepare shapes\n",
    "    shapes = (\n",
    "        ((geom, value) for geom, value in zip(gdf.geometry, gdf[value_field]))\n",
    "        if value_field else\n",
    "        ((geom, 1) for geom in gdf.geometry)\n",
    "    )\n",
    "\n",
    "    # Rasterize\n",
    "    raster = rasterize(\n",
    "        shapes=shapes,\n",
    "        out_shape=(height, width),\n",
    "        transform=transform,\n",
    "        fill= np.nan,\n",
    "        dtype='float32' if not value_field else 'float32'\n",
    "    )\n",
    "\n",
    "    # Write to GeoTIFF\n",
    "    band_name = f\"distance_road_{nas_name}\"\n",
    "    with rasterio.open(\n",
    "        output_path,\n",
    "        \"w\",\n",
    "        driver=\"GTiff\",\n",
    "        height=height,\n",
    "        width=width,\n",
    "        count=1,\n",
    "        dtype=raster.dtype,\n",
    "        crs=gdf.crs,\n",
    "        transform=transform,\n",
    "    ) as dst:\n",
    "        dst.write(raster, 1)\n",
    "        dst.set_band_description(1, band_name)\n",
    "\n",
    "    print(f\"Raster written to {output_path}\")\n",
    "\n",
    "def compute_and_export_road_distances(\n",
    "    my_roads: gpd.GeoDataFrame,\n",
    "    pos_endpoints: gpd.GeoDataFrame,\n",
    "    neg_endpoints: gpd.GeoDataFrame,\n",
    "    ramps_epoints: gpd.GeoDataFrame,\n",
    "    lakes_epoints: gpd.GeoDataFrame,\n",
    "    buffered_water: gpd.GeoDataFrame,\n",
    "    my_crs: str,\n",
    "    my_path: str,\n",
    "    my_state: str,\n",
    "    nas_name: str,\n",
    "    resolution: int = 100\n",
    ") -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Computes shortest road network distances from invasive presence points to absence points,\n",
    "    joins those distances to ramp/lake endpoints, and outputs a raster and plot.\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame: Waterbodies with distance_roads_total assigned.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure CRS consistency\n",
    "    my_roads = my_roads.to_crs(my_crs)\n",
    "    pos_endpoints = pos_endpoints.to_crs(my_crs)\n",
    "    neg_endpoints = neg_endpoints.to_crs(my_crs)\n",
    "\n",
    "    # Build the road network and nearest node index\n",
    "    G = build_network_optimized(my_roads)\n",
    "    kd_tree, graph_nodes = build_nearest_node_index(G)\n",
    "\n",
    "    # Run Dijkstra from positive to negative endpoints\n",
    "    dijkstra_results = assign_endpoints_multi_source_dijkstra(\n",
    "        presences_gdf=pos_endpoints,\n",
    "        endpoints_gdf=neg_endpoints,\n",
    "        G=G,\n",
    "        kd_tree=kd_tree,\n",
    "        precision=6\n",
    "    )\n",
    "    dist_df = pd.DataFrame(dijkstra_results)\n",
    "    dist_df = dist_df.loc[dist_df.groupby('epointID')['distance_roads'].idxmin()].reset_index(drop=True)\n",
    "\n",
    "    # Merge distances to endpoints\n",
    "    ramps_w_dist = pd.merge(ramps_epoints, dist_df, on='epointID', how='left')\n",
    "    lakes_w_dist = pd.merge(lakes_epoints, dist_df, on='epointID', how='left')\n",
    "\n",
    "    # Clean and select needed columns\n",
    "    keep_cols = ['geometry', 'target_point_id', 'epointID', 'waterID', 'Present', 'distance_roads', 'distance_to_nearest']\n",
    "    ramps_clean = ramps_w_dist[keep_cols].dropna()\n",
    "    lakes_clean = lakes_w_dist[keep_cols].dropna()\n",
    "\n",
    "    # Downcast numeric columns\n",
    "    for df in [ramps_clean, lakes_clean]:\n",
    "        for col in ['distance_roads', 'distance_to_nearest']:\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "\n",
    "    # Fill missing distances\n",
    "    ramps_clean['adjusted_distance_to_nearest'] = ramps_clean['distance_to_nearest']\n",
    "    lakes_clean['adjusted_distance_to_nearest'] = lakes_clean['distance_to_nearest']\n",
    "\n",
    "    ramps_clean['distance_roads_filled'] = ramps_clean['distance_roads'].fillna(0)\n",
    "    lakes_clean['distance_roads_filled'] = lakes_clean['distance_roads'].fillna(0)\n",
    "\n",
    "    # Compute total distances\n",
    "    ramps_clean['distance_roads_total'] = 0\n",
    "    ramps_clean.loc[ramps_clean['Present'] == 0, 'distance_roads_total'] = (\n",
    "        ramps_clean['distance_roads'] + ramps_clean['adjusted_distance_to_nearest']\n",
    "    )\n",
    "    ramps_clean.loc[ramps_clean['Present'] != 0, 'distance_roads_total'] = ramps_clean['distance_roads_filled']\n",
    "\n",
    "    lakes_clean['distance_roads_total'] = 0\n",
    "    lakes_clean.loc[lakes_clean['Present'] == 0, 'distance_roads_total'] = (\n",
    "        lakes_clean['distance_roads'] + lakes_clean['adjusted_distance_to_nearest']\n",
    "    )\n",
    "    lakes_clean.loc[lakes_clean['Present'] != 0, 'distance_roads_total'] = lakes_clean['distance_roads_filled']\n",
    "\n",
    "    # Clean up intermediate columns\n",
    "    ramps_clean.drop(columns=['distance_roads_filled'], inplace=True)\n",
    "    lakes_clean.drop(columns=['distance_roads_filled'], inplace=True)\n",
    "\n",
    "    # Final merge and assignment\n",
    "    final_dist_df = pd.concat([ramps_clean, lakes_clean], ignore_index=True)\n",
    "    min_dist_by_water = final_dist_df.loc[final_dist_df.groupby(\"waterID\")[\"distance_roads_total\"].idxmin()]\n",
    "    water_w_dist = pd.merge(buffered_water, min_dist_by_water, on=\"waterID\", how=\"left\")\n",
    "    water_w_dist_final = water_w_dist[[\"waterID\", \"geometry_x\", \"distance_roads_total\"]].rename(columns={\"geometry_x\": \"geometry\"}).dropna()\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    water_w_dist_final.plot(\n",
    "        ax=ax, column=\"distance_roads_total\", cmap=\"viridis\", linewidth=0.5, legend=True\n",
    "    )\n",
    "    ax.set_title(\"Water with Distance to Roads\", fontsize=14)\n",
    "    ax.set_xlabel(\"Longitude\")\n",
    "    ax.set_ylabel(\"Latitude\")\n",
    "    plt.show()\n",
    "\n",
    "    # Export raster\n",
    "    output_filename = f\"{my_path}{my_state}_road_distance_{nas_name}.tif\"\n",
    "    vector_to_raster(\n",
    "        gdf=water_w_dist_final,\n",
    "        output_path=output_filename,\n",
    "        nas_name=nas_name,  # must match function signature\n",
    "        value_field=\"distance_roads_total\",\n",
    "        resolution=resolution\n",
    "    )\n",
    "\n",
    "    return water_w_dist_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953688f1-bd76-451c-b2a2-577a51dec72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import and format data for road network/distance analysis\n",
    "my_roads, pos_endpoints, neg_endpoints, ramps_in_water_epoints, lakes_no_ramp_epoints = extract_roads_and_endpoints(\n",
    "    local_path=training_path,\n",
    "    state_abbr=training_state_abbr,\n",
    "    state_fips=training_fip,\n",
    "    state_name=training_state_abbr,\n",
    "    buffered_water = buffered_water_training,\n",
    "    nas_id=nas_id,\n",
    "    my_crs=my_crs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a84bab-686d-4aee-8ed4-afc7f6d7ce3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_with_dist = compute_and_export_road_distances(\n",
    "    my_roads=my_roads,\n",
    "    pos_endpoints=pos_endpoints,\n",
    "    neg_endpoints=neg_endpoints,\n",
    "    ramps_epoints=ramps_in_water_epoints,\n",
    "    lakes_epoints=lakes_no_ramp_epoints,\n",
    "    buffered_water=buffered_water_training,\n",
    "    my_crs=my_crs,\n",
    "    my_path= training_path,\n",
    "    my_state=training_state_abbr,\n",
    "    nas_name= nas_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4ac656-ef2a-4b07-80f2-a47082fff9e0",
   "metadata": {},
   "source": [
    "# Stream/River Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b571b8-11c9-4aba-8365-d6086a584503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance to source streams\n",
    "discon_value = 999999.0  # Define a consistent disconnection value\n",
    "@contextmanager\n",
    "def tqdm_joblib(tqdm_object):\n",
    "    class TqdmBatchCompletionCallback(joblib.parallel.BatchCompletionCallBack):\n",
    "        def __call__(self, *args, **kwargs):\n",
    "            tqdm_object.update(n=self.batch_size)\n",
    "            return super().__call__(*args, **kwargs)\n",
    "\n",
    "    old_callback = joblib.parallel.BatchCompletionCallBack\n",
    "    joblib.parallel.BatchCompletionCallBack = TqdmBatchCompletionCallback\n",
    "    try:\n",
    "        yield tqdm_object\n",
    "    finally:\n",
    "        joblib.parallel.BatchCompletionCallBack = old_callback\n",
    "        tqdm_object.close()\n",
    "def build_network(streams_gdf):\n",
    "    G = nx.DiGraph()\n",
    "    for geom in streams_gdf.geometry:\n",
    "        if geom is None or geom.is_empty:\n",
    "            continue\n",
    "\n",
    "        if isinstance(geom, LineString):\n",
    "            coords = list(geom.coords)\n",
    "            edges = zip(coords[:-1], coords[1:])\n",
    "        elif isinstance(geom, MultiLineString):\n",
    "            edges = []\n",
    "            for line in geom:\n",
    "                coords = list(line.coords)\n",
    "                edges.extend(zip(coords[:-1], coords[1:]))\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        for u, v in edges:\n",
    "            u_r = (round(u[0], 6), round(u[1], 6))\n",
    "            v_r = (round(v[0], 6), round(v[1], 6))\n",
    "            if u_r != v_r:\n",
    "                G.add_edge(u_r, v_r, weight=Point(u_r).distance(Point(v_r)))\n",
    "    return G\n",
    "\n",
    "def snap_points_to_vertices(points_gdf, vertices_tree, vertices_coords):\n",
    "    coords = np.array([(geom.x, geom.y) for geom in points_gdf.geometry])\n",
    "    _, idxs = vertices_tree.query(coords, k=1)\n",
    "    snapped_points = vertices_coords[idxs]\n",
    "    return snapped_points, idxs\n",
    "def calculate_dijkstra_for_point(source_node, G, vertices_in_buffer):\n",
    "    if source_node not in G:\n",
    "        print(f\"‚ö†Ô∏è  Source node {source_node} not in graph. Skipping.\")\n",
    "        return np.full(len(vertices_in_buffer), discon_value, dtype=np.float32)\n",
    "\n",
    "    try:\n",
    "        lengths = nx.single_source_dijkstra_path_length(G, source_node, weight='weight')\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error running Dijkstra from {source_node}: {e}\")\n",
    "        return np.full(len(vertices_in_buffer), discon_value, dtype=np.float32)\n",
    "\n",
    "    vertex_distances = np.full(len(vertices_in_buffer), discon_value, dtype=np.float32)\n",
    "    reachable = 0\n",
    "    for j, target_geom in enumerate(vertices_in_buffer.geometry):\n",
    "        target_node = (round(target_geom.x, 6), round(target_geom.y, 6))\n",
    "        if target_node in lengths:\n",
    "            vertex_distances[j] = lengths[target_node]\n",
    "            reachable += 1\n",
    "\n",
    "    print(f\"‚úÖ Source {source_node}: reached {reachable} / {len(vertices_in_buffer)} targets\")\n",
    "    return vertex_distances\n",
    "# --- Main Processing Function ---\n",
    "def process_state(state_abbr, my_path, streams_all, presence_all):\n",
    "    print(f\"\\n--- Processing {state_abbr} ---\")\n",
    "    shp_dir = os.path.join(my_path, f\"{state_abbr}_vertex_distances\")\n",
    "    os.makedirs(shp_dir, exist_ok=True)\n",
    "    shp_path = os.path.join(my_path, f\"{state_abbr}_vertex_distances.gpkg\")\n",
    "\n",
    "    try:\n",
    "        if streams_all.empty:\n",
    "            print(f\"Skipping {state_abbr}: no streams found.\")\n",
    "            return None\n",
    "\n",
    "        # Ensure CRS is consistent\n",
    "        streams = streams_all.copy()\n",
    "        if presence_all.crs != streams.crs:\n",
    "            presence_points = presence_all.to_crs(streams.crs)\n",
    "            print(f\"üìê Reprojected presence points to match stream CRS: {streams.crs}\")\n",
    "        else:\n",
    "            presence_points = presence_all.copy()\n",
    "\n",
    "        if presence_points.empty:\n",
    "            print(f\"{state_abbr}: no presence points found in state. Skipping.\")\n",
    "            return None\n",
    "\n",
    "        print(f\"Extracting vertices for {state_abbr}...\")\n",
    "        all_vertices = []\n",
    "        for geom in tqdm(streams.geometry, desc=f\"{state_abbr} stream vertices\", unit=\"geom\"):\n",
    "            if geom is None or geom.is_empty:\n",
    "                continue\n",
    "            if isinstance(geom, LineString):\n",
    "                all_vertices.extend(geom.coords)\n",
    "            elif isinstance(geom, MultiLineString):\n",
    "                for line in geom.geoms:\n",
    "                    all_vertices.extend(line.coords)\n",
    "\n",
    "        vertices_gdf = gpd.GeoDataFrame(\n",
    "            geometry=gpd.points_from_xy(\n",
    "                [pt[0] for pt in all_vertices],\n",
    "                [pt[1] for pt in all_vertices]\n",
    "            ),\n",
    "            crs=streams.crs\n",
    "        ).drop_duplicates()\n",
    "\n",
    "        vertices_gdf = vertices_gdf[~((vertices_gdf.geometry.x == 0) & (vertices_gdf.geometry.y == 0))]\n",
    "        vertices_coords = np.array([(pt.x, pt.y) for pt in vertices_gdf.geometry])\n",
    "        vertices_tree = cKDTree(vertices_coords)\n",
    "\n",
    "        G = build_network(streams)\n",
    "        print(f\"üìä Graph has {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
    "\n",
    "        snapped_coords, _ = snap_points_to_vertices(presence_points, vertices_tree, vertices_coords)\n",
    "        snapped_coords = np.round(snapped_coords, 6)\n",
    "\n",
    "        # Validate source nodes\n",
    "        valid_sources = [tuple(coord) for coord in snapped_coords if tuple(coord) in G]\n",
    "        print(f\"‚úÖ Valid source nodes in graph: {len(valid_sources)} / {len(snapped_coords)}\")\n",
    "\n",
    "        def is_node_in_graph(geom):\n",
    "            return (round(geom.x, 6), round(geom.y, 6)) in G\n",
    "\n",
    "        vertices_in_graph = vertices_gdf[vertices_gdf.geometry.apply(is_node_in_graph)]\n",
    "\n",
    "        if vertices_in_graph.empty:\n",
    "            print(f\"{state_abbr}: no stream vertices found in graph. Skipping.\")\n",
    "            return None\n",
    "\n",
    "        print(f\"Running parallel Dijkstra for {len(valid_sources)} valid presence-adjacent points in {state_abbr}...\")\n",
    "\n",
    "        presence_tree = cKDTree(snapped_coords)\n",
    "        vertex_coords = np.array([(geom.x, geom.y) for geom in vertices_in_graph.geometry])\n",
    "        _, nearest_presence_idxs = presence_tree.query(vertex_coords, k=1)\n",
    "\n",
    "        presence_to_vertices = defaultdict(list)\n",
    "        for v_idx, p_idx in enumerate(nearest_presence_idxs):\n",
    "            presence_to_vertices[p_idx].append(v_idx)\n",
    "\n",
    "        print(f\"Running optimized Dijkstra for {len(presence_to_vertices)} unique presence points in {state_abbr}...\")\n",
    "\n",
    "        min_distances = np.full(len(vertices_in_graph), discon_value, dtype=np.float32)\n",
    "\n",
    "        with tqdm_joblib(tqdm(desc=f\"{state_abbr} Dijkstra\", total=len(presence_to_vertices), unit=\"pt\")):\n",
    "            results = Parallel(n_jobs=2, backend=\"threading\")(\n",
    "                delayed(calculate_dijkstra_for_point)(\n",
    "                    tuple(snapped_coords[p_idx]),\n",
    "                    G,\n",
    "                    vertices_in_graph.iloc[v_idxs]\n",
    "                )\n",
    "                for p_idx, v_idxs in presence_to_vertices.items()\n",
    "                if tuple(snapped_coords[p_idx]) in G\n",
    "            )\n",
    "\n",
    "        for (p_idx, v_idxs), dist_array in zip(presence_to_vertices.items(), results):\n",
    "            for local_idx, global_idx in enumerate(v_idxs):\n",
    "                min_distances[global_idx] = min(min_distances[global_idx], dist_array[local_idx])\n",
    "\n",
    "        vertices_in_graph[\"distance_r\"] = min_distances.astype(np.float32)\n",
    "\n",
    "        print(f\"üìà distance_r summary: min={np.min(min_distances[min_distances < discon_value]):.2f}, \"\n",
    "              f\"max={np.max(min_distances[min_distances < discon_value]):.2f}, \"\n",
    "              f\"unreachable={(min_distances == discon_value).sum()}\")\n",
    "\n",
    "        vertices_in_graph.to_file(shp_path, driver=\"GPKG\")\n",
    "        print(f\"‚úÖ {state_abbr} vertex shapefile exported to: {shp_path}\")\n",
    "        return shp_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in {state_abbr}: {e}\")\n",
    "        return None\n",
    "# Cell 2: Utility functions\n",
    "\n",
    "def safe_read_file(fp):\n",
    "    try:\n",
    "        return gpd.read_file(fp)\n",
    "    except Exception as e:\n",
    "        raise IOError(f\"Error reading {fp}: {e}\")\n",
    "\n",
    "def build_kdtree(vertices):\n",
    "    coords = np.array(list(zip(vertices.geometry.x, vertices.geometry.y)))\n",
    "    tree = cKDTree(coords)\n",
    "    return tree, coords\n",
    "\n",
    "def query_parallel(tree, points, n_jobs=-1):\n",
    "    results = Parallel(n_jobs=n_jobs, backend=\"threading\")(\n",
    "        delayed(tree.query)(pt) for pt in tqdm(points, desc=\"Querying KDTree\", unit=\"pt\")\n",
    "    )\n",
    "    return zip(*results)\n",
    "\n",
    "def add_line_ids(stream_lines):\n",
    "    stream_lines['line_id'] = stream_lines.index.astype(str)\n",
    "    return stream_lines\n",
    "\n",
    "def assign_vertices_to_lines(vertices, stream_lines):\n",
    "    assert vertices.crs == stream_lines.crs, \"CRS mismatch between vertices and stream lines\"\n",
    "    print(\"Assigning vertices to stream lines via spatial join...\")\n",
    "    joined = gpd.sjoin(vertices, stream_lines[['line_id', 'geometry']], how='left', predicate='within')\n",
    "    if joined['line_id'].isna().all():\n",
    "        raise ValueError(\"No vertices could be matched to stream lines. Check geometries.\")\n",
    "    return joined\n",
    "\n",
    "def filter_disconnected_lines(vertices):\n",
    "    print(\"Filtering disconnected stream segments...\")\n",
    "    bad_line_ids = (\n",
    "        vertices.groupby('line_id')['distance_r']\n",
    "        .apply(lambda x: (x == discon_value).all())\n",
    "        .loc[lambda x: x].index\n",
    "    )\n",
    "    return bad_line_ids\n",
    "\n",
    "def split_waterbodies_by_connection(waterbodies, stream_lines, bad_line_ids):\n",
    "    print(\"Splitting waterbodies into connected and disconnected sets...\")\n",
    "    disconnected = stream_lines[stream_lines['line_id'].isin(bad_line_ids)]\n",
    "    connected = stream_lines[~stream_lines['line_id'].isin(bad_line_ids)]\n",
    "    waterbodies = waterbodies.to_crs(stream_lines.crs)\n",
    "    wb_disconnected = gpd.sjoin(waterbodies, disconnected, how='inner', predicate='intersects')\n",
    "    wb_connected = gpd.sjoin(waterbodies, connected, how='inner', predicate='intersects')\n",
    "    wb_disconnected_ids = set(wb_disconnected.index)\n",
    "    wb_connected_ids = set(wb_connected.index)\n",
    "    purely_disconnected_ids = wb_disconnected_ids - wb_connected_ids\n",
    "    return (\n",
    "        waterbodies[~waterbodies.index.isin(purely_disconnected_ids)],\n",
    "        waterbodies[waterbodies.index.isin(purely_disconnected_ids)]\n",
    "    )\n",
    "# Cell 3: Rasterization and plotting\n",
    "def rasterize_distances(vertices, waterbodies_connected, waterbodies_disconnected, output_raster_fp, nas_name, pixel_size=30):\n",
    "    print(\"Rasterizing distances to source...\")\n",
    "    vertices = vertices[vertices['distance_r'].notna()]\n",
    "    vertices = vertices[vertices['distance_r'] != discon_value]\n",
    "    vertices['distance_r'] = vertices['distance_r'].astype(float)\n",
    "\n",
    "    tree, coords = build_kdtree(vertices)\n",
    "    values = vertices['distance_r'].values\n",
    "\n",
    "    all_waterbodies = gpd.GeoSeries(pd.concat([waterbodies_connected.geometry, waterbodies_disconnected.geometry]))\n",
    "    minx, miny, maxx, maxy = all_waterbodies.total_bounds\n",
    "    width = int(np.ceil((maxx - minx) / pixel_size))\n",
    "    height = int(np.ceil((maxy - miny) / pixel_size))\n",
    "    transform = from_bounds(minx, miny, maxx, maxy, width, height)\n",
    "\n",
    "    x_coords = np.linspace(minx + pixel_size / 2, maxx - pixel_size / 2, width)\n",
    "    y_coords = np.linspace(maxy - pixel_size / 2, miny + pixel_size / 2, height)\n",
    "    xx, yy = np.meshgrid(x_coords, y_coords)\n",
    "    pixel_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    print(\"Rasterizing waterbody masks...\")\n",
    "    all_waterbody_mask = rasterize(\n",
    "        [(geom, 1) for geom in all_waterbodies],\n",
    "        out_shape=(height, width),\n",
    "        transform=transform,\n",
    "        fill=0,\n",
    "        dtype='uint8'\n",
    "    )\n",
    "    connected_mask = rasterize(\n",
    "        [(geom, 1) for geom in waterbodies_connected.geometry],\n",
    "        out_shape=(height, width),\n",
    "        transform=transform,\n",
    "        fill=0,\n",
    "        dtype='uint8'\n",
    "    )\n",
    "\n",
    "    output_array = np.full((height, width), discon_value, dtype='float32')\n",
    "    idx_connected = np.where(connected_mask.ravel() == 1)[0]\n",
    "\n",
    "    print(f\"Querying {len(idx_connected)} connected pixels to KDTree...\")\n",
    "    dist, nn_idx = query_parallel(tree, list(pixel_points[idx_connected]))\n",
    "    output_array.ravel()[idx_connected] = values[list(nn_idx)]\n",
    "    output_array[all_waterbody_mask == 0] = np.nan\n",
    "\n",
    "    valid_mask = output_array != discon_value\n",
    "    if np.any(valid_mask):\n",
    "        max_val = np.nanmax(output_array[valid_mask])\n",
    "        output_array[~valid_mask] = max_val\n",
    "    else:\n",
    "        print(\"Warning: No valid distances found. Raster may be empty.\")\n",
    "\n",
    "    print(f\"Writing output raster to {output_raster_fp}...\")\n",
    "    band_name = f\"distance_river_{nas_name}\"\n",
    "    with rasterio.open(\n",
    "        output_raster_fp,\n",
    "        'w',\n",
    "        driver='GTiff',\n",
    "        height=height,\n",
    "        width=width,\n",
    "        count=1,\n",
    "        dtype='float32',\n",
    "        crs=waterbodies_connected.crs,\n",
    "        transform=transform,\n",
    "        nodata=np.nan\n",
    "    ) as dst:\n",
    "        dst.write(output_array, 1)\n",
    "        dst.set_band_description(1, band_name)\n",
    "\n",
    "    return output_array\n",
    "\n",
    "def preview_raster(array):\n",
    "    plt.imshow(array, cmap='viridis')\n",
    "    plt.colorbar(label='Distance to Source')\n",
    "    plt.title('Rasterized Distance to Stream Source')\n",
    "    plt.show()\n",
    "\n",
    "def log_metadata(output_fp, metadata_dict):\n",
    "    print(f\"Logging metadata to {output_fp}...\")\n",
    "    with open(output_fp, 'w') as f:\n",
    "        json.dump(metadata_dict, f, indent=2)\n",
    "\n",
    "def prepare_stream_network(stream_path, simplify_tolerance, crs):\n",
    "    streams = gpd.read_file(stream_path).to_crs(crs)\n",
    "    streams = streams.drop_duplicates(subset='geometry').reset_index(drop=True)\n",
    "    dissolved = unary_union(streams.geometry)\n",
    "    simplified = gpd.GeoDataFrame(geometry=[dissolved], crs=crs).explode(index_parts=False).reset_index(drop=True)\n",
    "\n",
    "    if simplify_tolerance is not None:\n",
    "        simplified[\"geometry\"] = simplified.simplify(tolerance=simplify_tolerance, preserve_topology=True)\n",
    "\n",
    "    return simplified\n",
    "    \n",
    "def prepare_presence_points(presence_path, crs):\n",
    "    presence = gpd.read_file(presence_path)\n",
    "    return presence.to_crs(crs)\n",
    "def run_stream_distance_model(state_abbr, streams_all, presence_all, output_dir):\n",
    "    return process_state(\n",
    "        state_abbr=state_abbr,\n",
    "        streams_all=streams_all,\n",
    "        presence_all=presence_all,\n",
    "        my_path=output_dir\n",
    "    )    \n",
    "\n",
    "def postprocess_stream_network(vertices_fp, streams_fp, water_fp):\n",
    "    streams = add_line_ids(safe_read_file(streams_fp))\n",
    "    vertices = safe_read_file(vertices_fp)\n",
    "    waterbodies = safe_read_file(water_fp)\n",
    "    \n",
    "    vertices = assign_vertices_to_lines(vertices.to_crs(streams.crs), streams)\n",
    "    bad_line_ids = filter_disconnected_lines(vertices)\n",
    "    wb_connected, wb_disconnected = split_waterbodies_by_connection(waterbodies, streams, bad_line_ids)\n",
    "    \n",
    "    return vertices, wb_connected, wb_disconnected, bad_line_ids\n",
    "\n",
    "def make_stream_distance_raster(vertices, wb_connected, wb_disconnected, output_fp, metadata_fp, nas_name, pixel_size=100, state_abbr=None):\n",
    "    array = rasterize_distances(vertices, wb_connected, wb_disconnected, output_fp, nas_name, pixel_size)\n",
    "    log_metadata(metadata_fp, {\n",
    "        'state': state_abbr,\n",
    "        'pixel_size': pixel_size,\n",
    "        'bad_line_ids': list(filter_disconnected_lines(vertices)),\n",
    "        'vertex_count': len(vertices),\n",
    "        'raster_output': output_fp\n",
    "    })\n",
    "    return array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22bdbf4-1a25-42f6-a5ca-89ea3cd8b447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "stream_path = os.path.join(training_path, f\"full_streams.shp\")\n",
    "presence_path = os.path.join(training_path, f\"{training_state_abbr}_{nas_name}_pos_data.shp\")\n",
    "output_raster_fp = os.path.join(training_path, f\"{training_state_abbr}_dist_to_src_river_{nas_name}.tif\")\n",
    "metadata_fp = output_raster_fp.replace('.tif', '_metadata.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac67c9a-a217-42ff-a20c-e501af7a8f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare inputs\n",
    "streams_all = prepare_stream_network(stream_path, simplify_tolerance=1000, crs=my_crs)\n",
    "presence_all = prepare_presence_points(presence_path, crs=my_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797c1bc3-50a0-41e6-828a-0f3147056837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Run Dijkstra model\n",
    "run_stream_distance_model(training_state_abbr, streams_all, presence_all, training_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d74554-55c4-4448-b6fa-a46997b7241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Postprocess results\n",
    "vertices_fp = os.path.join(training_path, f\"{training_state_abbr}_vertex_distances.gpkg\")\n",
    "streams_fp = stream_path\n",
    "water_fp = os.path.join(training_path, f\"{training_state_abbr}_buffered_water.shp\")\n",
    "vertices, wb_conn, wb_disc, _ = postprocess_stream_network(vertices_fp, streams_fp, water_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09ab9da-3756-4146-a470-65d2aff38cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Rasterize\n",
    "raster_array = make_stream_distance_raster(vertices, wb_conn, wb_disc, output_raster_fp, metadata_fp, nas_name, pixel_size=100, state_abbr=training_state_abbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf31ded-47b0-4e3f-a13b-a18f2bd8b350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the predictors you just created and export a final combined raster \n",
    "# Define input files\n",
    "input_files_dist_training = [\n",
    "    training_path + training_state_abbr +'_road_distance_' + nas_name + \".tif\", \n",
    "    training_path + training_state_abbr + '_dist_to_src_river_' + nas_name + '.tif']\n",
    "#Combine rasters into a multi-band GeoTIFF\n",
    "output_file_dist_training = training_path + training_state_abbr + '_' + nas_name + \"_combined_dist.tif\" # this will replace the other combined tif you made.\n",
    "combine_geotiffs(input_files_dist_training, output_file_dist_training)\n",
    "\n",
    "print(f\"‚úÖ Multi-band raster saved as {output_file_dist_training}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249553ef-b341-45e8-ae99-47d94447f86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Homerange similarity: ***Under Construction***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b627c1e-d186-46d8-a59e-d9f6d03b4d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***You need to run this GEE script for your state and homerange first***\n",
    "# https://code.earthengine.google.com/b5d49bb675cc2d6583e866dc1dfb440b\n",
    "#get taxon key for your AIS from gbif\n",
    "#species.name_backbone(name='Dreissena polymorpha', kingdom='animal')\n",
    "my_training_state = training_state_abbr # should be the postal code abbreviation for the state you created the environmental raster for....\n",
    "my_nas_id = nas_id # go to USGS NAS database for species_ids (e.g., 5 = Zebra Mussels; 237 = Eurasian watermilfoil; 551 = Bighead carp)\n",
    "my_path = training_path\n",
    "homerange_raster = my_path + \"homerange_2003_2022.tif\"\n",
    "invaded_raster = my_path + \"inv_rsd_2003_2022.tif\"\n",
    "my_countries = [\"RU\", \"UA\", \"BG\", \"RO\", \"GE\", \"AZ\", \"TM\", \"KZ\"] # Endemic range countries for your taxa\n",
    "my_taxon = 2287072  # gbif taxon id ; Eurasian watermilfoil = 2362486; Zebra mussels = 2362486\n",
    "limit = 10000 # This is for the gbif function so you don't blow up your computer... Just kidding that shouldn't happen : )\n",
    "my_scale = 1000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f680bb5-e796-487e-9837-54a155328219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def gbif_api_call(taxon, country, limit):\n",
    "    \"\"\"Fetch GBIF occurrences for a given taxon and country.\"\"\"\n",
    "    URL_BASE = 'https://api.gbif.org/v1/'\n",
    "    url_request = f\"{URL_BASE}occurrence/search?taxonKey={taxon}&country={country}&limit={limit}\"  \n",
    "    response = requests.get(url_request, timeout=30)\n",
    "    return response.json()  # Return the JSON response directly\n",
    "\n",
    "def nas_api_call(nas_id, state):\n",
    "    URL_BASE = 'http://nas.er.usgs.gov/api/v2/'\n",
    "    url_request = f\"{URL_BASE}/occurrence/search?species_ID={nas_id}&state={my_training_state}\"\n",
    "    response = requests.get(url_request, timeout=None).json()\n",
    "    results = pd.json_normalize(response, 'results')\n",
    "    return results\n",
    "\n",
    "\n",
    "def sample_multiband_geotiff_with_names(raster_path, gdf):\n",
    "    \"\"\"\n",
    "    Samples a multi-band GeoTIFF at specified point locations from a GeoDataFrame,\n",
    "    using band names from the raster.\n",
    "\n",
    "    Parameters:\n",
    "    - raster_path (str): Path to the GeoTIFF file.\n",
    "    - gdf (GeoDataFrame): GeoDataFrame containing point geometries.\n",
    "\n",
    "    Returns:\n",
    "    - GeoDataFrame with additional columns for each band, using raster band names.\n",
    "    \"\"\"\n",
    "\n",
    "    # Open the raster file\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        # Reproject GeoDataFrame to match raster CRS if needed\n",
    "        if gdf.crs != src.crs:\n",
    "            gdf = gdf.to_crs(src.crs)\n",
    "\n",
    "        # Convert point geometries to raster pixel coordinates\n",
    "        coords = [(geom.x, geom.y) for geom in gdf.geometry]\n",
    "\n",
    "        # Sample raster at point locations (returns a list of tuples with values per band)\n",
    "        sampled_values = list(src.sample(coords))\n",
    "\n",
    "        # Get band names (if available, otherwise use default names)\n",
    "        band_names = src.descriptions if all(src.descriptions) else [f\"band_{i+1}\" for i in range(src.count)]\n",
    "\n",
    "        # Create new columns in the GeoDataFrame with the corresponding band names\n",
    "        for band_idx, band_name in enumerate(band_names):\n",
    "            gdf[band_name] = [val[band_idx] for val in sampled_values]\n",
    "\n",
    "    return gdf\n",
    "\n",
    "def filter_dataframe_columns(df, feature_choices):\n",
    "    return df[[col for col in df.columns if col in feature_choices or col == \"geometry\"]]\n",
    "\n",
    "def extract_fields(data):\n",
    "    \"\"\"Extract relevant fields from GBIF response.\"\"\"\n",
    "    extracted_data = []\n",
    "    for record in data:\n",
    "        entry = {\n",
    "            'key': record.get('key'),\n",
    "            'species': record.get('species'),\n",
    "            'decimalLatitude': record.get('decimalLatitude'),\n",
    "            'decimalLongitude': record.get('decimalLongitude'),\n",
    "            'countryCode': record.get('countryCode'),\n",
    "            'year': record.get('year')\n",
    "        }\n",
    "        extracted_data.append(entry)\n",
    "    return extracted_data\n",
    "\n",
    "def MESS(ref_df, pred_df):\n",
    "    # Extract geometry before dropping it\n",
    "    geometry = None\n",
    "    if \"geometry\" in pred_df.columns:\n",
    "        geometry = pred_df[\"geometry\"].copy()  # Save geometry separately\n",
    "        pred_df = pred_df.drop(columns=[\"geometry\", \"predID\"])  # Drop before calculations\n",
    "\n",
    "    # Ensure reference DataFrame does not include geometry\n",
    "    ref_numeric = ref_df.drop(columns=[\"geometry\"], errors=\"ignore\")  # Avoid geometry errors\n",
    "\n",
    "    # Compute min and max values for each variable\n",
    "    mins = dict(ref_numeric.min())\n",
    "    maxs = dict(ref_numeric.max())\n",
    "\n",
    "    def calculate_s(column):\n",
    "        values = ref_numeric[column]  # Reference values\n",
    "        sims = []\n",
    "\n",
    "        for element in np.array(pred_df[column]):\n",
    "            f = np.count_nonzero((values < element)) / values.size\n",
    "\n",
    "            if f == 0:\n",
    "                sim = ((element - mins[column]) / (maxs[column] - mins[column]))\n",
    "            elif 0 < f <= 50:\n",
    "                sim = 2 * f\n",
    "            elif 50 < f < 100:\n",
    "                sim = 2 * (1 - f)\n",
    "            elif f == 100:\n",
    "                sim = ((maxs[column] - element) / (maxs[column] - mins[column]))\n",
    "\n",
    "            sims.append(sim)\n",
    "\n",
    "        return sims\n",
    "\n",
    "    # Compute similarity scores for each predictor\n",
    "    sim_df = pd.DataFrame()\n",
    "    for c in pred_df.columns:\n",
    "        sim_df[c] = calculate_s(c)\n",
    "\n",
    "    # Compute MESS values\n",
    "    min_similarity = sim_df.min(axis=1)  # Least similar predictor's score\n",
    "    MoD = sim_df.idxmin(axis=1)  # Least similar predictor's name\n",
    "\n",
    "    # Combine results\n",
    "    MESS = pd.concat([min_similarity, MoD], axis=1)\n",
    "    MESS.columns = [\"MESS_Score\", \"Least_Similar_Variable\"]\n",
    "\n",
    "    # Reattach geometry if it was present\n",
    "    if geometry is not None:\n",
    "        print(\"Before reattaching geometry:\", MESS.dtypes)  # Debug print\n",
    "    \n",
    "        MESS[\"geometry\"] = geometry  # Re-add geometry\n",
    "        MESS = gpd.GeoDataFrame(MESS, geometry=\"geometry\", crs=4269)  # Convert back to GeoDataFrame\n",
    "        \n",
    "        print(\"After reattaching geometry:\", MESS.dtypes)  # Debug print\n",
    "        print(\"Geometry column exists?\", \"geometry\" in MESS.columns)\n",
    "    \n",
    "    return MESS\n",
    "\n",
    "def export_mess(joined_gdf: gpd.GeoDataFrame, resolution: int = my_scale):\n",
    "    # Ensure CRS is projected (use EPSG:5070 or appropriate for your region)\n",
    "    if joined_gdf.crs.to_epsg() != 5070:\n",
    "        joined_gdf = joined_gdf.to_crs(epsg=5070)\n",
    "\n",
    "    # Get bounds\n",
    "    bounds = joined_gdf.total_bounds  # [minx, miny, maxx, maxy]\n",
    "    print(f\"Bounds in projected CRS: {bounds}\")\n",
    "\n",
    "    # Compute raster size\n",
    "    width = int(np.ceil((bounds[2] - bounds[0]) / resolution))\n",
    "    height = int(np.ceil((bounds[3] - bounds[1]) / resolution))\n",
    "\n",
    "    if width <= 0 or height <= 0:\n",
    "        raise ValueError(f\"Invalid raster dimensions: width={width}, height={height}\")\n",
    "\n",
    "    # Define transform\n",
    "    transform = rasterio.transform.from_origin(bounds[0], bounds[3], resolution, resolution)\n",
    "\n",
    "    # Ensure \"mess\" column exists and is numeric\n",
    "    column_name = \"MESS_Score\"\n",
    "    if column_name not in joined_gdf.columns:\n",
    "        raise KeyError(f\"Column '{column_name}' is missing from the GeoDataFrame!\")\n",
    "\n",
    "    joined_gdf[column_name] = joined_gdf[column_name].fillna(0).astype(float)\n",
    "\n",
    "    # Prepare shapes for rasterization\n",
    "    shapes = [(geom, value) for geom, value in zip(joined_gdf.geometry, joined_gdf[column_name]) if not np.isnan(value)]\n",
    "\n",
    "    # Create raster\n",
    "    raster = rasterize(\n",
    "        shapes=shapes,\n",
    "        out_shape=(height, width),\n",
    "        transform=transform,\n",
    "        fill=0,\n",
    "        dtype=np.float32\n",
    "    )\n",
    "\n",
    "    # Save to file\n",
    "    output_filename = f\"{my_path}{my_training_state}_{column_name}.tif\"\n",
    "    with rasterio.open(\n",
    "        output_filename, \"w\",\n",
    "        driver=\"GTiff\",\n",
    "        height=height,\n",
    "        width=width,\n",
    "        count=1,\n",
    "        dtype=rasterio.float32,\n",
    "        crs=joined_gdf.crs,  # Use the same projected CRS\n",
    "        transform=transform\n",
    "    ) as dst:\n",
    "        dst.write(raster, 1)\n",
    "        dst.set_band_description(1, column_name)\n",
    "\n",
    "    # Check raster output\n",
    "    #print(f\"Raster saved as: {output_filename}\")\n",
    "    #print(f\"Unique raster values: {np.unique(raster)}\")  # Ensure non-zero values exist\n",
    "\n",
    "    # Plot the raster\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(raster, cmap=\"viridis\", extent=[bounds[0], bounds[2], bounds[1], bounds[3]])\n",
    "    plt.colorbar(label=f'{column_name}')\n",
    "    plt.title('Rasterized MESS')\n",
    "    plt.xlabel('X (meters)')\n",
    "    plt.ylabel('Y (meters)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cb3fc8-3387-422a-9106-187619853ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbif_result = []\n",
    "for country in my_countries:\n",
    "    result = gbif_api_call(my_taxon, country, limit)\n",
    "    gbif_result.extend(result.get(\"results\", []))  # Append results directly\n",
    "# Extract fields from all collected results\n",
    "homerange_points = pd.DataFrame(extract_fields(gbif_result))\n",
    "homerange_points = gpd.GeoDataFrame(\n",
    "    homerange_points, geometry=gpd.points_from_xy(homerange_points.decimalLongitude, homerange_points.decimalLatitude)).dropna().set_crs(4269).to_crs(5070)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904d23f4-197b-4970-8c19-19d8ae6aa4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pres_gdf = gpd.read_file(training_path + 'MN_ZM_pos_data.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4347849a-3214-4619-ab8c-32d73b055dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_gdf = gpd.read_file(training_path + 'MN_ZM_bg_data.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1803dc-9311-4bce-a1ed-da9dbbafac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Load training presence + background points ----\n",
    "# (assumes you already have them as GeoDataFrames with same CRS as raster)\n",
    "# e.g. pres_gdf and bg_gdf\n",
    "all_points = pd.concat([pres_gdf, bg_gdf], ignore_index=True)\n",
    "\n",
    "# ---- Open invaded raster ----\n",
    "raster_path = invaded_raster\n",
    "with rasterio.open(raster_path) as src:\n",
    "    points_proj = all_points.to_crs(src.crs)\n",
    "    coords = [(geom.x, geom.y) for geom in points_proj.geometry]\n",
    "    sampled_vals = list(src.sample(coords))\n",
    "\n",
    "# ---- Convert sampled results into DataFrame ----\n",
    "pred_data = pd.DataFrame(sampled_vals, columns=band_names)\n",
    "\n",
    "# ---- Add geometry + IDs ----\n",
    "my_pred_data = gpd.GeoDataFrame(\n",
    "    pd.concat([all_points.reset_index(drop=True), pred_data], axis=1),\n",
    "    geometry=all_points.geometry,\n",
    "    crs=all_points.crs\n",
    ")\n",
    "\n",
    "# ---- Clean up NoData values ----\n",
    "with rasterio.open(raster_path) as src:\n",
    "    if src.nodata is not None:\n",
    "        my_pred_data.replace(src.nodata, np.nan, inplace=True)\n",
    "\n",
    "# ---- Check result ----\n",
    "print(my_pred_data.info())\n",
    "print(my_pred_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257c4fa9-4fef-4260-9851-ed8424b2e72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_choices = ['NDBI', 'NDTI', 'NDSI', 'NDCI', 'GPP_Summer', 'gHM', \n",
    "            'Heat_Insolation', 'Topo_Diversity', 'Flashiness', 'LST_Summer',\n",
    "            'LST_Winter','NDVI','LST_Spring','LST_Fall', 'Precip_Winter', \n",
    "            'Precip_Spring', 'Precip_Summer', 'Precip_Fall', 'Drawdown', 'Runoff', 'geometry', 'predID']\n",
    "ref_data = sample_multiband_geotiff_with_names(homerange_raster, homerange_points)\n",
    "my_ref_data = filter_dataframe_columns(ref_data, feature_choices).dropna()\n",
    "my_pred_data = filter_dataframe_columns(my_pred_data, feature_choices).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cf78e4-3181-42be-a5ba-0cd62f6132c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_mess = MESS(my_ref_data, my_pred_data)\n",
    "my_mess_clean = my_mess.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f399d0-14ae-4d0a-98fd-6492ee0ca03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from scipy.interpolate import griddata\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "\n",
    "def interpolate_mess(mess_points_gdf, out_raster_path, res=100, method=\"nearest\", power=2):\n",
    "    \"\"\"\n",
    "    Interpolate MESS scores from points to a raster grid.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mess_points_gdf : GeoDataFrame\n",
    "        Must have columns 'geometry' (Point) and 'MESS'.\n",
    "    out_raster_path : str\n",
    "        Path to write GeoTIFF output.\n",
    "    res : float\n",
    "        Pixel resolution (map units).\n",
    "    method : str\n",
    "        Interpolation method: \"nearest\", \"linear\", \"cubic\", \"idw\", or \"kriging\".\n",
    "    power : float\n",
    "        Power parameter for IDW (default=2).\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- Extract coordinates and values ----\n",
    "    x = mess_points_gdf.geometry.x.values\n",
    "    y = mess_points_gdf.geometry.y.values\n",
    "    z = mess_points_gdf[\"MESS_Score\"].values\n",
    "\n",
    "    # ---- Define raster grid ----\n",
    "    xmin, ymin, xmax, ymax = mess_points_gdf.total_bounds\n",
    "    width = int((xmax - xmin) / res)\n",
    "    height = int((ymax - ymin) / res)\n",
    "\n",
    "    grid_x = np.linspace(xmin, xmax, width)\n",
    "    grid_y = np.linspace(ymin, ymax, height)\n",
    "    grid_xx, grid_yy = np.meshgrid(grid_x, grid_y)\n",
    "\n",
    "    if method == \"kriging\":\n",
    "        # Ordinary Kriging with default variogram\n",
    "        OK = OrdinaryKriging(\n",
    "            x, y, z,\n",
    "            variogram_model=\"spherical\",  # options: \"gaussian\", \"exponential\", \"linear\"\n",
    "            verbose=False,\n",
    "            enable_plotting=False\n",
    "        )\n",
    "        arr, _ = OK.execute(\"grid\", grid_x, grid_y)\n",
    "        arr = np.flipud(arr.data)  # flip y-axis for raster orientation\n",
    "\n",
    "    elif method == \"idw\":\n",
    "        # Inverse Distance Weighting\n",
    "        xi = grid_xx.flatten()\n",
    "        yi = grid_yy.flatten()\n",
    "        dist = np.sqrt((xi[:, None] - x[None, :])**2 + (yi[:, None] - y[None, :])**2)\n",
    "\n",
    "        # Avoid divide by zero (set very small distance)\n",
    "        dist[dist == 0] = 1e-10\n",
    "\n",
    "        weights = 1 / (dist ** power)\n",
    "        z_idw = np.sum(weights * z[None, :], axis=1) / np.sum(weights, axis=1)\n",
    "        arr = z_idw.reshape(grid_xx.shape)\n",
    "\n",
    "    else:\n",
    "        # Scipy griddata interpolation (nearest, linear, cubic)\n",
    "        arr = griddata(\n",
    "            (x, y), z, (grid_xx, grid_yy),\n",
    "            method=method\n",
    "        )\n",
    "\n",
    "    # ---- Raster transform ----\n",
    "    transform = from_origin(xmin, ymax, res, res)\n",
    "\n",
    "    # ---- Save as float32 raster ----\n",
    "    with rasterio.open(\n",
    "        out_raster_path,\n",
    "        \"w\",\n",
    "        driver=\"GTiff\",\n",
    "        height=arr.shape[0],\n",
    "        width=arr.shape[1],\n",
    "        count=1,\n",
    "        dtype=\"float32\",\n",
    "        crs=mess_points_gdf.crs,\n",
    "        transform=transform,\n",
    "        nodata=np.nan\n",
    "    ) as dst:\n",
    "        dst.write(arr.astype(\"float32\"), 1)\n",
    "\n",
    "    print(f\"‚úÖ Saved {method} interpolated MESS raster: {out_raster_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77f0172-5a83-49fe-a71c-48bad0ce2a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate_mess(my_mess_clean, training_path + training_state_abbr + '_homerange_sim.tif', res=1000, method=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a054054-e208-4aff-ba8b-5c454f9cd4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now upload all these predictor rasters to GEE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
