{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f571cc5-36da-4c2f-ae7f-1935516b9131",
   "metadata": {},
   "source": [
    "# User defined variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffc18a25-f28e-419e-adb0-99f9ea9048ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_state_abbr = 'MN' # State USPS abbreviation\n",
    "training_state_name = 'Minnesota' \n",
    "nas_id = 5 # European Frogbit = 1110; Eurasian watermilfoil = 237; Hydrilla verticillata = 6 (get these identifiers from https://nas.er.usgs.gov/api/v2/species)\n",
    "nas_name = 'ZM'\n",
    "# IA = 19; ID = 16; IL = 17; MN = 27; MO = 29; MT = 30; OR = 41;  WA = 53; WI = 55\n",
    "training_fip = 27\n",
    "# Replace last 2 digits with your state's FIP code\n",
    "training_path = 'my_data/' + training_state_abbr + '/' # leave this alone \n",
    "my_crs = \"EPSG:5070\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9b3dba6-25a5-4217-82a9-be1f5f8cf03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This version allows you to create predictors for in-state models where an AIS exist as well as a second set of predictors for \n",
    "predicting to your state if your AIS is not present. You only need to make the training predictors if you are training and predicting \n",
    "to the same state.  To predict to a different state you need to make predictors for that state too.\n",
    "'''\n",
    "#Import required Python packages. \n",
    "'''\n",
    "If you get an error here, search for the package name online + conda installation; open a new Anaconda PowerShell window;\n",
    "activate your environment and paste in the code you found online into the prompt.  It will most likely be: pip install \"package-name\" \n",
    "for Python packages or conda install conda-forge:: \"package-name\" for cross-platform packages (i.e., Scikit Learn).  \n",
    "'''\n",
    "\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import zipfile\n",
    "import random\n",
    "import io\n",
    "from io import StringIO, BytesIO\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "# Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from typing import Set, Tuple\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.mask import mask\n",
    "import rasterio.plot\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from affine import Affine\n",
    "import networkx as nx\n",
    "from shapely.geometry import Point, LineString, MultiLineString, MultiPoint\n",
    "from shapely.ops import snap, nearest_points\n",
    "from rasterio.transform import from_bounds\n",
    "from shapely.geometry import box\n",
    "from shapely.ops import unary_union\n",
    "from rasterio.merge import merge\n",
    "from rasterio.errors import RasterioIOError\n",
    "from joblib import Parallel, delayed\n",
    "# Machine Learning & Statistical Modeling\n",
    "import sklearn as skl\n",
    "from sklearn.neighbors import KDTree, KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import gaussian_process\n",
    "\n",
    "# Geostatistics & Interpolation\n",
    "import skgstat as skg\n",
    "import gstools as gs\n",
    "from skgstat import models\n",
    "from skgstat.util.likelihood import get_likelihood\n",
    "import scipy\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.spatial import distance_matrix\n",
    "from scipy.optimize import minimize\n",
    "from scipy.spatial import cKDTree\n",
    "from shapely.strtree import STRtree\n",
    "from scipy.interpolate import NearestNDInterpolator\n",
    "import scipy.ndimage\n",
    "import pykrige.kriging_tools as kt\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# APIs & Requests\n",
    "import requests\n",
    "from pygbif import occurrences as occ\n",
    "from pygbif import species\n",
    "from shapely import union_all\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from contextlib import contextmanager\n",
    "import joblib\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from multiprocessing import Manager, Lock\n",
    "import tempfile\n",
    "from shapely.geometry import CAP_STYLE\n",
    "from shapely.ops import unary_union, polygonize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cef442c4-0f8e-4b9f-8783-e00e153a5c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Water quality helper functions\n",
    "def get_usgs_stations_in_state(state_fip, state_abbr, my_path):\n",
    "    \"\"\"\n",
    "    Downloads USGS water monitoring station data and filters it to the specified state boundary.\n",
    "\n",
    "    Parameters:\n",
    "    - state_fip (str): State FIPS code in the form 'US:55'\n",
    "    - state_abbr (str): Two-letter state abbreviation (e.g., 'WI')\n",
    "    - my_path (str): Directory to save shapefile (must end with '/')\n",
    "\n",
    "    Returns:\n",
    "    - GeoDataFrame: Filtered monitoring stations within the state\n",
    "    \"\"\"\n",
    "    # === Step 1: Download USGS station data ===\n",
    "    print(\"üì• Downloading USGS station data...\")\n",
    "    url_base = 'https://www.waterqualitydata.us/data/Station/search?'\n",
    "    request_url = f\"{url_base}countrycode=US&statecode=US:{state_fip}\"\n",
    "    station_df = pd.read_csv(request_url)\n",
    "\n",
    "    stations = station_df[['MonitoringLocationIdentifier', 'LatitudeMeasure', 'LongitudeMeasure']].rename(columns={\n",
    "        'MonitoringLocationIdentifier': 'station_id',\n",
    "        'LatitudeMeasure': 'latitude',\n",
    "        'LongitudeMeasure': 'longitude'\n",
    "    })\n",
    "\n",
    "    stations_gdf = gpd.GeoDataFrame(\n",
    "        stations,\n",
    "        geometry=gpd.points_from_xy(stations.longitude, stations.latitude),\n",
    "        crs=\"EPSG:4326\"\n",
    "    ).to_crs(\"EPSG:5070\")\n",
    "\n",
    "    # === Step 2: Download state boundaries shapefile ===\n",
    "    print(\"üó∫Ô∏è  Downloading state boundary shapefile...\")\n",
    "    state_boundary_url = 'http://www2.census.gov/geo/tiger/TIGER2012/STATE/tl_2012_us_state.zip'\n",
    "    r = requests.get(state_boundary_url)\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    z.extractall(path=my_path)\n",
    "    \n",
    "    # Load shapefile\n",
    "    shp_path = os.path.join(my_path, [f for f in z.namelist() if f.endswith('.shp')][0])\n",
    "    state_boundary = gpd.read_file(shp_path).to_crs(\"EPSG:5070\")\n",
    "\n",
    "    # === Step 3: Filter to selected state ===\n",
    "    state = state_boundary[state_boundary['STUSPS'] == state_abbr]\n",
    "    if state.empty:\n",
    "        raise ValueError(f\"No state found for abbreviation '{state_abbr}'\")\n",
    "\n",
    "    # Match CRS and filter points within state\n",
    "    stations_gdf = stations_gdf.to_crs(state.crs)\n",
    "    filtered_stations = gpd.sjoin(\n",
    "        stations_gdf, state, how=\"inner\", predicate=\"within\"\n",
    "    ).drop(columns=[\"index_right\"])\n",
    "\n",
    "    # === Step 4: Save to file and return ===\n",
    "    out_file = os.path.join(my_path, f\"usgs_stations_{state_abbr}.shp\")\n",
    "    filtered_stations.to_file(out_file)\n",
    "    print(f\"‚úÖ Filtered station shapefile saved to: {out_file}\")\n",
    "\n",
    "    return filtered_stations\n",
    "\n",
    "\n",
    "\n",
    "def find_nearest(row, other_gdf):\n",
    "    # Find the index of the nearest geometry\n",
    "    nearest_idx = other_gdf.distance(row.geometry).idxmin()\n",
    "    return other_gdf.loc[nearest_idx]\n",
    "def add_nearest(gdf1, gdf2):\n",
    "    nearest_neighbors = gdf1.apply(lambda row: find_nearest(row, gdf2), axis=1)\n",
    "    # Add nearest neighbor information to the first GeoDataFrame\n",
    "    gdf1['nearest_id'] = nearest_neighbors['station_id']\n",
    "    gdf1['median'] = nearest_neighbors['median']\n",
    "    parameter_added = gdf1\n",
    "    return parameter_added\n",
    "def thin_geodataframe(gdf, min_dist=100):\n",
    "    \"\"\"\n",
    "    Spatially thin a GeoDataFrame using a minimum distance (in meters).\n",
    "    Ensures points are at least `min_dist` apart.\n",
    "    \"\"\"\n",
    "    if gdf.crs.to_epsg() != 5070:\n",
    "        gdf = gdf.to_crs(\"EPSG:5070\")\n",
    "\n",
    "    # Create mapping from geometry to index\n",
    "    geom_to_index = {id(geom): idx for idx, geom in zip(gdf.index, gdf.geometry)}\n",
    "    strtree = STRtree(gdf.geometry.values)\n",
    "\n",
    "    kept_geoms = []\n",
    "    taken_ids = set()\n",
    "\n",
    "    for geom in gdf.geometry:\n",
    "        if id(geom) in taken_ids:\n",
    "            continue\n",
    "\n",
    "        kept_geoms.append(geom)\n",
    "        buffered = geom.buffer(min_dist)\n",
    "        for neighbor in strtree.query(buffered):\n",
    "            taken_ids.add(id(neighbor))\n",
    "\n",
    "    return gpd.GeoDataFrame(geometry=kept_geoms, crs=gdf.crs)\n",
    "\n",
    "# Data acquisition & formating\n",
    "def download_wqp_csv(url, max_retries=3, backoff=5):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            with requests.get(url, stream=True, timeout=60) as r:\n",
    "                r.raise_for_status()\n",
    "                content = r.content  # read all bytes\n",
    "            return io.BytesIO(content)\n",
    "        except (requests.exceptions.RequestException, requests.exceptions.ChunkedEncodingError) as e:\n",
    "            print(f\"Download failed (attempt {attempt+1}/{max_retries}): {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Retrying in {backoff} seconds...\")\n",
    "                time.sleep(backoff)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "\n",
    "\n",
    "def get_water_quality(characteristic, state_fip, stations_gdf, my_path):\n",
    "    \"\"\"\n",
    "    Download, clean, summarize, and spatially join water quality data from WQP.\n",
    "\n",
    "    Parameters:\n",
    "    - characteristic (str): Water quality variable (e.g., 'pH', 'Calcium', 'Nitrogen', 'Phosphorus', 'Oxygen', 'Salinity', 'Temperature, water')\n",
    "    - state_fip (str): State FIPS code (e.g., '55' for Wisconsin)\n",
    "    - stations_gdf (GeoDataFrame): Monitoring stations to match to data\n",
    "    - my_path (str): Path to save output shapefile (include trailing slash)\n",
    "\n",
    "    Returns:\n",
    "    - GeoDataFrame: Stations with median value for the specified characteristic\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    \n",
    "    # === Step 1: Download data ===\n",
    "    print(f\"üì• Downloading {characteristic} data from WQP...\")\n",
    "    url_base = 'https://www.waterqualitydata.us/data/Result/search?'\n",
    "    request_url = f\"{url_base}countrycode=US&statecode=US:{state_fip}&characteristicName={characteristic}&mimeType=csv\"\n",
    "    csv_buffer = download_wqp_csv(request_url)\n",
    "    df = pd.read_csv(csv_buffer)\n",
    "    print(f\"‚úÖ Retrieved {len(df)} records\")\n",
    "\n",
    "    # === Step 2: Clean and rename ===\n",
    "    df = df.dropna(subset=['ResultMeasureValue'])\n",
    "    df['ResultMeasureValue'] = pd.to_numeric(df['ResultMeasureValue'], errors='coerce')\n",
    "    \n",
    "    df_clean = df[['CharacteristicName', 'ActivityStartDate', 'MonitoringLocationIdentifier',\n",
    "                   'ResultMeasureValue', 'ResultMeasure/MeasureUnitCode']].rename(columns={\n",
    "        \"CharacteristicName\": \"Characteristic\",\n",
    "        \"ActivityStartDate\": \"date\",\n",
    "        \"MonitoringLocationIdentifier\": \"station_id\",\n",
    "        \"ResultMeasureValue\": \"value\",\n",
    "        \"ResultMeasure/MeasureUnitCode\": \"unit\"\n",
    "    })\n",
    "\n",
    "    # === Step 3: Standardize units ===\n",
    "    df_clean['unit'] = df_clean['unit'].astype(str).str.strip().str.lower()\n",
    "\n",
    "    char_lower = characteristic.lower()\n",
    "    # Process Calcium, Nitrogen, Phosphorus as before\n",
    "    if char_lower in ['calcium', 'nitrogen', 'phosphorus']:\n",
    "        df_clean.loc[df_clean['unit'].isin(['ug/l', 'ppb', 'mg/g']), 'value'] /= 1000\n",
    "        df_clean['unit'] = df_clean['unit'].replace({\n",
    "            'mg/l': 'mg/l', 'mg/l as p': 'mg/l', 'mg/l po4': 'mg/l',\n",
    "            'mg/kg': 'mg/l', 'mg/kg as p': 'mg/l',\n",
    "            'ug/l': 'mg/l', 'ppb': 'mg/l', 'mg/g': 'mg/l'\n",
    "        })\n",
    "\n",
    "    elif char_lower == 'ph':\n",
    "        df_clean = df_clean[(df_clean['unit'].isna()) | (df_clean['unit'].isin(['std units', '', 'na']))]\n",
    "        df_clean['unit'] = 'std units'\n",
    "\n",
    "    elif char_lower == 'oxygen':\n",
    "        # Convert mg/L oxygen to % saturation using multiplier 12.67 (approximate)\n",
    "        df_clean.loc[df_clean['unit'] == 'mg/l', 'value'] *= 12.67\n",
    "        df_clean['unit'] = df_clean['unit'].replace({'mg/l': '% saturatn', '% by vol': '% saturatn'})\n",
    "\n",
    "    elif char_lower == 'salinity':\n",
    "        # Convert salinity to PSU (practical salinity units)\n",
    "        # Common units include ppt, PSU, mg/L (rare, then convert to ppt)\n",
    "        # 1 ppt ‚âà 1 PSU\n",
    "        # Convert mg/L to ppt by dividing by 1000 (assuming 1 ppt = 1000 mg/L)\n",
    "        # For simplicity, unify all to 'ppt'\n",
    "        # Handle common unit variants\n",
    "        def convert_salinity(row):\n",
    "            u = row['unit']\n",
    "            v = row['value']\n",
    "            if u in ['psu', 'ppt', 'parts per thousand', '‚Ä∞']:\n",
    "                return v\n",
    "            elif u in ['mg/l', 'mg/l as cl', 'mg/l as nacl']:\n",
    "                return v / 1000  # convert mg/L to ppt approx\n",
    "            elif u in ['g/l']:\n",
    "                return v * 1000  # g/L to ppt (1000 ppt per g/L)\n",
    "            else:\n",
    "                # unknown units, keep as is but warn\n",
    "                return v\n",
    "\n",
    "        df_clean['value'] = df_clean.apply(convert_salinity, axis=1)\n",
    "        df_clean['unit'] = 'ppt'\n",
    "\n",
    "    elif char_lower == 'temperature, water':\n",
    "        # Convert temperature to Celsius if needed\n",
    "        def convert_temp(row):\n",
    "            u = row['unit']\n",
    "            v = row['value']\n",
    "            if u in ['c', 'deg c', '¬∞c', 'celsius']:\n",
    "                return v\n",
    "            elif u in ['f', 'deg f', '¬∞f', 'fahrenheit']:\n",
    "                return (v - 32) * 5.0 / 9.0\n",
    "            elif u in ['k', 'kelvin']:\n",
    "                return v - 273.15\n",
    "            else:\n",
    "                # unknown units, keep as is but warn\n",
    "                return v\n",
    "\n",
    "        df_clean['value'] = df_clean.apply(convert_temp, axis=1)\n",
    "        df_clean['unit'] = 'c'\n",
    "\n",
    "    # === Step 4: Filter implausible values ===\n",
    "    if char_lower == 'calcium':\n",
    "        df_clean = df_clean[(df_clean['unit'] == 'mg/l') & (df_clean['value'].between(0, 300))]\n",
    "    elif char_lower == 'ph':\n",
    "        df_clean = df_clean[(df_clean['value'].between(4, 14))]\n",
    "    elif char_lower == 'nitrogen':\n",
    "        df_clean = df_clean[(df_clean['unit'] == 'mg/l') & (df_clean['value'].between(0, 500))]\n",
    "    elif char_lower == 'phosphorus':\n",
    "        df_clean = df_clean[(df_clean['unit'] == 'mg/l') & (df_clean['value'].between(0, 1))]\n",
    "    elif char_lower == 'oxygen':\n",
    "        df_clean = df_clean[(df_clean['unit'] == '% saturatn') & (df_clean['value'].between(0, 200))]\n",
    "    elif char_lower == 'salinity':\n",
    "        # plausible range for salinity in ppt (freshwater to ocean)\n",
    "        df_clean = df_clean[(df_clean['unit'] == 'ppt') & (df_clean['value'].between(0, 42))]\n",
    "    elif char_lower == 'temperature, water':\n",
    "        # plausible range in Celsius (e.g., -5 to 40 C)\n",
    "        df_clean = df_clean[(df_clean['unit'] == 'c') & (df_clean['value'].between(-5, 40))]\n",
    "\n",
    "    # === Step 5: Aggregate by station ===\n",
    "    summary = df_clean.groupby('station_id')['value'].median().reset_index()\n",
    "    summary.rename(columns={'value': 'median'}, inplace=True)\n",
    "\n",
    "    # === Step 6: Spatial join with training stations ===\n",
    "    stations_with = pd.merge(stations_gdf, summary, on='station_id', how='inner')\n",
    "    stations_missing = stations_gdf[~stations_gdf['station_id'].isin(stations_with['station_id'])]\n",
    "\n",
    "    # === Step 7: Fill missing via nearest station ===\n",
    "    print(f\"üîÑ Imputing missing {characteristic} values...\")\n",
    "    stations_filled = add_nearest(stations_missing, stations_with)\n",
    "    final_gdf = pd.concat([stations_with, stations_filled], axis=0).drop(columns=[\"nearest_id\", \"latitude\", \"longitude\"], errors='ignore')\n",
    "\n",
    "    # === Step 8: Save shapefile ===\n",
    "    out_name = f\"usgs_{characteristic.lower().replace(' ', '_').replace(',', '').replace('-', '_')}.shp\"\n",
    "    out_path = os.path.join(my_path, out_name)\n",
    "    final_gdf.to_file(out_path)\n",
    "    print(f\"‚úÖ {characteristic} data saved to: {out_path}\")\n",
    "\n",
    "    return final_gdf\n",
    "\n",
    "\n",
    "# Functions for making predictors\n",
    "def interpolate(stations_w_data, filename, bandname=\"Interpolated_Band\", pixel_size=100, use_kriging='auto'):\n",
    "    # Ensure CRS and extract coordinates\n",
    "    stations_w_data = stations_w_data.copy()\n",
    "    stations_w_data = stations_w_data.set_crs(\"EPSG:5070\")\n",
    "    stations_w_data['x'] = stations_w_data.geometry.x\n",
    "    stations_w_data['y'] = stations_w_data.geometry.y\n",
    "    samples_df = stations_w_data[['x', 'y', 'median']]\n",
    "    coords = np.column_stack((samples_df['x'], samples_df['y']))\n",
    "    vals = samples_df['median']\n",
    "\n",
    "    # Scatter plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    art = ax.scatter(samples_df['x'], samples_df['y'], s=10, c=vals, cmap='plasma', vmin=0, vmax=vals.max())\n",
    "    plt.colorbar(art)\n",
    "    plt.title(\"Original Data Points\")\n",
    "    plt.show()\n",
    "\n",
    "    # Define bounds and grid size\n",
    "    xmin, xmax = samples_df['x'].min(), samples_df['x'].max()\n",
    "    ymin, ymax = samples_df['y'].min(), samples_df['y'].max()\n",
    "\n",
    "    # Define grid size based on pixel resolution\n",
    "    ncols = int(np.ceil((xmax - xmin) / pixel_size))\n",
    "    nrows = int(np.ceil((ymax - ymin) / pixel_size))\n",
    "    xres = pixel_size\n",
    "    yres = pixel_size\n",
    "    \n",
    "    print(f\"üìè Pixel size: {xres} m x {yres} m\")\n",
    "    print(f\"üì¶ Grid dimensions: {ncols} cols √ó {nrows} rows\")\n",
    "    \n",
    "    # Smart fallback if 'auto' is used\n",
    "    if use_kriging == 'auto':\n",
    "        use_kriging = (ncols * nrows <= 1e6)\n",
    "    \n",
    "    # Interpolation block\n",
    "    if use_kriging:\n",
    "        try:\n",
    "            print(\"üîπ Attempting Ordinary Kriging interpolation...\")\n",
    "            maxlag = np.median(pdist(coords))\n",
    "            n_lags = min(30, max(10, int(len(coords) / 10)))\n",
    "            V = skg.Variogram(coords, vals, model='exponential', maxlag=maxlag, n_lags=n_lags, normalize=False)\n",
    "            ok = skg.OrdinaryKriging(V, min_points=1, max_points=8, mode='exact')\n",
    "    \n",
    "            grid_x = np.linspace(xmin, xmax, ncols)\n",
    "            grid_y = np.linspace(ymin, ymax, nrows)\n",
    "            xx, yy = np.meshgrid(grid_x, grid_y)\n",
    "    \n",
    "            field = ok.transform(xx.flatten(), yy.flatten()).reshape(xx.shape)\n",
    "            print(\"‚úÖ Kriging completed successfully.\")\n",
    "    \n",
    "        except (MemoryError, ValueError, np.linalg.LinAlgError):\n",
    "            print(\"‚ö†Ô∏è Kriging failed, switching to Nearest Neighbor interpolation...\")\n",
    "            use_kriging = False  # Fallback to KNN\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Skipping Kriging; using Nearest Neighbor interpolation...\")\n",
    "    \n",
    "    # KNN Interpolation (used either by default or after Kriging fails)\n",
    "    if not use_kriging:\n",
    "        grid_x = np.linspace(xmin, xmax, ncols)\n",
    "        grid_y = np.linspace(ymin, ymax, nrows)\n",
    "        xx, yy = np.meshgrid(grid_x, grid_y)\n",
    "    \n",
    "        knn = KNeighborsRegressor(n_neighbors=min(8, len(coords)), weights='distance', algorithm='kd_tree', n_jobs=-1)\n",
    "        knn.fit(coords, vals)\n",
    "        query_points = np.column_stack((xx.ravel(), yy.ravel()))\n",
    "        field = knn.predict(query_points).reshape(xx.shape)\n",
    "    \n",
    "        print(\"‚úÖ Nearest Neighbor interpolation completed.\")\n",
    "\n",
    "\n",
    "    # Fill NaNs\n",
    "    def fill_missing_values(field):\n",
    "        mask = np.isnan(field)\n",
    "        if np.all(mask):\n",
    "            raise ValueError(\"All values are NaN; interpolation cannot be performed.\")\n",
    "        nearest_indices = scipy.ndimage.distance_transform_edt(mask, return_distances=False, return_indices=True)\n",
    "        field[mask] = field[tuple(nearest_indices[i][mask] for i in range(field.ndim))]\n",
    "        return field\n",
    "\n",
    "    field = fill_missing_values(field)\n",
    "\n",
    "    # Prepare raster\n",
    "    arr = np.flip(field, axis=0).astype(np.float32)\n",
    "    transform = from_origin(xmin, ymax, xres, yres)\n",
    "\n",
    "    with rasterio.open(filename, 'w', driver='GTiff',\n",
    "                       height=arr.shape[0], width=arr.shape[1],\n",
    "                       count=1, dtype=arr.dtype,\n",
    "                       crs=\"EPSG:5070\", transform=transform) as dst:\n",
    "        dst.write(arr, 1)\n",
    "        dst.set_band_description(1, bandname)\n",
    "\n",
    "    print(f\"‚úÖ Raster saved: {filename}\")\n",
    "\n",
    "    # Plot final raster\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(arr, extent=[xmin, xmax, ymin, ymax], cmap='plasma', origin=\"upper\")\n",
    "    plt.colorbar(label=bandname)\n",
    "    plt.title(\"Interpolated Raster\")\n",
    "    plt.xlabel(\"X Coordinate\")\n",
    "    plt.ylabel(\"Y Coordinate\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def combine_geotiffs(input_files, output_file):\n",
    "    \"\"\"\n",
    "    Combine single- or multi-band GeoTIFFs into one multi-band GeoTIFF,\n",
    "    preserving band names where available.\n",
    "\n",
    "    Parameters:\n",
    "        input_files (list): List of input raster file paths.\n",
    "        output_file (str): Path to output multi-band GeoTIFF.\n",
    "        state (str): Optional metadata tag (not used in core logic).\n",
    "    \"\"\"\n",
    "    all_bands = []\n",
    "    band_descriptions = []\n",
    "    meta = None\n",
    "\n",
    "    for file in input_files:\n",
    "        try:\n",
    "            with rasterio.open(file) as src:\n",
    "                if meta is None:\n",
    "                    meta = src.meta.copy()\n",
    "                    meta.update({\n",
    "                        'count': 0,  # Will be updated after reading all bands\n",
    "                        'dtype': src.dtypes[0]  # Assumes consistent dtype\n",
    "                    })\n",
    "\n",
    "                for bidx in range(1, src.count + 1):\n",
    "                    band_data = src.read(bidx)\n",
    "                    all_bands.append(band_data)\n",
    "\n",
    "                    # Try to get band description (e.g., band name)\n",
    "                    desc = src.descriptions[bidx - 1] or f'band_{bidx}'\n",
    "                    band_descriptions.append(desc)\n",
    "\n",
    "        except RasterioIOError:\n",
    "            print(f\"‚ö†Ô∏è Warning: Could not read {file}, skipping...\")\n",
    "\n",
    "    if not all_bands:\n",
    "        raise RuntimeError(\"‚ùå No valid input rasters found.\")\n",
    "\n",
    "    meta['count'] = len(all_bands)\n",
    "\n",
    "    with rasterio.open(output_file, 'w', **meta) as dst:\n",
    "        for idx, band in enumerate(all_bands, start=1):\n",
    "            dst.write(band, idx)\n",
    "            if band_descriptions[idx - 1]:\n",
    "                dst.set_band_description(idx, band_descriptions[idx - 1])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b791c42-4834-470c-8822-f566d061f157",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "‚ö†Ô∏è  Note on Warnings:\n",
    "Some warnings may appear during this script. These warnings do not indicate pipeline failure and can be safely ignored unless followed by an error.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca726f14-f463-46f8-9222-44cf017372f8",
   "metadata": {},
   "source": [
    "# Start of Water Quality predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ec64d6-9d31-4ac3-8e7c-932c56fda9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_stations_gdf = get_usgs_stations_in_state(\n",
    "    state_fip= training_fip,      \n",
    "    state_abbr= training_state_abbr,\n",
    "    my_path= training_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266ef9a8-ee47-48e7-b706-09fe339c1812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcium\n",
    "training_ca = get_water_quality(\"Calcium\", state_fip=training_fip, stations_gdf=training_stations_gdf, my_path=training_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f991fd1a-3fdb-46e1-917e-3a6be419f53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pH\n",
    "training_pH= get_water_quality(\"pH\", state_fip=training_fip, stations_gdf=training_stations_gdf, my_path=training_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fa2053-0ad3-41be-9496-f6d05b036e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dissolved Oxygen\n",
    "training_DO = get_water_quality('Oxygen', state_fip=training_fip, stations_gdf=training_stations_gdf, my_path=training_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e549808a-df07-4d91-8210-874023bf2b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nitrogen\n",
    "training_N = get_water_quality(\"Nitrogen\", state_fip=training_fip, stations_gdf=training_stations_gdf, my_path=training_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60b64d5-1d3b-419b-8017-5f8501b7ffff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phosphorus\n",
    "training_Phos = get_water_quality(\"Phosphorus\", state_fip=training_fip, stations_gdf=training_stations_gdf, my_path=training_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2a8304-f4bf-49d8-9151-9016b33782ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salinity\n",
    "training_Salinity = get_water_quality('Salinity', state_fip=training_fip, stations_gdf=training_stations_gdf, my_path=training_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d0682e-0d92-4867-89bc-eb919b22946b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Water Temperature\n",
    "training_Temp = get_water_quality('Temperature', state_fip=training_fip, stations_gdf=training_stations_gdf, my_path=training_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d10750f-473e-4d16-9335-13702b340562",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate(training_ca, training_path + training_state_abbr + '_ca.tif', 'Ca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f62f2b-8f7a-4595-b90a-9b9c39cc5331",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate(training_pH, training_path + training_state_abbr + '_pH.tif', 'pH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ebd38d-ab62-4dfd-b26d-013b984c8324",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate(training_N, training_path + training_state_abbr + '_N.tif', 'Nitrogen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6833a24d-7038-4b02-9d3b-94f40d9b7771",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate(training_DO, training_path + training_state_abbr + '_do.tif', 'DO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cce07c-fcda-4652-befb-ad4f1cd729a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate(training_Phos, training_path + training_state_abbr + '_phos.tif', 'Phos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5219e3-db6c-49a2-95ed-b25ac40fd1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate(training_Salinity, training_path + training_state_abbr + '_salinity.tif', 'Salinity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59f4d9e-fa54-4abd-97be-4952c6b4ef46",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate(training_Temp, training_path + training_state_abbr + '_temp.tif', 'Temperature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d5fd19-d150-4034-9825-641a393420f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the predictors you just created and export a final combined raster \n",
    "# Define input files\n",
    "input_files_wq_training = [\n",
    "    training_path + training_state_abbr + \"_ca.tif\", training_path + training_state_abbr + \"_pH.tif\", training_path + training_state_abbr + \"_N.tif\",\n",
    "    training_path + training_state_abbr + \"_do.tif\", training_path + training_state_abbr + \"_phos.tif\"]\n",
    "#Combine rasters into a multi-band GeoTIFF\n",
    "output_file_wq_training = training_path + training_state_abbr + \"_combined_wq.tif\" # this will replace the other combined tif you made.\n",
    "combine_geotiffs(input_files_wq_training, output_file_wq_training)\n",
    "\n",
    "print(f\"‚úÖ Multi-band raster saved as {output_file_wq_training}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c13a2da-3989-412c-a3a8-fc4948c84adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nhd_waterbodies(state_name, local_path, output_prefix=None, save_files=True,\n",
    "                         make_background_water=False, training_path=None, training_state_abbr=None):\n",
    "    \"\"\"\n",
    "    Downloads, extracts, and loads NHD shapefiles for a U.S. state, returning lakes, rivers, and flowlines.\n",
    "    \"\"\"\n",
    "    if output_prefix is None:\n",
    "        output_prefix = state_name.lower()\n",
    "\n",
    "    os.makedirs(local_path, exist_ok=True)\n",
    "\n",
    "    # Use GPKG filepaths for caching\n",
    "    lakes_gpkg = os.path.join(local_path, f\"{output_prefix}_lakes.gpkg\")\n",
    "    rivers_gpkg = os.path.join(local_path, f\"{output_prefix}_rivers.gpkg\")\n",
    "    streams_gpkg = os.path.join(local_path, f\"{output_prefix}_streams.gpkg\")\n",
    "\n",
    "    if save_files and all(os.path.exists(f) for f in [lakes_gpkg, rivers_gpkg, streams_gpkg]):\n",
    "        print(\"üìÇ Found existing GPKG files. Loading from disk...\")\n",
    "        lakes = gpd.read_file(lakes_gpkg)\n",
    "        rivers = gpd.read_file(rivers_gpkg)\n",
    "        streams = gpd.read_file(streams_gpkg)\n",
    "    else:\n",
    "        # === Step 1‚Äì5: Download, extract, load ===\n",
    "        url = f\"https://prd-tnm.s3.amazonaws.com/StagedProducts/Hydrography/NHD/State/Shape/NHD_H_{state_name}_State_Shape.zip\"\n",
    "        print(f\"üì• Downloading NHD data for {state_name}...\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"‚ùå Failed to download NHD data for {state_name}\")\n",
    "\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".zip\") as tmp_file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                tmp_file.write(chunk)\n",
    "\n",
    "        with zipfile.ZipFile(tmp_file.name, 'r') as z:\n",
    "            z.extractall(path=local_path)\n",
    "        print(\"‚úÖ Extraction complete.\")\n",
    "\n",
    "        shp_files = glob.glob(os.path.join(local_path, \"**\", \"*.shp\"), recursive=True)\n",
    "        if not shp_files:\n",
    "            raise FileNotFoundError(\"‚ùå No shapefiles found in extracted directory.\")\n",
    "\n",
    "        lakes_path = next((f for f in shp_files if \"nhdwaterbody\" in f.lower()), None)\n",
    "        if not lakes_path:\n",
    "            raise FileNotFoundError(\"‚ùå No NHDWaterbody shapefile found.\")\n",
    "        lakes = gpd.read_file(lakes_path)\n",
    "\n",
    "        rivers_path = next((f for f in shp_files if \"nhdarea\" in f.lower()), None)\n",
    "        if not rivers_path:\n",
    "            raise FileNotFoundError(\"‚ùå No NHDArea shapefile found.\")\n",
    "        rivers = gpd.read_file(rivers_path)\n",
    "\n",
    "        print(\"üìÑ Loading and combining stream flowlines...\")\n",
    "        flowline_paths = [f for f in shp_files if \"nhdflowline\" in f.lower()]\n",
    "        if not flowline_paths:\n",
    "            raise FileNotFoundError(\"‚ùå No NHDFlowline shapefiles found.\")\n",
    "        stream_dfs = [gpd.read_file(fp) for fp in flowline_paths]\n",
    "        streams = gpd.GeoDataFrame(pd.concat(stream_dfs, ignore_index=True), crs=stream_dfs[0].crs)\n",
    "\n",
    "        if save_files:\n",
    "            print(\"üíæ Saving as GeoPackage files...\")\n",
    "            lakes.to_file(lakes_gpkg, driver=\"GPKG\")\n",
    "            rivers.to_file(rivers_gpkg, driver=\"GPKG\")\n",
    "            streams.to_file(streams_gpkg, driver=\"GPKG\")\n",
    "\n",
    "    # ‚úÖ Now return regardless of source (cached or new)\n",
    "    result = {\n",
    "        \"lakes\": lakes,\n",
    "        \"rivers\": rivers,\n",
    "        \"streams\": streams\n",
    "    }\n",
    "\n",
    "    if make_background_water and training_path and training_state_abbr:\n",
    "        bg_water = pd.concat([lakes, rivers], ignore_index=True)\n",
    "        output_file = os.path.join(training_path, f\"{training_state_abbr}_bg_water.shp\")\n",
    "        bg_water.to_file(output_file)\n",
    "        result[\"bg_water\"] = bg_water\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def buffer_water_layers(\n",
    "    streams: gpd.GeoDataFrame = None,\n",
    "    lakes: gpd.GeoDataFrame = None,\n",
    "    rivers: gpd.GeoDataFrame = None,\n",
    "    simplify_tolerance: float = None,\n",
    "    filter_streams: bool = True,\n",
    "    save_path: str = None,\n",
    "    use_cached: bool = True,\n",
    "    export_merged_filename: str = None\n",
    ") -> tuple[gpd.GeoDataFrame, gpd.GeoDataFrame, gpd.GeoDataFrame, gpd.GeoDataFrame, gpd.GeoDataFrame]:\n",
    "    \"\"\"\n",
    "    Buffer and merge water layers, optionally using cached files.\n",
    "    \"\"\"\n",
    "\n",
    "    crs = \"EPSG:5070\"\n",
    "\n",
    "    # Define cache paths\n",
    "    if save_path:\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        stream_file = os.path.join(save_path, \"buffered_streams.gpkg\")\n",
    "        lake_file = os.path.join(save_path, \"buffered_lakes.gpkg\")\n",
    "        river_file = os.path.join(save_path, \"buffered_rivers.gpkg\")\n",
    "        full_stream_file = os.path.join(save_path, \"full_streams.shp\")\n",
    "\n",
    "        cache_exists = all(os.path.exists(f) for f in [stream_file, lake_file, river_file, full_stream_file])\n",
    "        if use_cached and cache_exists:\n",
    "            print(\"üìÇ Loading buffered layers from cache...\")\n",
    "            buffered_streams = gpd.read_file(stream_file)\n",
    "            buffered_lakes = gpd.read_file(lake_file)\n",
    "            buffered_rivers = gpd.read_file(river_file)\n",
    "            full_streams = gpd.read_file(full_stream_file)\n",
    "\n",
    "            buffered_water = pd.concat([buffered_streams, buffered_lakes, buffered_rivers], ignore_index=True)\n",
    "            buffered_water = buffered_water[buffered_water.geometry.notnull()]\n",
    "            buffered_water[\"waterID\"] = range(1, len(buffered_water) + 1)\n",
    "\n",
    "            if export_merged_filename and not os.path.exists(export_merged_filename):\n",
    "                print(f\"üíæ Merging and exporting combined buffered layer to {export_merged_filename}...\")\n",
    "                buffered_water.to_file(export_merged_filename)\n",
    "\n",
    "            return buffered_streams, buffered_lakes, buffered_rivers, full_streams, buffered_water\n",
    "\n",
    "    # If no cache or not using cache, we require streams/lakes/rivers\n",
    "    if streams is None or lakes is None or rivers is None:\n",
    "        raise ValueError(\"streams, lakes, and rivers must be provided if not using cached files.\")\n",
    "\n",
    "    # === Process from scratch ===\n",
    "    print(\"[1/6] Projecting all layers...\")\n",
    "    streams = streams.to_crs(crs)\n",
    "    lakes = lakes.to_crs(crs)\n",
    "    rivers = rivers.to_crs(crs)\n",
    "    full_streams = streams.copy()\n",
    "\n",
    "    print(\"[2/6] Filtering lakes and rivers with areasqkm ‚â• 0.25...\")\n",
    "    lakes = lakes[lakes.is_valid & lakes.geometry.notnull() & (lakes[\"areasqkm\"] >= 0.25)]\n",
    "    rivers = rivers[rivers.is_valid & rivers.geometry.notnull() & (rivers[\"areasqkm\"] >= 0.25)]\n",
    "\n",
    "    print(\"[3/6] Preparing stream geometry...\")\n",
    "    # Drop invalid or missing geometries before simplification\n",
    "    filtered_streams = streams.copy()\n",
    "    filtered_streams = filtered_streams[filtered_streams.geometry.notnull()].copy()\n",
    "    filtered_streams = filtered_streams[filtered_streams.is_valid].copy()\n",
    "    \n",
    "    # Now apply simplification\n",
    "    if simplify_tolerance is not None:\n",
    "        print(f\"[4/6] Simplifying stream geometries with tolerance {simplify_tolerance}...\")\n",
    "        filtered_streams[\"geometry\"] = filtered_streams.geometry.simplify(\n",
    "            tolerance=simplify_tolerance, preserve_topology=True)\n",
    "    else:\n",
    "        print(\"[4/6] Skipping simplification.\")\n",
    "\n",
    "    print(\"[5/6] Buffering lakes and rivers...\")\n",
    "    lakes[\"geometry\"] = lakes.geometry.buffer(50)\n",
    "    rivers[\"geometry\"] = rivers.geometry.buffer(50)\n",
    "\n",
    "    print(\"[6/6] Buffering stream geometries...\")\n",
    "    filtered_streams[\"geometry\"] = filtered_streams.geometry.buffer(50, cap_style=CAP_STYLE.flat)\n",
    "\n",
    "    # Optional save\n",
    "    if save_path:\n",
    "        print(\"üíæ Saving buffered layers...\")\n",
    "        filtered_streams.to_file(stream_file, driver=\"GPKG\")\n",
    "        lakes.to_file(lake_file, driver=\"GPKG\")\n",
    "        rivers.to_file(river_file, driver=\"GPKG\")\n",
    "        full_streams.to_file(full_stream_file)\n",
    "\n",
    "    buffered_water = pd.concat([filtered_streams, lakes, rivers], ignore_index=True)\n",
    "    buffered_water = buffered_water[buffered_water.geometry.notnull()]\n",
    "    buffered_water['waterID'] = range(1, len(buffered_water) + 1)\n",
    "\n",
    "    if export_merged_filename:\n",
    "        print(f\"üß± Merging and exporting combined buffered layer to {export_merged_filename}...\")\n",
    "        buffered_water.to_file(export_merged_filename)\n",
    "\n",
    "    return filtered_streams, lakes, rivers, full_streams, buffered_water\n",
    "# Biological predictor and training/background data helper functions\n",
    "def make_bg_data(\n",
    "    waterbody_gdf,\n",
    "    training_path,\n",
    "    training_state_abbr,\n",
    "    nas_name,\n",
    "    seed=42,\n",
    "    max_attempts=100,\n",
    "    thin=True,\n",
    "    min_dist=1000\n",
    "):\n",
    "    target_crs = \"EPSG:5070\"\n",
    "    \n",
    "    # Auto-load presence data\n",
    "    pos_data_path = f\"{training_path}{training_state_abbr}_{nas_name}_pos_data.shp\"\n",
    "    species_gdf_thin = gpd.read_file(pos_data_path)\n",
    "\n",
    "    # Reproject everything if needed\n",
    "    if waterbody_gdf.crs != target_crs:\n",
    "        waterbody_gdf = waterbody_gdf.to_crs(target_crs)\n",
    "    if species_gdf_thin.crs != target_crs:\n",
    "        species_gdf_thin = species_gdf_thin.to_crs(target_crs)\n",
    "\n",
    "    # Filter waterbodies by area\n",
    "    if 'areasqkm' not in waterbody_gdf.columns:\n",
    "        raise ValueError(\"Expected 'areasqkm' column in waterbody_gdf\")\n",
    "    waterbody_gdf = waterbody_gdf[waterbody_gdf['areasqkm'] > 0.25].copy()\n",
    "\n",
    "    presence_points = species_gdf_thin[species_gdf_thin['Present'] == 1]\n",
    "    sindex = presence_points.sindex\n",
    "\n",
    "    def has_presence(geom):\n",
    "        if geom.is_empty or not geom.is_valid:\n",
    "            return True\n",
    "        bounds = geom.bounds\n",
    "        candidates = list(sindex.intersection(bounds))\n",
    "        return any(presence_points.iloc[i].geometry.intersects(geom) for i in candidates)\n",
    "\n",
    "    non_ais_waterbodies = waterbody_gdf[~waterbody_gdf['geometry'].apply(has_presence)].copy()\n",
    "\n",
    "    random.seed(seed)\n",
    "    bg_points = []\n",
    "    for geom in non_ais_waterbodies.geometry.values:\n",
    "        minx, miny, maxx, maxy = geom.bounds\n",
    "        for _ in range(max_attempts):\n",
    "            x = random.uniform(minx, maxx)\n",
    "            y = random.uniform(miny, maxy)\n",
    "            pt = Point(x, y)\n",
    "            if geom.contains(pt):\n",
    "                bg_points.append(pt)\n",
    "                break\n",
    "\n",
    "    bg_gdf = gpd.GeoDataFrame(geometry=bg_points, crs=target_crs)\n",
    "\n",
    "    if thin and not bg_gdf.empty:\n",
    "        bg_gdf = thin_geodataframe(bg_gdf, min_dist=min_dist)\n",
    "\n",
    "    bg_gdf['Present'] = 0\n",
    "\n",
    "    # Save to shapefile\n",
    "    output_path = f\"{training_path}{training_state_abbr}_{nas_name}_bg_data.shp\"\n",
    "    bg_gdf.to_file(output_path)\n",
    "\n",
    "    return bg_gdf\n",
    "\n",
    "def process_nas_occurrences(\n",
    "    state,\n",
    "    crs,\n",
    "    path,\n",
    "    target_species_id=None,\n",
    "    target_species_name=None,\n",
    "    buffer_shapefile_suffix=\"_buffered_water.shp\",\n",
    "    species_columns=None,\n",
    "    snap_tolerance=100,\n",
    "    make_background=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Downloads, filters, spatially joins, and pivots NAS invasive species data.\n",
    "\n",
    "    Parameters:\n",
    "        state (str): US state abbreviation (e.g., 'MN')\n",
    "        crs (str or int): Target CRS (e.g., 'EPSG:5070')\n",
    "        path (str): Path to save/load shapefiles (must end with '/')\n",
    "        target_species_id (str, optional): SpeciesID to filter for a single species\n",
    "        target_species_name (str, optional): Common name to include in filename if target_species_id is used\n",
    "        buffer_shapefile_suffix (str): Suffix for the buffered waterbodies shapefile\n",
    "        species_columns (list): List of invasive group column names\n",
    "        snap_tolerance (int): Max distance for nearest spatial join in meters\n",
    "        make_background (bool): Whether to generate and save background points\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame: Final dataframe with invasive species richness per lake\n",
    "    \"\"\"\n",
    "    if species_columns is None:\n",
    "        species_columns = ['Inv_Algae', 'Inv_Crustaceans', 'Inv_Fish', 'Inv_Mollusks', 'Inv_Plants']\n",
    "\n",
    "    # Step 1: Call NAS API\n",
    "    url = f\"http://nas.er.usgs.gov/api/v2/occurrence/search?state={state}\"\n",
    "    response = requests.get(url, timeout=None).json()\n",
    "    results = pd.json_normalize(response, 'results')\n",
    "\n",
    "    # Step 2: Filter and clean\n",
    "    all_nas_data = results[results['status'] == 'established'].dropna()\n",
    "    all_nas_data = all_nas_data[[\"speciesID\", \"commonName\", \"group\", \"decimalLatitude\", \"decimalLongitude\"]]\n",
    "\n",
    "    # Step 3: Create GeoDataFrame and save\n",
    "    nas_gdf = gpd.GeoDataFrame(\n",
    "        all_nas_data,\n",
    "        geometry=gpd.points_from_xy(all_nas_data.decimalLongitude, all_nas_data.decimalLatitude),\n",
    "        crs=\"EPSG:4326\"\n",
    "    ).to_crs(crs).drop(columns=[\"decimalLatitude\", \"decimalLongitude\"])\n",
    "\n",
    "    species_gdf = nas_gdf[nas_gdf[\"speciesID\"] == target_species_id]\n",
    "    species_gdf_thin = thin_geodataframe(species_gdf, min_dist=100)\n",
    "    species_gdf_thin['Present'] = 1\n",
    "    species_filename = f\"{path}{state}_{target_species_name}_pos_data.shp\"\n",
    "    species_gdf_thin.to_file(species_filename)\n",
    "\n",
    "    nas_gdf = nas_gdf[nas_gdf[\"speciesID\"] != target_species_id]\n",
    "    nas_gdf.to_file(f\"{path}{state}_nas.shp\")\n",
    "\n",
    "    nas_gdf = gpd.read_file(f\"{path}{state}_nas.shp\").to_crs(crs)\n",
    "    buffered_water = gpd.read_file(f\"{path}{state}{buffer_shapefile_suffix}\").to_crs(crs)\n",
    "\n",
    "    NAS_ais_obs_df = gpd.sjoin_nearest(nas_gdf[['speciesID', 'commonName', 'group', 'geometry']],\n",
    "                                       buffered_water, how=\"inner\", max_distance=snap_tolerance)\n",
    "\n",
    "    NAS_ais_obs_df['group'] = NAS_ais_obs_df['group'].replace({\n",
    "        'Algae': 'Inv_Algae',\n",
    "        'Plants': 'Inv_Plants',\n",
    "        'Fishes': 'Inv_Fish',\n",
    "        'Crustaceans-Cladocerans': 'Inv_Crustaceans',\n",
    "        'Crustaceans-Amphipods': 'Inv_Crustaceans',\n",
    "        'Mollusks-Bivalves': 'Inv_Mollusks',\n",
    "        'Mollusks-Gastropods': 'Inv_Mollusks'\n",
    "    })\n",
    "\n",
    "    grouped = NAS_ais_obs_df[['waterID', 'commonName', 'group']].groupby(['waterID', 'group'])['commonName'].nunique().reset_index()\n",
    "    pivot_df = grouped.pivot(index='waterID', columns='group', values='commonName').reset_index().fillna(0)\n",
    "    lakes_w_invasives = pd.merge(buffered_water, pivot_df, on='waterID', how='left')\n",
    "\n",
    "    for col in species_columns:\n",
    "        if col not in lakes_w_invasives.columns:\n",
    "            lakes_w_invasives[col] = 0.0\n",
    "        else:\n",
    "            lakes_w_invasives[col] = lakes_w_invasives[col].astype('float64')\n",
    "\n",
    "    inv_rich = lakes_w_invasives[species_columns + ['geometry']].copy()\n",
    "    inv_gdf = gpd.GeoDataFrame(inv_rich, geometry='geometry', crs=lakes_w_invasives.crs)\n",
    "\n",
    "    # Step 11: Optional background generation\n",
    "    if make_background:\n",
    "        make_bg_data(\n",
    "            waterbody_gdf=buffered_water,\n",
    "            training_path=path,\n",
    "            training_state_abbr=state,\n",
    "            nas_name=target_species_name,\n",
    "            seed=42,\n",
    "            max_attempts=100,\n",
    "            thin=True,\n",
    "            min_dist=1000\n",
    "        )\n",
    "\n",
    "    return inv_gdf\n",
    "\n",
    "\n",
    "def export_inv_richness(inv_rich_gdf, output_path):\n",
    "    # Convert input data to EPSG:5070 BEFORE getting bounds\n",
    "    inv_rich_gdf = inv_rich_gdf\n",
    "\n",
    "    data_columns = ['Inv_Algae', 'Inv_Crustaceans', 'Inv_Fish', 'Inv_Mollusks', 'Inv_Plants']\n",
    "\n",
    "    # Ensure valid geometry\n",
    "    inv_rich_gdf = inv_rich_gdf[inv_rich_gdf.geometry.notnull() & inv_rich_gdf.geometry.is_valid]\n",
    "\n",
    "    # Ensure all required columns exist\n",
    "    for col in data_columns:\n",
    "        if col not in inv_rich_gdf.columns:\n",
    "            inv_rich_gdf[col] = 0\n",
    "    \n",
    "    # Warn if file exists\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"Warning: {output_path} already exists and will be overwritten.\")\n",
    "\n",
    "    if inv_rich_gdf.empty:\n",
    "        raise ValueError(\"Error: The input GeoDataFrame is empty!\")\n",
    "\n",
    "    # Get bounding box in CRS 5070\n",
    "    xmin, ymin, xmax, ymax = inv_rich_gdf.total_bounds  \n",
    "    pixel_size = 100 \n",
    "\n",
    "    # Ensure bounds are valid\n",
    "    if xmin == xmax:\n",
    "        xmin -= pixel_size\n",
    "        xmax += pixel_size\n",
    "    if ymin == ymax:\n",
    "        ymin -= pixel_size\n",
    "        ymax += pixel_size\n",
    "\n",
    "    # Recalculate width and height\n",
    "    width = max(1, int((xmax - xmin) / pixel_size))\n",
    "    height = max(1, int((ymax - ymin) / pixel_size))\n",
    "\n",
    "    # Correct transform in CRS 5070\n",
    "    transform = Affine(pixel_size, 0, xmin, 0, -pixel_size, ymax)\n",
    "    num_bands = len(data_columns) + 1  # Extra band for sum\n",
    "    raster = np.zeros((num_bands, height, width), dtype=np.float32)\n",
    "\n",
    "    # Debug: Check valid geometries\n",
    "    valid_geom_count = inv_rich_gdf.geometry.notnull().sum()\n",
    "    print(f\"Valid geometries count: {valid_geom_count}\")\n",
    "\n",
    "    # Rasterize each data column\n",
    "    for i, column in enumerate(data_columns):\n",
    "        shapes = [(geom, value) for geom, value in zip(inv_rich_gdf.geometry, inv_rich_gdf[column]) if geom is not None and not np.isnan(value)]\n",
    "        if not shapes:\n",
    "            print(f\"Skipping {column}: No valid geometries!\")\n",
    "            continue  # Skip empty bands\n",
    "        raster_band = rasterize(\n",
    "            shapes,\n",
    "            out_shape=(height, width),\n",
    "            transform=transform,\n",
    "            fill=0,\n",
    "            dtype='float32',\n",
    "            default_value=1,  # Ensure full coverage of waterbody\n",
    "            all_touched=True  # Ensures every pixel within the geometry gets a value\n",
    "        )\n",
    "        shapes = [(geom, value) for geom, value in zip(inv_rich_gdf.geometry, inv_rich_gdf[column])]\n",
    "        raster[i] = raster_band  # Keep original values\n",
    "        print(f\"Rasterized {column}: Min={raster_band.min()}, Max={raster_band.max()}, Non-zero pixels={np.count_nonzero(raster_band)}\")\n",
    "\n",
    "    # Compute the sum band\n",
    "    raster[-1] = np.sum(raster[:-1], axis=0)\n",
    "    print(f\"Final Sum Band: Min={raster[-1].min()}, Max={raster[-1].max()}, Non-zero pixels={np.count_nonzero(raster[-1])}\")\n",
    "\n",
    "    # Save as a multi-band GeoTIFF in EPSG:5070\n",
    "    with rasterio.open(\n",
    "        output_path, 'w',\n",
    "        driver='GTiff',\n",
    "        height=height,\n",
    "        width=width,\n",
    "        count=num_bands,\n",
    "        dtype=raster.dtype,\n",
    "        crs=\"EPSG:5070\",\n",
    "        transform=transform\n",
    "    ) as dst:\n",
    "        for band in range(num_bands):\n",
    "            dst.write(raster[band], band + 1)\n",
    "        band_names = data_columns + [\"Inv_Richness\"]\n",
    "        for band, name in enumerate(band_names, start=1):\n",
    "            dst.set_band_description(band, name)\n",
    "\n",
    "    # Debug: Plot overlay of geometries on raster\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.imshow(raster[-1], cmap='viridis', extent=[xmin, xmax, ymin, ymax], origin=\"upper\")\n",
    "    gpd.GeoSeries(inv_rich_gdf.geometry).plot(ax=ax, facecolor=\"none\", edgecolor=\"red\")\n",
    "    plt.title(\"Overlaying Geometries on Raster\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot bands\n",
    "    fig, axes = plt.subplots(1, num_bands, figsize=(15, 5))\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(raster[i], cmap='viridis')\n",
    "        ax.set_title(band_names[i])\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Raster file saved as '{output_path}' in CRS {\"EPSG:5070\"} with band names: {band_names}\")\n",
    "\n",
    "def process_and_export_native_fish_raster(\n",
    "    my_path: str,\n",
    "    my_crs: str,\n",
    "    state_abbr: str,\n",
    "    resolution: int = 100,\n",
    "    fish_shapefile: str = 'usgs_native_fish_rich.shp',\n",
    "    state_shapefile: str = 'tl_2012_us_state.shp',\n",
    "    buffer_shapefile_suffix: str = '_buffered_water.shp'\n",
    "):\n",
    "    def clip_points_by_polygon(points_gdf, polygon_gdf):\n",
    "        if points_gdf.crs != polygon_gdf.crs:\n",
    "            points_gdf = points_gdf.to_crs(polygon_gdf.crs)\n",
    "        clipped_points = gpd.sjoin(points_gdf, polygon_gdf, how='inner')\n",
    "        return clipped_points.drop(columns=polygon_gdf.columns.difference(['geometry']))\n",
    "\n",
    "    def sum_numeric_columns(gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "        sum_columns = [col for col in gdf.columns if col.isdigit() and len(col) == 6]\n",
    "        gdf[\"Native_Fish_Richness\"] = gdf[sum_columns].sum(axis=1)\n",
    "        return gdf[[\"Native_Fish_Richness\", \"geometry\"]]\n",
    "\n",
    "    def spatial_join_with_nearest(poly_gdf: gpd.GeoDataFrame, point_gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "        if poly_gdf.crs != point_gdf.crs:\n",
    "            poly_gdf = poly_gdf.to_crs(point_gdf.crs)\n",
    "        return gpd.sjoin_nearest(poly_gdf, point_gdf, how=\"left\", distance_col=\"distance\")\n",
    "\n",
    "    def export_native_raster(joined_gdf: gpd.GeoDataFrame, my_path, state_abbr, column_name: str = \"Native_Fish_Richness\"):\n",
    "        bounds = joined_gdf.total_bounds\n",
    "        transform = rasterio.transform.from_origin(bounds[0], bounds[3], resolution, resolution)\n",
    "        out_shape = (\n",
    "            int(np.ceil((bounds[3] - bounds[1]) / resolution)),\n",
    "            int(np.ceil((bounds[2] - bounds[0]) / resolution))\n",
    "        )\n",
    "\n",
    "        raster = rasterize(\n",
    "            [(geom, value) for geom, value in zip(joined_gdf.geometry, joined_gdf[column_name])],\n",
    "            out_shape=out_shape,\n",
    "            transform=transform,\n",
    "            fill=0,\n",
    "            dtype=rasterio.float32\n",
    "        )\n",
    "\n",
    "        parent_dir = os.path.abspath(os.path.join(my_path, os.pardir))\n",
    "        output_filename = os.path.join(my_path, f\"{state_abbr}_{column_name}.tif\")\n",
    "\n",
    "        with rasterio.open(\n",
    "            output_filename, \"w\",\n",
    "            driver=\"GTiff\",\n",
    "            height=out_shape[0],\n",
    "            width=out_shape[1],\n",
    "            count=1,\n",
    "            dtype=rasterio.float32,\n",
    "            crs=\"EPSG:5070\",\n",
    "            transform=transform\n",
    "        ) as dst:\n",
    "            dst.write(raster, 1)\n",
    "            dst.set_band_description(1, column_name)\n",
    "\n",
    "        # Optional plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(raster, cmap='viridis', extent=[bounds[0], bounds[2], bounds[1], bounds[3]])\n",
    "        plt.colorbar(label=f'{column_name} Richness')\n",
    "        plt.title('Rasterized Native Fish Richness')\n",
    "        plt.xlabel('Longitude')\n",
    "        plt.ylabel('Latitude')\n",
    "        plt.show()\n",
    "\n",
    "    # Load or download native fish data\n",
    "    fish_path = os.path.join(my_path, fish_shapefile)\n",
    "    if os.path.exists(fish_path):\n",
    "        fish_gdf = gpd.read_file(fish_path).to_crs(my_crs)\n",
    "    else:\n",
    "        url = \"https://www.sciencebase.gov/catalog/file/get/6086df60d34eadd49d31b04a?f=__disk__ad%2F21%2Ffc%2Fad21fc677379f4e45caa4bd506ca1c587d5f01f7\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            csv_data = StringIO(response.text)\n",
    "            fish_df = pd.read_csv(csv_data)\n",
    "            fish_gdf = gpd.GeoDataFrame(\n",
    "                fish_df,\n",
    "                geometry=gpd.points_from_xy(fish_df.longitude, fish_df.latitude),\n",
    "                crs=\"EPSG:4326\"\n",
    "            ).to_crs(my_crs)\n",
    "        else:\n",
    "            raise RuntimeError(f\"Failed to download native fish data: {response.status_code}\")\n",
    "\n",
    "    # Load state boundary and buffered water\n",
    "    state_boundary = gpd.read_file(os.path.join(my_path, state_shapefile)).dropna().to_crs(my_crs)\n",
    "    state_boundary = state_boundary[state_boundary['STUSPS'] == state_abbr]\n",
    "    buffered_water_path = os.path.join(my_path, state_abbr + buffer_shapefile_suffix)\n",
    "    buffered_water = gpd.read_file(buffered_water_path).to_crs(my_crs)\n",
    "\n",
    "    # Process native fish richness\n",
    "    clipped_fish = clip_points_by_polygon(fish_gdf, state_boundary)\n",
    "    native_fish_gdf = sum_numeric_columns(clipped_fish)\n",
    "    water_with_fish = spatial_join_with_nearest(buffered_water, native_fish_gdf)\n",
    "\n",
    "    # Convert int columns to float\n",
    "    for col in water_with_fish.select_dtypes(include=['int64']).columns:\n",
    "        water_with_fish[col] = water_with_fish[col].astype('float64')\n",
    "\n",
    "    export_native_raster(water_with_fish, my_path, state_abbr)\n",
    "def gdf_to_multiband_tif(gdf, columns, output_path, pixel_size=100, crs_epsg=5070):\n",
    "    \"\"\"\n",
    "    Rasterize selected columns of a GeoDataFrame into a single multi-band GeoTIFF.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    gdf : GeoDataFrame\n",
    "        Must contain geometry and numeric columns to rasterize.\n",
    "    columns : list of str\n",
    "        Column names to rasterize into separate bands.\n",
    "    output_path : str\n",
    "        Path to save the GeoTIFF.\n",
    "    pixel_size : float\n",
    "        Output raster resolution in CRS units (default 100 for UTM meters).\n",
    "    crs_epsg : int\n",
    "        EPSG code for raster CRS (default 26915 for MN).\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure numeric columns (replace NaN with 0 so rasterize doesn't crash)\n",
    "    gdf = gdf.copy()\n",
    "    for col in columns:\n",
    "        gdf[col] = gdf[col].fillna(0).astype(float)\n",
    "\n",
    "    # Reproject to target CRS\n",
    "    gdf = gdf.to_crs(epsg=crs_epsg)\n",
    "\n",
    "    # Get bounds and create transform\n",
    "    minx, miny, maxx, maxy = gdf.total_bounds\n",
    "    width = int(np.ceil((maxx - minx) / pixel_size))\n",
    "    height = int(np.ceil((maxy - miny) / pixel_size))\n",
    "    transform = from_origin(minx, maxy, pixel_size, pixel_size)\n",
    "\n",
    "    # Prepare bands\n",
    "    bands = []\n",
    "    for col in columns:\n",
    "        shapes = ((geom, value) for geom, value in zip(gdf.geometry, gdf[col]))\n",
    "        band_array = rasterize(\n",
    "            shapes=shapes,\n",
    "            out_shape=(height, width),\n",
    "            transform=transform,\n",
    "            fill=0,\n",
    "            dtype=\"float32\"\n",
    "        )\n",
    "        bands.append(band_array)\n",
    "\n",
    "    # Stack into (bands, height, width)\n",
    "    bands_stack = np.stack(bands, axis=0)\n",
    "\n",
    "    # Write to multi-band GeoTIFF\n",
    "    with rasterio.open(\n",
    "        output_path,\n",
    "        'w',\n",
    "        driver='GTiff',\n",
    "        height=height,\n",
    "        width=width,\n",
    "        count=len(columns),\n",
    "        dtype='float32',\n",
    "        crs=f\"EPSG:{crs_epsg}\",\n",
    "        transform=transform\n",
    "    ) as dst:\n",
    "        for i in range(len(columns)):\n",
    "            dst.write(bands_stack[i], i + 1)\n",
    "\n",
    "    print(f\"‚úÖ Multi-band raster saved to: {output_path}\")\n",
    "def TSI_api(my_wid_list, my_format):\n",
    "    responses = []\n",
    "    URL_BASE = 'https://services.pca.state.mn.us/api/v1/'\n",
    "    for wid in my_wid_list:\n",
    "            url_request = f\"{URL_BASE}surfacewater/water-units/trophic-statuses?wid={wid}&format={my_format}\"  \n",
    "            response = requests.get(url_request, timeout=None)\n",
    "            responses.append(response.json())\n",
    "            return responses\n",
    "\n",
    "def download_ramps(local_path):\n",
    "        url = \"https://www.sciencebase.gov/catalog/file/get/63b81b50d34e92aad3cc004d?facet=Boatramps_United_States_final_20230104\"\n",
    "        extensions = {'.dbf', '.prj', '.shp', '.shx'}\n",
    "        response = requests.get(url, stream=True, timeout=60)\n",
    "        response.raise_for_status()\n",
    "        with zipfile.ZipFile(BytesIO(response.content)) as zip_ref:\n",
    "            os.makedirs(local_path, exist_ok=True)\n",
    "            for f in zip_ref.namelist():\n",
    "                if os.path.splitext(f)[1].lower() in extensions:\n",
    "                    zip_ref.extract(f, local_path)\n",
    "\n",
    "def pick_awetype(values):\n",
    "    values = list(pd.Series(values).dropna().unique())\n",
    "    if not values:\n",
    "        return None\n",
    "    if 1 in values: return 1\n",
    "    if 3 in values: return 3\n",
    "    if 2 in values: return 2\n",
    "    return values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5881a0c6-767b-454c-93e1-6827811bf8df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b769aa-9b60-427b-864a-3b8372bbde5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the shapefile ZIP\n",
    "MN_water_url = \"https://resources.gisdata.mn.gov/pub/gdrs/data/pub/us_mn_state_dnr/water_dnr_hydrography/shp_water_dnr_hydrography.zip\"\n",
    "# Send a GET request to fetch the ZIP file\n",
    "MN_water_response = requests.get(MN_water_url)\n",
    "MN_water_response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "# Open the ZIP file from the response content\n",
    "with zipfile.ZipFile(io.BytesIO(MN_water_response.content)) as zip_ref:\n",
    "    # Extract all contents into the current directory\n",
    "    zip_ref.extractall(training_path)\n",
    "\n",
    "print(\"mn_dnr_hydrography Shapefile extracted to directory.\")\n",
    "\n",
    "# Download Watercourse alteration and use risk lakes to create rasters \n",
    "watercourse_alteration_url= 'https://resources.gisdata.mn.gov/pub/gdrs/data/pub/us_mn_state_pca/water_altered_watercourse/shp_water_altered_watercourse.zip'\n",
    "# Send a GET request to fetch the ZIP file\n",
    "watercourse_alteration_response = requests.get(watercourse_alteration_url)\n",
    "watercourse_alteration_response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "\n",
    "# Open the ZIP file from the response content\n",
    "with zipfile.ZipFile(io.BytesIO(watercourse_alteration_response.content)) as zip_ref:\n",
    "    # Extract all contents into the current directory\n",
    "    zip_ref.extractall(training_path)\n",
    "\n",
    "print(\"watercourse alteration Shapefile extracted to directory.\")\n",
    "\n",
    "sf_water_station_url = \"https://resources.gisdata.mn.gov/pub/gdrs/data/pub/us_mn_state_pca/env_eda_surfacewater_stations/shp_env_eda_surfacewater_stations.zip\"\n",
    "# Send a GET request to fetch the ZIP file\n",
    "sf_water_station_response = requests.get(sf_water_station_url)\n",
    "sf_water_station_response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "\n",
    "# Open the ZIP file from the response content\n",
    "with zipfile.ZipFile(io.BytesIO(sf_water_station_response.content)) as zip_ref:\n",
    "    # Extract all contents into the current directory\n",
    "    zip_ref.extractall(training_path)\n",
    "\n",
    "print(\"surface water stations Shapefile extracted to directory.\")\n",
    "\n",
    "#Spatially join ramps to risk lakes \n",
    "\n",
    "download_ramps(training_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13cb9a35-e950-44cd-8054-d5e6b67d9239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the lake shapefile you just downloaded (It takes a minute)\n",
    "MNDNR_lakes = gpd.read_file(training_path + 'dnr_hydro_features_all.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae54f5b3-4ac1-4cbd-8cbe-344b4fc3d2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakes_gdf = MNDNR_lakes.dropna(subset=['dowlknum']).to_crs(my_crs)\n",
    "lakes_gdf['dowlknum'] = lakes_gdf['dowlknum'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73c916e2-076c-4c03-8fcf-23cb71d44626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leif.howard\\AppData\\Local\\Temp\\ipykernel_7360\\705980240.py:14: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  lakes_with_risk.to_file(training_path + \"lakes_risk.shp\")\n",
      "WARNING:Normalized/laundered field name: 'Boater_movement' to 'Boater_mov'\n",
      "WARNING:Normalized/laundered field name: 'Water_connectivity' to 'Water_conn'\n"
     ]
    }
   ],
   "source": [
    "risk_df = pd.read_csv(training_path + 'All_risk_table.csv')  # Adjust path and filename accordingly\n",
    "# Example: DOW field may be 'DOWLKNUM' or similar\n",
    "dow_field = 'dowlknum'  # change if needed\n",
    "# Rename risk_df DOW column to match shapefile for join\n",
    "risk_df = risk_df.rename(columns={\"DOW number\": dow_field, 'Boater movement risk score': 'Boater_movement', 'Water connectivity risk score': 'Water_connectivity'})\n",
    "# 3) Merge lakes and risk data on DOW number\n",
    "lakes_risk = lakes_gdf.merge(risk_df[[dow_field, 'Boater_movement', 'Water_connectivity']],\n",
    "                         on=dow_field,\n",
    "                         how=\"inner\")\n",
    "\n",
    "# 4) Project to a metric CRS (e.g., NAD83 UTM Zone 15N EPSG:26915) for 100m pixels\n",
    "lakes_with_risk = lakes_risk[['dowlknum', 'geometry',\n",
    "       'Boater_movement', 'Water_connectivity']].to_crs(my_crs)\n",
    "lakes_with_risk.to_file(training_path + \"lakes_risk.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4cac0a6-89b2-4521-bbdb-76db454774a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "AWEvtType_gdf =  gpd.read_file(training_path + 'Altered_Watercourse.shp').to_crs(my_crs)# has AWEvtType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e56e244-5bac-4628-9c73-ae0342bb4dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-pass nearest join (distance=0 means intersect)\n",
    "nearest_all = gpd.sjoin_nearest(\n",
    "    lakes_with_risk,\n",
    "    AWEvtType_gdf[[\"AWEvtType\", \"geometry\"]],\n",
    "    how=\"left\",\n",
    "    distance_col=\"nearest_dist\",\n",
    "    max_distance=2000  # limit in meters\n",
    ")\n",
    "\n",
    "# Apply priority logic in one groupby\n",
    "final_df = (\n",
    "    nearest_all.groupby(\"dowlknum\", as_index=False)\n",
    "    .agg({\"AWEvtType\": pick_awetype})\n",
    ")\n",
    "\n",
    "# This is now a plain DataFrame\n",
    "AWEvtType_for_merge = final_df[[\"dowlknum\", \"AWEvtType\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e35d704-d9dc-476e-8301-0b7dbd588e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "AWEvtType_for_merge.to_csv(training_path + \"AWEvtType.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b447ed40-cd4d-4849-8a8a-a3dd88f7a051",
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_gdf = gpd.read_file(training_path + \"eda_surfacewater_stations.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28dea87c-0ad0-47ee-9128-4446c636cb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wid = eda_gdf['wid'].unique()\n",
    "batch_list = []\n",
    "return_list = []\n",
    "for i in wid:\n",
    "    batch_list.append(i)\n",
    "    if len(batch_list)  == 100:\n",
    "        return_list.append(TSI_api(batch_list, 'json'))\n",
    "        batch_list.clear()\n",
    "\n",
    "if batch_list:\n",
    "    return_list.append(TSI_api(batch_list,  'json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f284140-fad1-4eb1-9c5c-b4a913dc60c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten all 'data' lists from each API result\n",
    "all_data = []\n",
    "for api_result_group in return_list:   # outer list\n",
    "    for api_result in api_result_group:  # inner list of dicts\n",
    "        if api_result.get(\"data\"):  # only if not empty\n",
    "            all_data.extend(api_result[\"data\"])\n",
    "\n",
    "# Create DataFrame if there's data\n",
    "if all_data:\n",
    "    tsi_df = pd.DataFrame(all_data)\n",
    "    # Keep only wid + tpTsi, ensure numeric\n",
    "    tsi_df = tsi_df[['wid', 'tpTsi']].copy()\n",
    "    tsi_df['tpTsi'] = pd.to_numeric(tsi_df['tpTsi'], errors='coerce')\n",
    "    tsi_df = tsi_df.dropna(subset=[\"tpTsi\"]).reset_index(drop=True)\n",
    "else:\n",
    "    tsi_df = pd.DataFrame(columns=['wid', 'tpTsi'])\n",
    "    tsi_df = tsi_df.dropna(subset=[\"tpTsi\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "572b8871-ba74-46c0-bac7-a84c1e279ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:One or several characters couldn't be converted correctly from UTF-8 to ISO-8859-1.  This warning will not be emitted anymore.\n"
     ]
    }
   ],
   "source": [
    "# --- Ensure wid has matching types ---\n",
    "eda_gdf[\"wid\"] = eda_gdf[\"wid\"].astype(str)\n",
    "tsi_df[\"wid\"] = tsi_df[\"wid\"].astype(str)\n",
    "\n",
    "# --- Merge on wid ---\n",
    "eda_w_tsi = eda_gdf.merge(tsi_df, on=\"wid\", how=\"left\")\n",
    "\n",
    "# --- Save updated shapefile ---\n",
    "eda_w_tsi.to_file(\"eda_surface_stations_with_tsi.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77f8fc53-4ff6-4a20-94b8-e5f4a8c47f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsi_df_clean = eda_w_tsi.dropna(subset=[\"tpTsi\"]).reset_index(drop=True)\n",
    "# Ensure both GeoDataFrames are in the same CRS\n",
    "if tsi_df_clean.crs != lakes_gdf.crs:\n",
    "    tsi_df_clean = tsi_df_clean.to_crs(lakes_gdf.crs)\n",
    "\n",
    "# Nearest spatial join: each lake gets nearest station's tpTSI\n",
    "lakes_with_tsi = gpd.sjoin_nearest(\n",
    "    lakes_gdf, \n",
    "    tsi_df_clean,\n",
    "    how=\"left\",\n",
    "    distance_col=\"nearest_dist\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b370b921-f7ec-42cc-aafc-111704829ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsi_for_merge = lakes_with_tsi[[\"dowlknum\", \"tpTsi\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b65aa67e-a5b1-44e7-a41c-f52165dedc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ramps = gpd.read_file(training_path + 'Boatramps_United_States_final_20230104.shp').set_crs(my_crs, allow_override=True)\n",
    "ramp_geo = ramps.loc[ramps['State'] == training_state_abbr, ['geometry']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a41c2301-5146-4abe-ad1a-2327f760aaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ramps_gdf = ramp_geo.to_crs(my_crs)\n",
    "# Spatial join to get each ramp assigned to a lake polygon\n",
    "ramps_with_lake = gpd.sjoin(ramps_gdf, lakes_gdf, how=\"inner\", predicate=\"within\")\n",
    "\n",
    "# Count ramps per lake\n",
    "ramps_count = ramps_with_lake.groupby(\"dowlknum\").size().reset_index(name=\"n_boat_ramps\")\n",
    "\n",
    "# Merge back to lakes\n",
    "lakes_gdf[\"dowlknum\"] = lakes_gdf[\"dowlknum\"].astype(int)\n",
    "ramps_count[\"dowlknum\"] = ramps_count[\"dowlknum\"].astype(int)\n",
    "\n",
    "ramps_gdf = lakes_gdf.merge(ramps_count, on=\"dowlknum\", how=\"left\")\n",
    "\n",
    "# Fill NaN for lakes with no ramps\n",
    "ramps_gdf[\"n_boat_ramps\"] = ramps_gdf[\"n_boat_ramps\"].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0097e542-5abc-4295-bae0-cc902eefb2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ramps_for_merge = ramps_gdf[[\"dowlknum\", \"n_boat_ramps\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b17691dc-8424-4f3c-8a29-62129678a337",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = (\n",
    "    lakes_with_risk\n",
    "    .merge(tsi_for_merge, on=\"dowlknum\", how=\"left\")\n",
    "    .merge(ramps_for_merge, on=\"dowlknum\", how=\"left\")\n",
    "    .merge(AWEvtType_for_merge, on=\"dowlknum\", how=\"left\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b74bc833-796a-434f-aa65-c4c82ab87c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"AWEvtType\"] = merged_df[\"AWEvtType\"].fillna(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "05d0966b-15c4-489e-98ef-6579f7f6134e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Multi-band raster saved to: my_data/MN/lakes_risk_multiband.tif\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "columns_to_raster = [\n",
    "    \"Boater_movement\",\n",
    "    \"Water_connectivity\",\n",
    "    \"tpTsi\",\n",
    "    \"n_boat_ramps\",\n",
    "    \"AWEvtType\"\n",
    "]\n",
    "\n",
    "gdf_to_multiband_tif(\n",
    "    gdf=merged_risk,\n",
    "    columns=columns_to_raster,\n",
    "    output_path=training_path + \"lakes_risk_multiband.tif\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00c36f9-09cb-4bec-9696-715cc0021b94",
   "metadata": {},
   "source": [
    "# Start of Biological Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea19635-26ab-45b0-b96f-2acef2957114",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download/import water data.\n",
    "water_data_training = get_nhd_waterbodies(\n",
    "    state_name=training_state_name,\n",
    "    local_path=training_path,\n",
    "    output_prefix=training_state_abbr,\n",
    "    save_files=True,\n",
    "    make_background_water=True, # set to false here, unless you want this for future model training within your state\n",
    "    training_path=training_path,\n",
    "    training_state_abbr=training_state_abbr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddde4eae-06be-49ce-bc45-4530a422ff69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buffer individual water layers\n",
    "buffered_streams_training, buffered_lakes_training, buffered_rivers_training, full_streams_training, buffered_water_training = buffer_water_layers(\n",
    "    streams=water_data_training[\"streams\"],\n",
    "    lakes=water_data_training[\"lakes\"],\n",
    "    rivers=water_data_training[\"rivers\"],\n",
    "    filter_streams=False,\n",
    "    save_path= training_path,\n",
    "    use_cached=True,\n",
    "    export_merged_filename=f\"{training_path}{training_state_abbr}_buffered_water.shp\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5849a2a-97b7-4776-8942-13b03d0fce25",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_rich_training = process_nas_occurrences(\n",
    "    state= training_state_abbr,\n",
    "    crs= my_crs,\n",
    "    path= training_path,\n",
    "    target_species_id= nas_id,\n",
    "    target_species_name= nas_name,\n",
    "    make_background=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2446567c-bdeb-49e3-a4f8-161289897a74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6496350-c1d0-487f-8e40-4dcb4807a44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_inv_richness(inv_rich_training, training_path + training_state_abbr + '_' + nas_name +'_inv_richness.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953fb947-7162-4dcb-812e-3c068b480730",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_and_export_native_fish_raster(\n",
    "    my_path=training_path, \n",
    "    my_crs=\"EPSG:5070\",\n",
    "    state_abbr=training_state_abbr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7e69ccec-7933-4b37-b0ec-be93c2a592b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\masters\\Lib\\site-packages\\requests\\models.py:971\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    972\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    973\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\masters\\Lib\\site-packages\\simplejson\\__init__.py:514\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, use_decimal, allow_nan, **kw)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    511\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    512\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    513\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_decimal \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nan \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_decoder\u001b[38;5;241m.\u001b[39mdecode(s)\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\masters\\Lib\\site-packages\\simplejson\\decoder.py:386\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w, _PY3)\u001b[0m\n\u001b[0;32m    385\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(s, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding)\n\u001b[1;32m--> 386\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_decode(s)\n\u001b[0;32m    387\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\masters\\Lib\\site-packages\\simplejson\\decoder.py:416\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx, _w, _PY3)\u001b[0m\n\u001b[0;32m    415\u001b[0m         idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m--> 416\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx\u001b[38;5;241m=\u001b[39m_w(s, idx)\u001b[38;5;241m.\u001b[39mend())\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m api_result \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m dow_list:\n\u001b[1;32m---> 18\u001b[0m     api_result\u001b[38;5;241m.\u001b[39mappend(api_lakefinder(\u001b[38;5;28mid\u001b[39m))\n",
      "Cell \u001b[1;32mIn[61], line 8\u001b[0m, in \u001b[0;36mapi_lakefinder\u001b[1;34m(lake_id)\u001b[0m\n\u001b[0;32m      6\u001b[0m url_request_lakefinder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mURL_BASE_lakefinder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m?type=lake_survey&id=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlake_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \n\u001b[0;32m      7\u001b[0m response_lakefinder \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url_request_lakefinder, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m----> 8\u001b[0m responses_lakefinder\u001b[38;5;241m.\u001b[39mappend(response_lakefinder\u001b[38;5;241m.\u001b[39mjson())\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m responses_lakefinder\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\masters\\Lib\\site-packages\\requests\\models.py:975\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    972\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    973\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[1;32m--> 975\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[38;5;241m.\u001b[39mmsg, e\u001b[38;5;241m.\u001b[39mdoc, e\u001b[38;5;241m.\u001b[39mpos)\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "# MN native diversity predictors\n",
    "# Function to get records from database API\n",
    "def api_lakefinder(lake_id):\n",
    "    responses_lakefinder = []\n",
    "    URL_BASE_lakefinder = 'https://maps2.dnr.state.mn.us/cgi-bin/lakefinder/detail.cgi'\n",
    "    url_request_lakefinder = f\"{URL_BASE_lakefinder}?type=lake_survey&id={lake_id}\" \n",
    "    response_lakefinder = requests.get(url_request_lakefinder, timeout=None)\n",
    "    responses_lakefinder.append(response_lakefinder.json())\n",
    "    return responses_lakefinder\n",
    "\n",
    "my_lakes = pd.read_csv(training_path + \"All_risk_table.csv\")\n",
    "\n",
    "id_codes = my_lakes['DOW number'].astype(str)\n",
    "\n",
    "dow_list = id_codes[0:1000].values.tolist()\n",
    "api_result = []\n",
    "for id in dow_list:\n",
    "    api_result.append(api_lakefinder(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3d486008-fb2c-486a-b0b0-0fd1ca901bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def api_lakefinder(lake_id):\n",
    "    URL_BASE = 'https://maps2.dnr.state.mn.us/cgi-bin/lakefinder/detail.cgi'\n",
    "    url = f\"{URL_BASE}?type=lake_survey&id={lake_id}\" \n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}  # mimic browser\n",
    "    response = requests.get(url, headers=headers, timeout=10)\n",
    "    \n",
    "    if response.status_code != 200 or not response.text.strip():\n",
    "        print(f\"Failed to fetch or empty page for lake_id {lake_id}\")\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Example: find survey table or info - this depends on actual page structure\n",
    "    # You will need to inspect the HTML to know where fish data is\n",
    "    \n",
    "    # Example placeholder: get the lake name from a known tag/class/id\n",
    "    lake_name_tag = soup.find('h1')  # adjust selector as needed\n",
    "    lake_name = lake_name_tag.text.strip() if lake_name_tag else 'Unknown'\n",
    "    \n",
    "    # TODO: Parse survey data tables or sections here\n",
    "    \n",
    "    return {\n",
    "        'lake_id': lake_id,\n",
    "        'lake_name': lake_name,\n",
    "        # add more extracted info here\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bc82246d-71b5-46fc-b5a4-bab4390f1ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch or empty page for lake_id 32005800\n",
      "Failed to fetch or empty page for lake_id 53002800\n",
      "Failed to fetch or empty page for lake_id 42007400\n",
      "Failed to fetch or empty page for lake_id 53002100\n",
      "Failed to fetch or empty page for lake_id 51001800\n",
      "Failed to fetch or empty page for lake_id 42006600\n",
      "Failed to fetch or empty page for lake_id 41005800\n",
      "Failed to fetch or empty page for lake_id 51002400\n",
      "Failed to fetch or empty page for lake_id 53002000\n",
      "Failed to fetch or empty page for lake_id 51003800\n",
      "Failed to fetch or empty page for lake_id 51002000\n",
      "Failed to fetch or empty page for lake_id 51004000\n",
      "Failed to fetch or empty page for lake_id 42005400\n",
      "Failed to fetch or empty page for lake_id 42001300\n",
      "Failed to fetch or empty page for lake_id 32006900\n",
      "Failed to fetch or empty page for lake_id 42009600\n",
      "Failed to fetch or empty page for lake_id 42000500\n",
      "Failed to fetch or empty page for lake_id 51000600\n",
      "Failed to fetch or empty page for lake_id 42005200\n",
      "Failed to fetch or empty page for lake_id 53000700\n",
      "Failed to fetch or empty page for lake_id 59000100\n",
      "Failed to fetch or empty page for lake_id 41005500\n",
      "Failed to fetch or empty page for lake_id 51003900\n",
      "Failed to fetch or empty page for lake_id 41008200\n",
      "Failed to fetch or empty page for lake_id 42004700\n",
      "Failed to fetch or empty page for lake_id 51002100\n",
      "Failed to fetch or empty page for lake_id 51002700\n",
      "Failed to fetch or empty page for lake_id 42000300\n",
      "Failed to fetch or empty page for lake_id 32008400\n",
      "Failed to fetch or empty page for lake_id 41004300\n",
      "Failed to fetch or empty page for lake_id 67000200\n",
      "Failed to fetch or empty page for lake_id 67000100\n",
      "Failed to fetch or empty page for lake_id 87013400\n",
      "Failed to fetch or empty page for lake_id 87001500\n",
      "Failed to fetch or empty page for lake_id 87001300\n",
      "Failed to fetch or empty page for lake_id 17005400\n",
      "Failed to fetch or empty page for lake_id 41006700\n",
      "Failed to fetch or empty page for lake_id 87001600\n",
      "Failed to fetch or empty page for lake_id 42001400\n",
      "Failed to fetch or empty page for lake_id 53004500\n",
      "Failed to fetch or empty page for lake_id 51004600\n",
      "Failed to fetch or empty page for lake_id 87003000\n",
      "Failed to fetch or empty page for lake_id 41008900\n",
      "Failed to fetch or empty page for lake_id 41011500\n",
      "Failed to fetch or empty page for lake_id 51014100\n",
      "Failed to fetch or empty page for lake_id 51013900\n",
      "Failed to fetch or empty page for lake_id 87012600\n",
      "Failed to fetch or empty page for lake_id 53001600\n",
      "Failed to fetch or empty page for lake_id 42005500\n",
      "Failed to fetch or empty page for lake_id 42009300\n",
      "Failed to fetch or empty page for lake_id 42007000\n",
      "Failed to fetch or empty page for lake_id 41011000\n",
      "Failed to fetch or empty page for lake_id 42000200\n",
      "Failed to fetch or empty page for lake_id 51006800\n",
      "Failed to fetch or empty page for lake_id 17006000\n",
      "Failed to fetch or empty page for lake_id 51004300\n",
      "Failed to fetch or empty page for lake_id 46005200\n",
      "Failed to fetch or empty page for lake_id 46003900\n",
      "Failed to fetch or empty page for lake_id 46003000\n",
      "Failed to fetch or empty page for lake_id 46009600\n",
      "Failed to fetch or empty page for lake_id 32002200\n",
      "Failed to fetch or empty page for lake_id 46003400\n",
      "Failed to fetch or empty page for lake_id 46004300\n",
      "Failed to fetch or empty page for lake_id 46002400\n",
      "Failed to fetch or empty page for lake_id 46002700\n",
      "Failed to fetch or empty page for lake_id 46002000\n",
      "Failed to fetch or empty page for lake_id 46004800\n",
      "Failed to fetch or empty page for lake_id 46013000\n",
      "Failed to fetch or empty page for lake_id 46011100\n",
      "Failed to fetch or empty page for lake_id 46001200\n",
      "Failed to fetch or empty page for lake_id 32003300\n",
      "Failed to fetch or empty page for lake_id 46003100\n",
      "Failed to fetch or empty page for lake_id 46003700\n",
      "Failed to fetch or empty page for lake_id 46008400\n",
      "Failed to fetch or empty page for lake_id 46002500\n",
      "Failed to fetch or empty page for lake_id 46004200\n",
      "Failed to fetch or empty page for lake_id 46012100\n",
      "Failed to fetch or empty page for lake_id 46001000\n",
      "Failed to fetch or empty page for lake_id 46010300\n",
      "Failed to fetch or empty page for lake_id 46014500\n",
      "Failed to fetch or empty page for lake_id 72005400\n",
      "Failed to fetch or empty page for lake_id 72005500\n",
      "Failed to fetch or empty page for lake_id 83004000\n",
      "Failed to fetch or empty page for lake_id 46001300\n",
      "Failed to fetch or empty page for lake_id 32002300\n",
      "Failed to fetch or empty page for lake_id 83002100\n",
      "Failed to fetch or empty page for lake_id 8003500\n",
      "Failed to fetch or empty page for lake_id 8001000\n",
      "Failed to fetch or empty page for lake_id 8001400\n",
      "Failed to fetch or empty page for lake_id 8002700\n",
      "Failed to fetch or empty page for lake_id 8002900\n",
      "Failed to fetch or empty page for lake_id 8002600\n",
      "Failed to fetch or empty page for lake_id 8001300\n",
      "Failed to fetch or empty page for lake_id 8001600\n",
      "Failed to fetch or empty page for lake_id 8001100\n",
      "Failed to fetch or empty page for lake_id 7014700\n",
      "Failed to fetch or empty page for lake_id 8002500\n",
      "Failed to fetch or empty page for lake_id 8001800\n",
      "Failed to fetch or empty page for lake_id 8004500\n",
      "Failed to fetch or empty page for lake_id 8000400\n"
     ]
    }
   ],
   "source": [
    "my_lakes = pd.read_csv(training_path + \"All_risk_table.csv\")\n",
    "dow_list = my_lakes['DOW number'].astype(str).tolist()\n",
    "\n",
    "api_results = []\n",
    "for lake_id in dow_list[:100]:  # limit to first 1000\n",
    "    data = api_lakefinder(lake_id)\n",
    "    if data:\n",
    "        api_results.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e743e514-af7c-42f8-930c-68cdf1eef27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e17385-50f0-4689-bc7b-72a28801b29e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7e4efc-3b7a-4332-a6b4-29c83ddd804d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter out unsuccessful responses (DOWs that did not have survey data)\n",
    "def get_success_entries(api_data):\n",
    "    # Flattening the list and filtering only dictionary items where 'status' is 'SUCCESS'\n",
    "    success_entries = [item for sublist in api_data if isinstance(sublist, list) \n",
    "                       for item in sublist if isinstance(item, dict) and item.get('status') == 'SUCCESS']\n",
    "    return success_entries\n",
    "\n",
    "# Function to extract DOW, survey date and cpue by species and return seperate records for each survey\n",
    "def extract_fish_data(data):\n",
    "    results = []\n",
    "    \n",
    "    # Loop through the data\n",
    "    for lake in data:\n",
    "        dow_number = lake['result']['DOWNumber']\n",
    "        \n",
    "        for survey in lake['result']['surveys']:\n",
    "            survey_date = survey['surveyDate']\n",
    "            \n",
    "            # Initialize total fish count and CPUE dictionary\n",
    "            total_fish_count = 0\n",
    "            cpue_by_species = {}\n",
    "            \n",
    "            # Extract fish catch summaries\n",
    "            for fish in survey['fishCatchSummaries']:\n",
    "                species = fish['species']\n",
    "                total_catch = fish['totalCatch']\n",
    "                cpue = fish['CPUE']\n",
    "                \n",
    "                # Add to the total fish count\n",
    "                total_fish_count += total_catch\n",
    "                \n",
    "                # Store CPUE by species\n",
    "                cpue_by_species[species] = cpue\n",
    "            \n",
    "            # Append the result to the list\n",
    "            results.append({\n",
    "                'DOWNumber': dow_number,\n",
    "                'surveyDate': survey_date,\n",
    "                'totalFishCount': total_fish_count,\n",
    "                'CPUEBySpecies': cpue_by_species\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Function to filter records with surveyDate > 2002\n",
    "def filter_records_by_date(data):\n",
    "    filtered_data = []\n",
    "    for record in data:\n",
    "        survey_date = datetime.strptime(record['surveyDate'], '%Y-%m-%d')\n",
    "        if survey_date.year > 2002:\n",
    "            filtered_data.append(record)\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "# Function to check if all values in CPUEBySpecies can be converted to float\n",
    "def can_convert_to_float(cpue_by_species):\n",
    "    try:\n",
    "        return all(float(value) for value in cpue_by_species.values())\n",
    "    except ValueError:\n",
    "        return False\n",
    "# Function to calculate mean CPUEBySpecies for each DOWNumber\n",
    "def calculate_mean_cpue(data):\n",
    "    species_totals = defaultdict(lambda: defaultdict(float))\n",
    "    species_counts = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    # Group records by DOWNumber and accumulate CPUE values\n",
    "    for record in data:\n",
    "        dow = record['DOWNumber']\n",
    "        for species, cpue in record['CPUEBySpecies'].items():\n",
    "            species_totals[dow][species] += float(cpue)\n",
    "            species_counts[dow][species] += 1\n",
    "\n",
    "    # Calculate means\n",
    "    species_means = {}\n",
    "    for dow, species_data in species_totals.items():\n",
    "        species_means[dow] = {}\n",
    "        for species, total_cpue in species_data.items():\n",
    "            count = species_counts[dow][species]\n",
    "            species_means[dow][species] = total_cpue / count if count > 0 else 0\n",
    "    \n",
    "    return species_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42c1263-8616-4906-96d8-cd02bf76e593",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_result2 = []\n",
    "for id in dow_list2:\n",
    "    api_result2.append(api_call(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b039ddb9-460f-4bb1-b7a9-a03c80000950",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_result3 = []\n",
    "for id in dow_list3:\n",
    "    api_result3.append(api_call(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9fd05d-5a0b-44eb-873f-10578945c47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api_result3b = []\n",
    "# for id in dow_list3b:\n",
    "#     api_result3b.append(api_call(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e3033b-fe83-44fb-91a3-fd0c7a280b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_result4 = []\n",
    "for id in dow_list4:\n",
    "    api_result4.append(api_call(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a7fa4b-8716-4396-a174-3ff7a0e58254",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_result5 = []\n",
    "for id in dow_list5:\n",
    "    api_result5.append(api_call(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eafe07-ad23-47c4-acbd-fd242d6a6a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_result6 = []\n",
    "for id in dow_list6:\n",
    "    api_result6.append(api_call(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57d9e56-6427-4fd0-b7da-2c7f8737144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_result7 = []\n",
    "for id in dow_list7:\n",
    "    api_result7.append(api_call(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38730591-e39b-4aa2-9587-8e4ab0b3ddf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_result8 = []\n",
    "for id in dow_list8:\n",
    "    api_result8.append(api_call(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c868cf7b-02e2-490e-b57b-275e05633f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_result9 = []\n",
    "for id in dow_list9:\n",
    "    api_result9.append(api_call(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250373c3-4479-4831-a8f9-d03f89f18d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_result10 = []\n",
    "for id in dow_list10:\n",
    "    api_result10.append(api_call(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e689d92c-80a6-40a2-8064-0932af144799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_invalid_cpue(data_list):\n",
    "    for item in data_list:\n",
    "        item['CPUEBySpecies'] = {species: cpue for species, cpue in item['CPUEBySpecies'].items() if cpue != '?'}\n",
    "\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3afae8-4d3b-4e9f-b26a-a9fb89fa378f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_success_entries(api_result)\n",
    "fish_data = extract_fish_data(data)\n",
    "filtered_data = filter_records_by_date(fish_data)\n",
    "filtered_data_b = remove_invalid_cpue(filtered_data)\n",
    "mean_cpue_by_species = calculate_mean_cpue(filtered_data_b)\n",
    "cpue_df = pd.DataFrame.from_dict(mean_cpue_by_species, orient='index').reset_index()\n",
    "cpue_df_b = cpue_df.rename(columns={'index': 'DOWNumber'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d3076c-63f3-4459-9ca6-beac90387c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = get_success_entries(api_result2)\n",
    "fish_data2 = extract_fish_data(data2)\n",
    "filtered_data2 = filter_records_by_date(fish_data2)\n",
    "filtered_data_2b = remove_invalid_cpue(filtered_data2)\n",
    "mean_cpue_by_species2 = calculate_mean_cpue(filtered_data_2b)\n",
    "cpue_df2 = pd.DataFrame.from_dict(mean_cpue_by_species2, orient='index').reset_index()\n",
    "cpue_df_2b = cpue_df2.rename(columns={'index': 'DOWNumber'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b3865a-0162-4a18-8d3c-06d9c60aef1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = get_success_entries(api_result3)\n",
    "fish_data3 = extract_fish_data(data3)\n",
    "filtered_data3 = filter_records_by_date(fish_data3)\n",
    "filtered_data_3b = remove_invalid_cpue(filtered_data3)\n",
    "mean_cpue_by_species3 = calculate_mean_cpue(filtered_data_3b)\n",
    "cpue_df3 = pd.DataFrame.from_dict(mean_cpue_by_species3, orient='index').reset_index()\n",
    "cpue_df_3b = cpue_df3.rename(columns={'index': 'DOWNumber'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72ed1e0-d6d6-449d-8517-39f010d44816",
   "metadata": {},
   "outputs": [],
   "source": [
    "data4 = get_success_entries(api_result4)\n",
    "fish_data4 = extract_fish_data(data4)\n",
    "filtered_data4 = filter_records_by_date(fish_data4)\n",
    "filtered_data_4b = remove_invalid_cpue(filtered_data4)\n",
    "mean_cpue_by_species4 = calculate_mean_cpue(filtered_data_4b)\n",
    "cpue_df4 = pd.DataFrame.from_dict(mean_cpue_by_species4, orient='index').reset_index()\n",
    "cpue_df_4b = cpue_df4.rename(columns={'index': 'DOWNumber'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebf0db1-f934-4454-b683-d60845a6c85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data5 = get_success_entries(api_result5)\n",
    "fish_data5 = extract_fish_data(data5)\n",
    "filtered_data5 = filter_records_by_date(fish_data5)\n",
    "filtered_data_5b = remove_invalid_cpue(filtered_data5)\n",
    "mean_cpue_by_species5 = calculate_mean_cpue(filtered_data_5b)\n",
    "cpue_df5 = pd.DataFrame.from_dict(mean_cpue_by_species5, orient='index').reset_index()\n",
    "cpue_df_5b = cpue_df5.rename(columns={'index': 'DOWNumber'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6fc89b-43f6-46be-b14c-5e56db9ac8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data6 = get_success_entries(api_result6)\n",
    "fish_data6 = extract_fish_data(data6)\n",
    "filtered_data6 = filter_records_by_date(fish_data6)\n",
    "filtered_data_6b = remove_invalid_cpue(filtered_data6)\n",
    "mean_cpue_by_species6 = calculate_mean_cpue(filtered_data_6b)\n",
    "cpue_df6 = pd.DataFrame.from_dict(mean_cpue_by_species6, orient='index').reset_index()\n",
    "cpue_df_6b = cpue_df6.rename(columns={'index': 'DOWNumber'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ab8626-2380-4ad5-a54b-c20419f51119",
   "metadata": {},
   "outputs": [],
   "source": [
    "data7 = get_success_entries(api_result7)\n",
    "fish_data7 = extract_fish_data(data7)\n",
    "filtered_data7 = filter_records_by_date(fish_data7)\n",
    "filtered_data_7b = remove_invalid_cpue(filtered_data7)\n",
    "mean_cpue_by_species7 = calculate_mean_cpue(filtered_data_7b)\n",
    "cpue_df7 = pd.DataFrame.from_dict(mean_cpue_by_species7, orient='index').reset_index()\n",
    "cpue_df_7b = cpue_df7.rename(columns={'index': 'DOWNumber'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6814935b-b20e-4051-84b7-44f3ad8ccf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "data8 = get_success_entries(api_result8)\n",
    "fish_data8 = extract_fish_data(data8)\n",
    "filtered_data8 = filter_records_by_date(fish_data8)\n",
    "filtered_data_8b = remove_invalid_cpue(filtered_data8)\n",
    "mean_cpue_by_species8 = calculate_mean_cpue(filtered_data_8b)\n",
    "cpue_df8 = pd.DataFrame.from_dict(mean_cpue_by_species8, orient='index').reset_index()\n",
    "cpue_df_8b = cpue_df8.rename(columns={'index': 'DOWNumber'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d16708-e93e-465d-9158-72142a6c3a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "data9 = get_success_entries(api_result9)\n",
    "fish_data9 = extract_fish_data(data9)\n",
    "filtered_data9 = filter_records_by_date(fish_data9)\n",
    "filtered_data_9b = remove_invalid_cpue(filtered_data9)\n",
    "mean_cpue_by_species9 = calculate_mean_cpue(filtered_data_9b)\n",
    "cpue_df9 = pd.DataFrame.from_dict(mean_cpue_by_species9, orient='index').reset_index()\n",
    "cpue_df_9b = cpue_df9.rename(columns={'index': 'DOWNumber'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74916e6f-08e5-44f5-98c5-631e8ba86337",
   "metadata": {},
   "outputs": [],
   "source": [
    "data10 = get_success_entries(api_result10)\n",
    "fish_data10 = extract_fish_data(data10)\n",
    "filtered_data10 = filter_records_by_date(fish_data10)\n",
    "filtered_data_10b = remove_invalid_cpue(filtered_data10)\n",
    "mean_cpue_by_species10 = calculate_mean_cpue(filtered_data_10b)\n",
    "cpue_df10 = pd.DataFrame.from_dict(mean_cpue_by_species10, orient='index').reset_index()\n",
    "cpue_df_10b = cpue_df10.rename(columns={'index': 'DOWNumber'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5733a7f4-0fda-4af1-8bee-680f170ef22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_result_df = pd.concat([cpue_df_b, cpue_df_2b, cpue_df_3b, cpue_df_4b,\n",
    "  cpue_df_5b, cpue_df_6b, cpue_df_7b,cpue_df_8b, cpue_df_9b, cpue_df_10b], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2480e87-eae2-4d76-a646-ce1dee6e036d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This takes a while...save as csv so you don't have to repeat\n",
    "fish_result_df.to_csv('fish_survey_all_lakes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4daed4-8dd6-47a9-afd9-0271ed590c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_result_df = pd.read_csv(\"fish_survey_all_lakes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23abc693-0d72-4811-9451-7852d1cb49cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shannon_wiener_index(row):\n",
    "    # Remove NaN values from the row\n",
    "    row = row.dropna()\n",
    "    \n",
    "    # Total abundance in the row (sum of all species abundances)\n",
    "    total_abundance = row.sum()\n",
    "    \n",
    "    # If total abundance is zero, return 0 (no diversity)\n",
    "    if total_abundance == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Proportions of each species\n",
    "    proportions = row / total_abundance\n",
    "    \n",
    "    # Calculate the Shannon-Wiener index\n",
    "    shannon_index = -np.sum(proportions * np.log(proportions))\n",
    "    \n",
    "    return shannon_index\n",
    "\n",
    "def calculate_shannon_index(df):\n",
    "    # Apply the Shannon-Wiener index function to each row (excluding the first column)\n",
    "    shannon_indices = df.iloc[:, 1:].apply(shannon_wiener_index, axis=1)\n",
    "    \n",
    "    # Create a new DataFrame with the row identifier and Shannon index\n",
    "    result_df = pd.DataFrame({\n",
    "        'DOWNumber': df.iloc[:, 0],\n",
    "        'Shannon Index': shannon_indices\n",
    "    })\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf399a6f-6d1d-4db9-a154-1ff77cf01d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_richness(df):\n",
    "    # Count the number of non-NaN cells in each row, excluding the first column (identifier)\n",
    "    richness = df.iloc[:, 1:].notna().sum(axis=1)\n",
    "    \n",
    "    # Create a new DataFrame with the row identifier and richness\n",
    "    result_df = pd.DataFrame({\n",
    "        'DOWNumber': df.iloc[:, 0],\n",
    "        'Richness': richness\n",
    "    })\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c178b5-5a98-42c9-ac85-3ebc674e47d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiener_df = calculate_shannon_index(fish_result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8c0749-3519-4ddc-8c60-463fd50cfc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "richness_df = calculate_richness(fish_result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9c6fc1-f1f6-40ff-b991-6286e871612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamefish = ['DOWNumber', 'LMB', 'SMB', 'CRP', 'SUN', 'MUE', 'NOP', 'WAE','YEP']\n",
    "gamefish_df = fish_result_df[gamefish]\n",
    "gamefish_wiener_df = calculate_shannon_index(gamefish_df).rename(columns={'Shannon_Index': 'Shannon_Index_gamefish'})\n",
    "gamefish_richness_df = calculate_richness(gamefish_df).rename(columns={'Richness': 'Richness_gamefish'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec830b9e-eb3e-481d-9987-78cdcaca24b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamefish_comp = gamefish_wiener_df.merge(gamefish_richness_df, on = 'DOWNumber')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec0b789-80d5-4d85-9ec0-6523c12e7d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamefish_comp.to_csv('gamefish_comp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be16e3fc-563f-4618-98a5-a08aaa3654a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CAT = ['DOWNumber', 'BLB', 'BRB', 'BLH', 'CCF', 'FCF', 'STC', 'TPM','YEB']\n",
    "CAT_df = fish_result_df[CAT]\n",
    "CAT_wiener_df = calculate_shannon_index(CAT_df).rename(columns={'Shannon_Index': 'Shannon_Index_CAT'})\n",
    "CAT_richness_df = calculate_richness(CAT_df).rename(columns={'Richness': 'Richness_CAT'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989772f1-ac49-4260-bcbe-5cac724737bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "CENT = ['DOWNumber','BLC','BLG','CRP','SUN','GSF','HSF','LMB','OSS',\n",
    "'PMK','RKB','SMB','WAM','WHC']\n",
    "CENT_df = fish_result_df[CENT]\n",
    "CENT_wiener_df = calculate_shannon_index(CENT_df).rename(columns={'Shannon_Index': 'Shannon_Index_CENT'})\n",
    "CENT_richness_df = calculate_richness(CENT_df).rename(columns={'Richness': 'Richness_CENT'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61179a75-d6b7-470e-ad93-08eb2e13e75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DAR = ['DOWNumber','BSD','FTD','DAR','IOD','JND','LED',\n",
    "'MDD','DAR','RVD','SHD','WSD']\n",
    "DAR_df = fish_result_df[DAR]\n",
    "DAR_wiener_df = calculate_shannon_index(DAR_df).rename(columns={'Shannon_Index': 'Shannon_Index_DAR'})\n",
    "DAR_richness_df = calculate_richness(DAR_df).rename(columns={'Richness': 'Richness_DAR'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57478da5-4f5b-41ed-b394-e280b3b7a51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ESX = ['DOWNumber','MUE','NOP','TME']\n",
    "ESX_df = fish_result_df[ESX]\n",
    "ESX_wiener_df = calculate_shannon_index(ESX_df).rename(columns={'Shannon_Index': 'Shannon_Index_ESX'})\n",
    "ESX_richness_df = calculate_richness(ESX_df).rename(columns={'Richness': 'Richness_ESX'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae273f9f-ddda-4f20-8008-820203e1e175",
   "metadata": {},
   "outputs": [],
   "source": [
    "OTM = ['DOWNumber','CRC','HHC','SLC','BKF','GOF','BND',\n",
    "'FND','LND','NRD','PRD','BNM','BRM','BKS',\n",
    "'BHM','CNM','CSR','OTM','FHM','PGM','QBS','BMS','BCS','BNS','CSH',\n",
    "'EMS','SHI','GOS','MMS','PGS','RVS','SDS','SFS','SPO','WDS']\n",
    "OTM_df = fish_result_df[OTM]\n",
    "OTM_wiener_df = calculate_shannon_index(OTM_df).rename(columns={'Shannon_Index': 'Shannon_Index_OTM'})\n",
    "OTM_richness_df = calculate_richness(OTM_df).rename(columns={'Richness': 'Richness_OTM'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0cfd59-4adc-4b02-baf9-ff30be8114ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRCH = ['DOWNumber','SAR','WAE','WAS','YEP']\n",
    "PRCH_df = fish_result_df[PRCH]\n",
    "PRCH_wiener_df = calculate_shannon_index(PRCH_df).rename(columns={'Shannon_Index': 'Shannon_Index_PRCH'})\n",
    "PRCH_richness_df = calculate_richness(PRCH_df).rename(columns={'Richness': 'Richness_PRCH'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf201cb-919d-41c5-bbec-a04b497eca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAL_Other = ['DOWNumber','LKW','SJC','TLC']\n",
    "SAL_Other_df = fish_result_df[SAL_Other]\n",
    "SAL_Other_wiener_df = calculate_shannon_index(SAL_Other_df).rename(columns={'Shannon_Index': 'Shannon_Index_SAL_Other'})\n",
    "SAL_Other_richness_df = calculate_richness(SAL_Other_df).rename(columns={'Richness': 'Richness_SAL_Other'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6626219-7887-4d1e-8476-595c4085b25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCUL = ['DOWNumber','DWS','SCU','MTS','SMS']\n",
    "SCUL_df = fish_result_df[SCUL]\n",
    "SCUL_wiener_df = calculate_shannon_index(SCUL_df).rename(columns={'Shannon_Index': 'Shannon_Index_SCUL'})\n",
    "SCUL_richness_df = calculate_richness(SCUL_df).rename(columns={'Richness': 'Richness_SCUL'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003cfab8-e612-459d-a9c4-3ccb077fdda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUCK = ['DOWNumber','BIB','SAB','CAP','OTS','CPS','HFS','RCS','BLS',\n",
    "'WTS','RHS','GLR','GRR','RRH','SHR','SLR','LNS','NHS','SPS']\n",
    "SUCK_df = fish_result_df[SUCK]\n",
    "SUCK_wiener_df = calculate_shannon_index(SUCK_df).rename(columns={'Shannon_Index': 'Shannon_Index_SUCK'})\n",
    "SUCK_richness_df = calculate_richness(SUCK_df).rename(columns={'Richness': 'Richness_SUCK'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257690c7-c7cf-4294-b524-21fd9b1de0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list = [CAT_wiener_df, CENT_wiener_df, DAR_wiener_df, ESX_wiener_df, OTM_wiener_df, PRCH_wiener_df,\n",
    "             SAL_Other_wiener_df, SCUL_wiener_df, SUCK_wiener_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb796119-8669-4058-8a72-d0bc91ffec64",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = reduce(lambda left, right: pd.merge(left, right, on='DOWNumber', how='outer'), alpha_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59912f2-2c42-4388-abc7-7509ace70558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b90ed0d-1e64-4c00-a2a1-e654fe721d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the predictors you just created and export a final combined raster \n",
    "# Define input files\n",
    "input_files_bio_training = [\n",
    "    training_path + training_state_abbr +'_' + nas_name + \"_inv_richness.tif\", \n",
    "    training_path + training_state_abbr + \"_Native_Fish_Richness.tif\"]\n",
    "#Combine rasters into a multi-band GeoTIFF\n",
    "output_file_bio_training = training_path + training_state_abbr + '_' + nas_name + \"_combined_bio.tif\" # this will replace the other combined tif you made.\n",
    "combine_geotiffs(input_files_bio_training, output_file_bio_training)\n",
    "\n",
    "print(f\"‚úÖ Multi-band raster saved as {output_file_bio_training}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c5a859-49f9-48ad-a526-a663b10f8f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Note these distance predictors are highly correlated. You should make the predictor that is most relevant to your taxa\n",
    "or if your target does spread by both mechanisms make both and let the model sort it out. This is kind of nuanced as Road Distance may also\n",
    "be informative of AIS which were originally stocked and now spread by stream/river network such as Rainbow and Brook Trout.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86165c10-7536-4738-9335-cd7c2a8e2f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boat visitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06489da0-ce30-4107-9e12-538aab105f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505dd450-a411-4c88-ae82-064e97151260",
   "metadata": {},
   "outputs": [],
   "source": [
    "ramps_result = sum_numramps_by_dow(ramps_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92a9590-d1ca-48f0-9a56-2f4dd4e3ddeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b735174-34e4-4b28-a9d5-95c27afb4646",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0426fe-2d73-46ef-8b03-a7b3cd82727a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aac0a4-f318-4133-9506-119cf4b1f1ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92117b4c-5d65-4838-9a2f-eb95d3722505",
   "metadata": {},
   "source": [
    "# Road Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45e7eda-b7e0-4f36-bb6b-5970a0ea885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_roads_and_endpoints(\n",
    "    local_path,\n",
    "    state_abbr,\n",
    "    state_fips,\n",
    "    state_name,\n",
    "    buffered_water,\n",
    "    nas_id,\n",
    "    my_crs,\n",
    "    snap_dist=500\n",
    "):\n",
    "    def download_tiger_roads(state_fips, local_path):\n",
    "        url = f'https://www2.census.gov/geo/tiger/TIGER2022/PRISECROADS/tl_2022_{state_fips}_prisecroads.zip'\n",
    "        r = requests.get(url)\n",
    "        if r.status_code != 200:\n",
    "            raise Exception(f\"‚ùå Failed to download from {url}\")\n",
    "        z = zipfile.ZipFile(BytesIO(r.content))\n",
    "        z.extractall(path=local_path)\n",
    "\n",
    "    def download_ramps(local_path):\n",
    "        url = \"https://www.sciencebase.gov/catalog/file/get/63b81b50d34e92aad3cc004d?facet=Boatramps_United_States_final_20230104\"\n",
    "        extensions = {'.dbf', '.prj', '.shp', '.shx'}\n",
    "        response = requests.get(url, stream=True, timeout=60)\n",
    "        response.raise_for_status()\n",
    "        with zipfile.ZipFile(BytesIO(response.content)) as zip_ref:\n",
    "            os.makedirs(local_path, exist_ok=True)\n",
    "            for f in zip_ref.namelist():\n",
    "                if os.path.splitext(f)[1].lower() in extensions:\n",
    "                    zip_ref.extract(f, local_path)\n",
    "\n",
    "    def get_endpoints(geometry):\n",
    "        if geometry.geom_type == 'LineString':\n",
    "            return [geometry.coords[0], geometry.coords[-1]]\n",
    "        return []\n",
    "\n",
    "    def nas_api_call(nas_id, state_abbr):\n",
    "        url = f\"http://nas.er.usgs.gov/api/v2/occurrence/search?species_ID={nas_id}&state={state_abbr}\"\n",
    "        response = requests.get(url).json()\n",
    "        return pd.json_normalize(response, 'results')\n",
    "\n",
    "    def sjoin_nearest_to_centroid_replace_geom(left_gdf, right_gdf, **kwargs):\n",
    "        left_centroids = left_gdf.copy()\n",
    "        left_centroids[\"geometry_centroid\"] = left_centroids.geometry.centroid\n",
    "        left_centroids = left_centroids.set_geometry(\"geometry_centroid\")\n",
    "        right_temp = right_gdf.copy()\n",
    "        right_temp[\"geometry_right\"] = right_temp.geometry\n",
    "        right_temp = right_temp.set_geometry(\"geometry_right\")\n",
    "        joined = gpd.sjoin_nearest(left_centroids, right_temp, how=\"left\", **kwargs)\n",
    "        joined[\"distance\"] = joined.geometry_centroid.distance(joined[\"geometry_right\"])\n",
    "        result = left_gdf.copy()\n",
    "        result[\"geometry\"] = joined[\"geometry_right\"]\n",
    "        result[\"epointID\"] = joined[\"epointID\"]\n",
    "        result[\"distance_to_nearest\"] = joined[\"distance\"]\n",
    "        return result\n",
    "\n",
    "    # Step 1: Download ramps and roads\n",
    "    download_ramps(os.path.abspath(os.path.join(local_path)))\n",
    "    download_tiger_roads(state_fips, local_path)\n",
    "\n",
    "    # Step 2: Load data\n",
    "    ramps = gpd.read_file(local_path + 'Boatramps_United_States_final_20230104.shp').set_crs(my_crs, allow_override=True)\n",
    "    ramp_geo = ramps.loc[ramps['State'] == state_name, ['geometry']].copy()\n",
    "    ramp_geo['ramp_ID'] = range(1, len(ramp_geo) + 1)\n",
    "\n",
    "    buffered_water = buffered_water.to_crs(my_crs)\n",
    "    pos_data = nas_api_call(nas_id, state_abbr)\n",
    "    pos_gdf = gpd.GeoDataFrame(\n",
    "        pos_data[[\"decimalLatitude\", \"decimalLongitude\"]],\n",
    "        geometry=gpd.points_from_xy(pos_data.decimalLongitude, pos_data.decimalLatitude),\n",
    "        crs=\"EPSG:4326\"\n",
    "    ).to_crs(my_crs)\n",
    "\n",
    "    # Step 3: Identify water presence/absence\n",
    "    water_check = buffered_water.sjoin(pos_gdf, how=\"left\", predicate=\"contains\")\n",
    "    water_check = water_check.drop_duplicates(subset=\"waterID\")\n",
    "    neg_water = water_check[water_check['index_right'].isna()].drop(columns=[\"index_right\", \"decimalLatitude\", \"decimalLongitude\"], errors=\"ignore\")\n",
    "    pos_water = water_check.dropna(subset=[\"index_right\"]).drop(columns=[\"index_right\", \"decimalLatitude\", \"decimalLongitude\"], errors=\"ignore\")\n",
    "    pos_water[\"Present\"], neg_water[\"Present\"] = 1.0, 0.0\n",
    "    water_with_presence = pd.concat([pos_water, neg_water])\n",
    "\n",
    "    # Step 4: Match ramps to water\n",
    "    ramps_in_water = ramp_geo.sjoin(water_with_presence, how=\"left\", predicate=\"within\")\n",
    "    ramps_not_in_water = ramps_in_water[ramps_in_water['index_right'].isna()].drop(columns=[\"index_right\"], errors=\"ignore\").copy()\n",
    "    ramps_in_water = ramps_in_water.dropna(subset=[\"index_right\"]).drop(columns=[\"index_right\"], errors=\"ignore\")\n",
    "    ramps_in_water[\"waterID\"] = ramps_in_water[\"waterID\"].astype(\"int64\")\n",
    "\n",
    "    water_ids_with_ramps = ramps_in_water['waterID'].tolist()\n",
    "    water_no_ramps = water_with_presence[~water_with_presence['waterID'].isin(water_ids_with_ramps)]\n",
    "\n",
    "    # Step 5: Extract and deduplicate road endpoints\n",
    "    my_roads = gpd.read_file(os.path.join(local_path, f\"tl_2022_{state_fips}_prisecroads.shp\")).to_crs(my_crs)\n",
    "    endpoints = my_roads['geometry'].apply(get_endpoints).explode()\n",
    "    endpoints_df = pd.DataFrame(endpoints.tolist(), columns=['x', 'y']).drop_duplicates()\n",
    "    endpoints_gdf = gpd.GeoDataFrame(endpoints_df, geometry=gpd.points_from_xy(endpoints_df['x'], endpoints_df['y']), crs=my_crs).drop(columns=['x', 'y'])\n",
    "    endpoints_gdf['epointID'] = range(1, len(endpoints_gdf) + 1)\n",
    "\n",
    "    # Step 6: Snap ramps and lakes to endpoints\n",
    "    ramps_in_water_sj = sjoin_nearest_to_centroid_replace_geom(ramps_in_water, endpoints_gdf)\n",
    "    lakes_no_ramp_sj = sjoin_nearest_to_centroid_replace_geom(water_no_ramps, endpoints_gdf)\n",
    "    lakes_no_ramp_epoints = lakes_no_ramp_sj[['waterID', 'geometry', 'Present', 'epointID', 'distance_to_nearest']].dropna()\n",
    "    ramps_in_water_epoints = ramps_in_water_sj[['waterID', 'geometry', 'Present', 'epointID', 'distance_to_nearest']].dropna()\n",
    "    all_endpoints = pd.concat([ramps_in_water_epoints, lakes_no_ramp_epoints])\n",
    "    pos_endpoints = all_endpoints.loc[all_endpoints['Present'] == 1.0]\n",
    "    neg_endpoints = all_endpoints.loc[all_endpoints['Present'] == 0.0]\n",
    "\n",
    "    return my_roads, pos_endpoints, neg_endpoints, ramps_in_water_epoints, lakes_no_ramp_epoints\n",
    "\n",
    "def build_network_optimized(road_network_gdf, precision=6):\n",
    "    \"\"\"\n",
    "    Build a NetworkX graph from a GeoDataFrame of road LineStrings.\n",
    "\n",
    "    Nodes are rounded to a given precision to reduce floating-point redundancy.\n",
    "\n",
    "    Args:\n",
    "        road_network_gdf (GeoDataFrame): Must contain LineString or MultiLineString geometries.\n",
    "        precision (int): Number of decimal places to round coordinates to for node deduplication.\n",
    "\n",
    "    Returns:\n",
    "        networkx.Graph: Graph with nodes as (x, y) tuples and edges weighted by length.\n",
    "    \"\"\"\n",
    "    import networkx as nx\n",
    "    from shapely.geometry import LineString\n",
    "\n",
    "    G = nx.Graph()\n",
    "\n",
    "    for geom in road_network_gdf.geometry:\n",
    "        if geom is None:\n",
    "            continue\n",
    "\n",
    "        # Handle both LineString and MultiLineString\n",
    "        if isinstance(geom, LineString):\n",
    "            lines = [geom]\n",
    "        elif hasattr(geom, 'geoms'):  # MultiLineString\n",
    "            lines = list(geom.geoms)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        for line in lines:\n",
    "            coords = list(line.coords)\n",
    "            for u, v in zip(coords[:-1], coords[1:]):\n",
    "                if len(u) < 2 or len(v) < 2:\n",
    "                    continue  # skip invalid coordinate pairs\n",
    "\n",
    "                u_rounded = tuple(round(c, precision) for c in u[:2])\n",
    "                v_rounded = tuple(round(c, precision) for c in v[:2])\n",
    "                dist = LineString([u, v]).length\n",
    "\n",
    "                # Add undirected edge with distance as weight\n",
    "                G.add_edge(u_rounded, v_rounded, weight=dist)\n",
    "\n",
    "    return G\n",
    "\n",
    "def build_nearest_node_index(graph):\n",
    "    \"\"\"\n",
    "    Build a KDTree for efficient nearest-node lookup in a NetworkX graph.\n",
    "\n",
    "    Args:\n",
    "        graph (networkx.Graph): A graph with 2D tuple coordinates as nodes.\n",
    "\n",
    "    Returns:\n",
    "        cKDTree: KD-tree built from graph node coordinates.\n",
    "        list: List of node tuples in the same order as the KD-tree.\n",
    "    \"\"\"\n",
    "    # Ensure all nodes are 2D points (x, y)\n",
    "    nodes = [node for node in graph.nodes if len(node) == 2]\n",
    "    coords = [list(node) for node in nodes]  # convert tuples to lists for KDTree\n",
    "\n",
    "    tree = cKDTree(coords)\n",
    "    return tree, nodes\n",
    "    \n",
    "def find_nearest_node_kd_tree(kd_tree, point, precision=6):\n",
    "    \"\"\"\n",
    "    Find the nearest graph node to a given point using a KD-tree.\n",
    "\n",
    "    Args:\n",
    "        kd_tree (cKDTree): KDTree built from graph node coordinates.\n",
    "        point (shapely.geometry.Point): Point to find the nearest node to.\n",
    "        precision (int): Decimal places to round the coordinates.\n",
    "\n",
    "    Returns:\n",
    "        tuple: The (x, y) coordinates of the nearest node, rounded.\n",
    "    \"\"\"\n",
    "    _, index = kd_tree.query([point.x, point.y])\n",
    "    nearest_coords = kd_tree.data[index]\n",
    "    return tuple(round(coord, precision) for coord in nearest_coords)\n",
    "    \n",
    "def assign_endpoints_multi_source_dijkstra(presences_gdf, endpoints_gdf, G, kd_tree, precision=6):\n",
    "    \"\"\"\n",
    "    Assign each endpoint to its nearest presence based on road network distance.\n",
    "\n",
    "    Args:\n",
    "        presences_gdf (GeoDataFrame): GeoDataFrame of presence points with 'epointID'.\n",
    "        endpoints_gdf (GeoDataFrame): GeoDataFrame of negative endpoints with 'epointID'.\n",
    "        G (networkx.Graph): Graph of road network.\n",
    "        kd_tree (cKDTree): KDTree built from graph nodes.\n",
    "        precision (int): Decimal precision for rounding coordinates.\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: Each dict contains 'epointID', 'target_point_id', and 'distance_roads'.\n",
    "    \"\"\"\n",
    "    # Step 1: Snap presence points to graph nodes\n",
    "    presence_nodes = {}\n",
    "    node_to_presence_id = {}\n",
    "\n",
    "    for _, row in tqdm(presences_gdf.iterrows(), total=len(presences_gdf), desc=\"Snapping presences\"):\n",
    "        node = find_nearest_node_kd_tree(kd_tree, row.geometry.centroid, precision)\n",
    "        if not G.has_node(node):\n",
    "            continue  # Skip if the node isn't actually in the graph\n",
    "        presence_nodes[row[\"epointID\"]] = node\n",
    "        node_to_presence_id[node] = row[\"epointID\"]\n",
    "\n",
    "    if not presence_nodes:\n",
    "        raise ValueError(\"No presence points could be snapped to graph nodes.\")\n",
    "\n",
    "    # Step 2: Multi-source Dijkstra from all presence nodes\n",
    "    source_nodes = list(presence_nodes.values())\n",
    "    distances, predecessors = nx.multi_source_dijkstra(G, sources=source_nodes, weight='weight')\n",
    "\n",
    "    # Step 3: For each negative endpoint, find the closest presence\n",
    "    results = []\n",
    "\n",
    "    for _, row in tqdm(endpoints_gdf.iterrows(), total=len(endpoints_gdf), desc=\"Processing negatives\"):\n",
    "        neg_id = row[\"epointID\"]\n",
    "        neg_point = row.geometry.centroid\n",
    "        neg_node = find_nearest_node_kd_tree(kd_tree, neg_point, precision)\n",
    "\n",
    "        if not G.has_node(neg_node):\n",
    "            results.append({\n",
    "                \"epointID\": neg_id,\n",
    "                \"target_point_id\": None,\n",
    "                \"distance_roads\": np.inf\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        if neg_node in source_nodes:\n",
    "            # Direct match to a presence node\n",
    "            dist = 0.0\n",
    "            target_id = node_to_presence_id[neg_node]\n",
    "        elif neg_node in distances:\n",
    "            # Trace back the path to the nearest source\n",
    "            path = []\n",
    "            current = neg_node\n",
    "            while current not in source_nodes:\n",
    "                prev = predecessors.get(current)\n",
    "                if not prev:\n",
    "                    break\n",
    "                current = prev[0]  # Multi-source returns list of predecessors\n",
    "                path.append(current)\n",
    "\n",
    "            source_node = current if current in source_nodes else None\n",
    "            target_id = node_to_presence_id.get(source_node, None)\n",
    "            dist = distances[neg_node]\n",
    "        else:\n",
    "            # Not reachable\n",
    "            dist = np.inf\n",
    "            target_id = None\n",
    "\n",
    "        results.append({\n",
    "            \"epointID\": neg_id,\n",
    "            \"target_point_id\": target_id,\n",
    "            \"distance_roads\": dist\n",
    "        })\n",
    "\n",
    "    return results\n",
    "    \n",
    "def vector_to_raster(gdf, output_path, nas_name, value_field=None, resolution=100):\n",
    "    # Ensure geometries are valid\n",
    "    gdf = gdf[~gdf.geometry.isna()]\n",
    "    gdf = gdf[gdf.geometry.is_valid]\n",
    "\n",
    "    # Ensure it has a CRS\n",
    "    if gdf.crs is None:\n",
    "        raise ValueError(\"GeoDataFrame must have a CRS.\")\n",
    "\n",
    "    # Reproject to projected CRS if necessary\n",
    "    if gdf.crs.is_geographic:\n",
    "        gdf = gdf.to_crs(\"EPSG:5070\")  # NAD83 / Conus Albers (meters)\n",
    "\n",
    "    # Get bounds\n",
    "    minx, miny, maxx, maxy = gdf.total_bounds\n",
    "    width = int(np.ceil((maxx - minx) / resolution))\n",
    "    height = int(np.ceil((maxy - miny) / resolution))\n",
    "\n",
    "    # Check again\n",
    "    if width <= 0 or height <= 0:\n",
    "        raise ValueError(\"Calculated width or height is <= 0. Check your resolution and geometry bounds.\")\n",
    "\n",
    "    # Create transform\n",
    "    transform = from_origin(minx, maxy, resolution, resolution)\n",
    "\n",
    "    # Prepare shapes\n",
    "    shapes = (\n",
    "        ((geom, value) for geom, value in zip(gdf.geometry, gdf[value_field]))\n",
    "        if value_field else\n",
    "        ((geom, 1) for geom in gdf.geometry)\n",
    "    )\n",
    "\n",
    "    # Rasterize\n",
    "    raster = rasterize(\n",
    "        shapes=shapes,\n",
    "        out_shape=(height, width),\n",
    "        transform=transform,\n",
    "        fill= np.nan,\n",
    "        dtype='float32' if not value_field else 'float32'\n",
    "    )\n",
    "\n",
    "    # Write to GeoTIFF\n",
    "    band_name = f\"distance_road_{nas_name}\"\n",
    "    with rasterio.open(\n",
    "        output_path,\n",
    "        \"w\",\n",
    "        driver=\"GTiff\",\n",
    "        height=height,\n",
    "        width=width,\n",
    "        count=1,\n",
    "        dtype=raster.dtype,\n",
    "        crs=gdf.crs,\n",
    "        transform=transform,\n",
    "    ) as dst:\n",
    "        dst.write(raster, 1)\n",
    "        dst.set_band_description(1, band_name)\n",
    "\n",
    "    print(f\"Raster written to {output_path}\")\n",
    "\n",
    "def compute_and_export_road_distances(\n",
    "    my_roads: gpd.GeoDataFrame,\n",
    "    pos_endpoints: gpd.GeoDataFrame,\n",
    "    neg_endpoints: gpd.GeoDataFrame,\n",
    "    ramps_epoints: gpd.GeoDataFrame,\n",
    "    lakes_epoints: gpd.GeoDataFrame,\n",
    "    buffered_water: gpd.GeoDataFrame,\n",
    "    my_crs: str,\n",
    "    my_path: str,\n",
    "    my_state: str,\n",
    "    nas_name: str,\n",
    "    resolution: int = 100\n",
    ") -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Computes shortest road network distances from invasive presence points to absence points,\n",
    "    joins those distances to ramp/lake endpoints, and outputs a raster and plot.\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame: Waterbodies with distance_roads_total assigned.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure CRS consistency\n",
    "    my_roads = my_roads.to_crs(my_crs)\n",
    "    pos_endpoints = pos_endpoints.to_crs(my_crs)\n",
    "    neg_endpoints = neg_endpoints.to_crs(my_crs)\n",
    "\n",
    "    # Build the road network and nearest node index\n",
    "    G = build_network_optimized(my_roads)\n",
    "    kd_tree, graph_nodes = build_nearest_node_index(G)\n",
    "\n",
    "    # Run Dijkstra from positive to negative endpoints\n",
    "    dijkstra_results = assign_endpoints_multi_source_dijkstra(\n",
    "        presences_gdf=pos_endpoints,\n",
    "        endpoints_gdf=neg_endpoints,\n",
    "        G=G,\n",
    "        kd_tree=kd_tree,\n",
    "        precision=6\n",
    "    )\n",
    "    dist_df = pd.DataFrame(dijkstra_results)\n",
    "    dist_df = dist_df.loc[dist_df.groupby('epointID')['distance_roads'].idxmin()].reset_index(drop=True)\n",
    "\n",
    "    # Merge distances to endpoints\n",
    "    ramps_w_dist = pd.merge(ramps_epoints, dist_df, on='epointID', how='left')\n",
    "    lakes_w_dist = pd.merge(lakes_epoints, dist_df, on='epointID', how='left')\n",
    "\n",
    "    # Clean and select needed columns\n",
    "    keep_cols = ['geometry', 'target_point_id', 'epointID', 'waterID', 'Present', 'distance_roads', 'distance_to_nearest']\n",
    "    ramps_clean = ramps_w_dist[keep_cols].dropna()\n",
    "    lakes_clean = lakes_w_dist[keep_cols].dropna()\n",
    "\n",
    "    # Downcast numeric columns\n",
    "    for df in [ramps_clean, lakes_clean]:\n",
    "        for col in ['distance_roads', 'distance_to_nearest']:\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "\n",
    "    # Fill missing distances\n",
    "    ramps_clean['adjusted_distance_to_nearest'] = ramps_clean['distance_to_nearest']\n",
    "    lakes_clean['adjusted_distance_to_nearest'] = lakes_clean['distance_to_nearest']\n",
    "\n",
    "    ramps_clean['distance_roads_filled'] = ramps_clean['distance_roads'].fillna(0)\n",
    "    lakes_clean['distance_roads_filled'] = lakes_clean['distance_roads'].fillna(0)\n",
    "\n",
    "    # Compute total distances\n",
    "    ramps_clean['distance_roads_total'] = 0\n",
    "    ramps_clean.loc[ramps_clean['Present'] == 0, 'distance_roads_total'] = (\n",
    "        ramps_clean['distance_roads'] + ramps_clean['adjusted_distance_to_nearest']\n",
    "    )\n",
    "    ramps_clean.loc[ramps_clean['Present'] != 0, 'distance_roads_total'] = ramps_clean['distance_roads_filled']\n",
    "\n",
    "    lakes_clean['distance_roads_total'] = 0\n",
    "    lakes_clean.loc[lakes_clean['Present'] == 0, 'distance_roads_total'] = (\n",
    "        lakes_clean['distance_roads'] + lakes_clean['adjusted_distance_to_nearest']\n",
    "    )\n",
    "    lakes_clean.loc[lakes_clean['Present'] != 0, 'distance_roads_total'] = lakes_clean['distance_roads_filled']\n",
    "\n",
    "    # Clean up intermediate columns\n",
    "    ramps_clean.drop(columns=['distance_roads_filled'], inplace=True)\n",
    "    lakes_clean.drop(columns=['distance_roads_filled'], inplace=True)\n",
    "\n",
    "    # Final merge and assignment\n",
    "    final_dist_df = pd.concat([ramps_clean, lakes_clean], ignore_index=True)\n",
    "    min_dist_by_water = final_dist_df.loc[final_dist_df.groupby(\"waterID\")[\"distance_roads_total\"].idxmin()]\n",
    "    water_w_dist = pd.merge(buffered_water, min_dist_by_water, on=\"waterID\", how=\"left\")\n",
    "    water_w_dist_final = water_w_dist[[\"waterID\", \"geometry_x\", \"distance_roads_total\"]].rename(columns={\"geometry_x\": \"geometry\"}).dropna()\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    water_w_dist_final.plot(\n",
    "        ax=ax, column=\"distance_roads_total\", cmap=\"viridis\", linewidth=0.5, legend=True\n",
    "    )\n",
    "    ax.set_title(\"Water with Distance to Roads\", fontsize=14)\n",
    "    ax.set_xlabel(\"Longitude\")\n",
    "    ax.set_ylabel(\"Latitude\")\n",
    "    plt.show()\n",
    "\n",
    "    # Export raster\n",
    "    output_filename = f\"{my_path}{my_state}_road_distance_{nas_name}.tif\"\n",
    "    vector_to_raster(\n",
    "        gdf=water_w_dist_final,\n",
    "        output_path=output_filename,\n",
    "        nas_name=nas_name,  # must match function signature\n",
    "        value_field=\"distance_roads_total\",\n",
    "        resolution=resolution\n",
    "    )\n",
    "\n",
    "    return water_w_dist_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953688f1-bd76-451c-b2a2-577a51dec72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import and format data for road network/distance analysis\n",
    "my_roads, pos_endpoints, neg_endpoints, ramps_in_water_epoints, lakes_no_ramp_epoints = extract_roads_and_endpoints(\n",
    "    local_path=training_path,\n",
    "    state_abbr=training_state_abbr,\n",
    "    state_fips=training_fip,\n",
    "    state_name=training_state_abbr,\n",
    "    buffered_water = buffered_water_training,\n",
    "    nas_id=nas_id,\n",
    "    my_crs=my_crs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a84bab-686d-4aee-8ed4-afc7f6d7ce3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_with_dist = compute_and_export_road_distances(\n",
    "    my_roads=my_roads,\n",
    "    pos_endpoints=pos_endpoints,\n",
    "    neg_endpoints=neg_endpoints,\n",
    "    ramps_epoints=ramps_in_water_epoints,\n",
    "    lakes_epoints=lakes_no_ramp_epoints,\n",
    "    buffered_water=buffered_water_training,\n",
    "    my_crs=my_crs,\n",
    "    my_path= training_path,\n",
    "    my_state=training_state_abbr,\n",
    "    nas_name= nas_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90999ee6-0731-4f6e-a1ff-1d8e81df8c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Public launches\n",
    "# \n",
    "def sum_numramps_by_dow(gdf):\n",
    "    # Check if required columns are in the GeoDataFrame\n",
    "    if 'dow' not in gdf.columns or 'NUMRAMPS' not in gdf.columns:\n",
    "        raise ValueError(\"GeoDataFrame must contain 'dow' and 'NUMRAMPS' columns.\")\n",
    "    \n",
    "    # Group by 'dow' and sum the 'NUMRAMPS' column\n",
    "    grouped = gdf.groupby('dow')['NUMRAMPS'].sum().reset_index()\n",
    "    \n",
    "    return grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4ac656-ef2a-4b07-80f2-a47082fff9e0",
   "metadata": {},
   "source": [
    "# Stream/River Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b571b8-11c9-4aba-8365-d6086a584503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance to source streams\n",
    "discon_value = 999999.0  # Define a consistent disconnection value\n",
    "@contextmanager\n",
    "def tqdm_joblib(tqdm_object):\n",
    "    class TqdmBatchCompletionCallback(joblib.parallel.BatchCompletionCallBack):\n",
    "        def __call__(self, *args, **kwargs):\n",
    "            tqdm_object.update(n=self.batch_size)\n",
    "            return super().__call__(*args, **kwargs)\n",
    "\n",
    "    old_callback = joblib.parallel.BatchCompletionCallBack\n",
    "    joblib.parallel.BatchCompletionCallBack = TqdmBatchCompletionCallback\n",
    "    try:\n",
    "        yield tqdm_object\n",
    "    finally:\n",
    "        joblib.parallel.BatchCompletionCallBack = old_callback\n",
    "        tqdm_object.close()\n",
    "def build_network(streams_gdf):\n",
    "    G = nx.DiGraph()\n",
    "    for geom in streams_gdf.geometry:\n",
    "        if geom is None or geom.is_empty:\n",
    "            continue\n",
    "\n",
    "        if isinstance(geom, LineString):\n",
    "            coords = list(geom.coords)\n",
    "            edges = zip(coords[:-1], coords[1:])\n",
    "        elif isinstance(geom, MultiLineString):\n",
    "            edges = []\n",
    "            for line in geom:\n",
    "                coords = list(line.coords)\n",
    "                edges.extend(zip(coords[:-1], coords[1:]))\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        for u, v in edges:\n",
    "            u_r = (round(u[0], 6), round(u[1], 6))\n",
    "            v_r = (round(v[0], 6), round(v[1], 6))\n",
    "            if u_r != v_r:\n",
    "                G.add_edge(u_r, v_r, weight=Point(u_r).distance(Point(v_r)))\n",
    "    return G\n",
    "\n",
    "def snap_points_to_vertices(points_gdf, vertices_tree, vertices_coords):\n",
    "    coords = np.array([(geom.x, geom.y) for geom in points_gdf.geometry])\n",
    "    _, idxs = vertices_tree.query(coords, k=1)\n",
    "    snapped_points = vertices_coords[idxs]\n",
    "    return snapped_points, idxs\n",
    "def calculate_dijkstra_for_point(source_node, G, vertices_in_buffer):\n",
    "    if source_node not in G:\n",
    "        print(f\"‚ö†Ô∏è  Source node {source_node} not in graph. Skipping.\")\n",
    "        return np.full(len(vertices_in_buffer), discon_value, dtype=np.float32)\n",
    "\n",
    "    try:\n",
    "        lengths = nx.single_source_dijkstra_path_length(G, source_node, weight='weight')\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error running Dijkstra from {source_node}: {e}\")\n",
    "        return np.full(len(vertices_in_buffer), discon_value, dtype=np.float32)\n",
    "\n",
    "    vertex_distances = np.full(len(vertices_in_buffer), discon_value, dtype=np.float32)\n",
    "    reachable = 0\n",
    "    for j, target_geom in enumerate(vertices_in_buffer.geometry):\n",
    "        target_node = (round(target_geom.x, 6), round(target_geom.y, 6))\n",
    "        if target_node in lengths:\n",
    "            vertex_distances[j] = lengths[target_node]\n",
    "            reachable += 1\n",
    "\n",
    "    print(f\"‚úÖ Source {source_node}: reached {reachable} / {len(vertices_in_buffer)} targets\")\n",
    "    return vertex_distances\n",
    "# --- Main Processing Function ---\n",
    "def process_state(state_abbr, my_path, streams_all, presence_all):\n",
    "    print(f\"\\n--- Processing {state_abbr} ---\")\n",
    "    shp_dir = os.path.join(my_path, f\"{state_abbr}_vertex_distances\")\n",
    "    os.makedirs(shp_dir, exist_ok=True)\n",
    "    shp_path = os.path.join(my_path, f\"{state_abbr}_vertex_distances.gpkg\")\n",
    "\n",
    "    try:\n",
    "        if streams_all.empty:\n",
    "            print(f\"Skipping {state_abbr}: no streams found.\")\n",
    "            return None\n",
    "\n",
    "        # Ensure CRS is consistent\n",
    "        streams = streams_all.copy()\n",
    "        if presence_all.crs != streams.crs:\n",
    "            presence_points = presence_all.to_crs(streams.crs)\n",
    "            print(f\"üìê Reprojected presence points to match stream CRS: {streams.crs}\")\n",
    "        else:\n",
    "            presence_points = presence_all.copy()\n",
    "\n",
    "        if presence_points.empty:\n",
    "            print(f\"{state_abbr}: no presence points found in state. Skipping.\")\n",
    "            return None\n",
    "\n",
    "        print(f\"Extracting vertices for {state_abbr}...\")\n",
    "        all_vertices = []\n",
    "        for geom in tqdm(streams.geometry, desc=f\"{state_abbr} stream vertices\", unit=\"geom\"):\n",
    "            if geom is None or geom.is_empty:\n",
    "                continue\n",
    "            if isinstance(geom, LineString):\n",
    "                all_vertices.extend(geom.coords)\n",
    "            elif isinstance(geom, MultiLineString):\n",
    "                for line in geom.geoms:\n",
    "                    all_vertices.extend(line.coords)\n",
    "\n",
    "        vertices_gdf = gpd.GeoDataFrame(\n",
    "            geometry=gpd.points_from_xy(\n",
    "                [pt[0] for pt in all_vertices],\n",
    "                [pt[1] for pt in all_vertices]\n",
    "            ),\n",
    "            crs=streams.crs\n",
    "        ).drop_duplicates()\n",
    "\n",
    "        vertices_gdf = vertices_gdf[~((vertices_gdf.geometry.x == 0) & (vertices_gdf.geometry.y == 0))]\n",
    "        vertices_coords = np.array([(pt.x, pt.y) for pt in vertices_gdf.geometry])\n",
    "        vertices_tree = cKDTree(vertices_coords)\n",
    "\n",
    "        G = build_network(streams)\n",
    "        print(f\"üìä Graph has {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
    "\n",
    "        snapped_coords, _ = snap_points_to_vertices(presence_points, vertices_tree, vertices_coords)\n",
    "        snapped_coords = np.round(snapped_coords, 6)\n",
    "\n",
    "        # Validate source nodes\n",
    "        valid_sources = [tuple(coord) for coord in snapped_coords if tuple(coord) in G]\n",
    "        print(f\"‚úÖ Valid source nodes in graph: {len(valid_sources)} / {len(snapped_coords)}\")\n",
    "\n",
    "        def is_node_in_graph(geom):\n",
    "            return (round(geom.x, 6), round(geom.y, 6)) in G\n",
    "\n",
    "        vertices_in_graph = vertices_gdf[vertices_gdf.geometry.apply(is_node_in_graph)]\n",
    "\n",
    "        if vertices_in_graph.empty:\n",
    "            print(f\"{state_abbr}: no stream vertices found in graph. Skipping.\")\n",
    "            return None\n",
    "\n",
    "        print(f\"Running parallel Dijkstra for {len(valid_sources)} valid presence-adjacent points in {state_abbr}...\")\n",
    "\n",
    "        presence_tree = cKDTree(snapped_coords)\n",
    "        vertex_coords = np.array([(geom.x, geom.y) for geom in vertices_in_graph.geometry])\n",
    "        _, nearest_presence_idxs = presence_tree.query(vertex_coords, k=1)\n",
    "\n",
    "        presence_to_vertices = defaultdict(list)\n",
    "        for v_idx, p_idx in enumerate(nearest_presence_idxs):\n",
    "            presence_to_vertices[p_idx].append(v_idx)\n",
    "\n",
    "        print(f\"Running optimized Dijkstra for {len(presence_to_vertices)} unique presence points in {state_abbr}...\")\n",
    "\n",
    "        min_distances = np.full(len(vertices_in_graph), discon_value, dtype=np.float32)\n",
    "\n",
    "        with tqdm_joblib(tqdm(desc=f\"{state_abbr} Dijkstra\", total=len(presence_to_vertices), unit=\"pt\")):\n",
    "            results = Parallel(n_jobs=2, backend=\"threading\")(\n",
    "                delayed(calculate_dijkstra_for_point)(\n",
    "                    tuple(snapped_coords[p_idx]),\n",
    "                    G,\n",
    "                    vertices_in_graph.iloc[v_idxs]\n",
    "                )\n",
    "                for p_idx, v_idxs in presence_to_vertices.items()\n",
    "                if tuple(snapped_coords[p_idx]) in G\n",
    "            )\n",
    "\n",
    "        for (p_idx, v_idxs), dist_array in zip(presence_to_vertices.items(), results):\n",
    "            for local_idx, global_idx in enumerate(v_idxs):\n",
    "                min_distances[global_idx] = min(min_distances[global_idx], dist_array[local_idx])\n",
    "\n",
    "        vertices_in_graph[\"distance_r\"] = min_distances.astype(np.float32)\n",
    "\n",
    "        print(f\"üìà distance_r summary: min={np.min(min_distances[min_distances < discon_value]):.2f}, \"\n",
    "              f\"max={np.max(min_distances[min_distances < discon_value]):.2f}, \"\n",
    "              f\"unreachable={(min_distances == discon_value).sum()}\")\n",
    "\n",
    "        vertices_in_graph.to_file(shp_path, driver=\"GPKG\")\n",
    "        print(f\"‚úÖ {state_abbr} vertex shapefile exported to: {shp_path}\")\n",
    "        return shp_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in {state_abbr}: {e}\")\n",
    "        return None\n",
    "# Cell 2: Utility functions\n",
    "\n",
    "def safe_read_file(fp):\n",
    "    try:\n",
    "        return gpd.read_file(fp)\n",
    "    except Exception as e:\n",
    "        raise IOError(f\"Error reading {fp}: {e}\")\n",
    "\n",
    "def build_kdtree(vertices):\n",
    "    coords = np.array(list(zip(vertices.geometry.x, vertices.geometry.y)))\n",
    "    tree = cKDTree(coords)\n",
    "    return tree, coords\n",
    "\n",
    "def query_parallel(tree, points, n_jobs=-1):\n",
    "    results = Parallel(n_jobs=n_jobs, backend=\"threading\")(\n",
    "        delayed(tree.query)(pt) for pt in tqdm(points, desc=\"Querying KDTree\", unit=\"pt\")\n",
    "    )\n",
    "    return zip(*results)\n",
    "\n",
    "def add_line_ids(stream_lines):\n",
    "    stream_lines['line_id'] = stream_lines.index.astype(str)\n",
    "    return stream_lines\n",
    "\n",
    "def assign_vertices_to_lines(vertices, stream_lines):\n",
    "    assert vertices.crs == stream_lines.crs, \"CRS mismatch between vertices and stream lines\"\n",
    "    print(\"Assigning vertices to stream lines via spatial join...\")\n",
    "    joined = gpd.sjoin(vertices, stream_lines[['line_id', 'geometry']], how='left', predicate='within')\n",
    "    if joined['line_id'].isna().all():\n",
    "        raise ValueError(\"No vertices could be matched to stream lines. Check geometries.\")\n",
    "    return joined\n",
    "\n",
    "def filter_disconnected_lines(vertices):\n",
    "    print(\"Filtering disconnected stream segments...\")\n",
    "    bad_line_ids = (\n",
    "        vertices.groupby('line_id')['distance_r']\n",
    "        .apply(lambda x: (x == discon_value).all())\n",
    "        .loc[lambda x: x].index\n",
    "    )\n",
    "    return bad_line_ids\n",
    "\n",
    "def split_waterbodies_by_connection(waterbodies, stream_lines, bad_line_ids):\n",
    "    print(\"Splitting waterbodies into connected and disconnected sets...\")\n",
    "    disconnected = stream_lines[stream_lines['line_id'].isin(bad_line_ids)]\n",
    "    connected = stream_lines[~stream_lines['line_id'].isin(bad_line_ids)]\n",
    "    waterbodies = waterbodies.to_crs(stream_lines.crs)\n",
    "    wb_disconnected = gpd.sjoin(waterbodies, disconnected, how='inner', predicate='intersects')\n",
    "    wb_connected = gpd.sjoin(waterbodies, connected, how='inner', predicate='intersects')\n",
    "    wb_disconnected_ids = set(wb_disconnected.index)\n",
    "    wb_connected_ids = set(wb_connected.index)\n",
    "    purely_disconnected_ids = wb_disconnected_ids - wb_connected_ids\n",
    "    return (\n",
    "        waterbodies[~waterbodies.index.isin(purely_disconnected_ids)],\n",
    "        waterbodies[waterbodies.index.isin(purely_disconnected_ids)]\n",
    "    )\n",
    "# Cell 3: Rasterization and plotting\n",
    "def rasterize_distances(vertices, waterbodies_connected, waterbodies_disconnected, output_raster_fp, nas_name, pixel_size=30):\n",
    "    print(\"Rasterizing distances to source...\")\n",
    "    vertices = vertices[vertices['distance_r'].notna()]\n",
    "    vertices = vertices[vertices['distance_r'] != discon_value]\n",
    "    vertices['distance_r'] = vertices['distance_r'].astype(float)\n",
    "\n",
    "    tree, coords = build_kdtree(vertices)\n",
    "    values = vertices['distance_r'].values\n",
    "\n",
    "    all_waterbodies = gpd.GeoSeries(pd.concat([waterbodies_connected.geometry, waterbodies_disconnected.geometry]))\n",
    "    minx, miny, maxx, maxy = all_waterbodies.total_bounds\n",
    "    width = int(np.ceil((maxx - minx) / pixel_size))\n",
    "    height = int(np.ceil((maxy - miny) / pixel_size))\n",
    "    transform = from_bounds(minx, miny, maxx, maxy, width, height)\n",
    "\n",
    "    x_coords = np.linspace(minx + pixel_size / 2, maxx - pixel_size / 2, width)\n",
    "    y_coords = np.linspace(maxy - pixel_size / 2, miny + pixel_size / 2, height)\n",
    "    xx, yy = np.meshgrid(x_coords, y_coords)\n",
    "    pixel_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    print(\"Rasterizing waterbody masks...\")\n",
    "    all_waterbody_mask = rasterize(\n",
    "        [(geom, 1) for geom in all_waterbodies],\n",
    "        out_shape=(height, width),\n",
    "        transform=transform,\n",
    "        fill=0,\n",
    "        dtype='uint8'\n",
    "    )\n",
    "    connected_mask = rasterize(\n",
    "        [(geom, 1) for geom in waterbodies_connected.geometry],\n",
    "        out_shape=(height, width),\n",
    "        transform=transform,\n",
    "        fill=0,\n",
    "        dtype='uint8'\n",
    "    )\n",
    "\n",
    "    output_array = np.full((height, width), discon_value, dtype='float32')\n",
    "    idx_connected = np.where(connected_mask.ravel() == 1)[0]\n",
    "\n",
    "    print(f\"Querying {len(idx_connected)} connected pixels to KDTree...\")\n",
    "    dist, nn_idx = query_parallel(tree, list(pixel_points[idx_connected]))\n",
    "    output_array.ravel()[idx_connected] = values[list(nn_idx)]\n",
    "    output_array[all_waterbody_mask == 0] = np.nan\n",
    "\n",
    "    valid_mask = output_array != discon_value\n",
    "    if np.any(valid_mask):\n",
    "        max_val = np.nanmax(output_array[valid_mask])\n",
    "        output_array[~valid_mask] = max_val\n",
    "    else:\n",
    "        print(\"Warning: No valid distances found. Raster may be empty.\")\n",
    "\n",
    "    print(f\"Writing output raster to {output_raster_fp}...\")\n",
    "    band_name = f\"distance_river_{nas_name}\"\n",
    "    with rasterio.open(\n",
    "        output_raster_fp,\n",
    "        'w',\n",
    "        driver='GTiff',\n",
    "        height=height,\n",
    "        width=width,\n",
    "        count=1,\n",
    "        dtype='float32',\n",
    "        crs=waterbodies_connected.crs,\n",
    "        transform=transform,\n",
    "        nodata=np.nan\n",
    "    ) as dst:\n",
    "        dst.write(output_array, 1)\n",
    "        dst.set_band_description(1, band_name)\n",
    "\n",
    "    return output_array\n",
    "\n",
    "def preview_raster(array):\n",
    "    plt.imshow(array, cmap='viridis')\n",
    "    plt.colorbar(label='Distance to Source')\n",
    "    plt.title('Rasterized Distance to Stream Source')\n",
    "    plt.show()\n",
    "\n",
    "def log_metadata(output_fp, metadata_dict):\n",
    "    print(f\"Logging metadata to {output_fp}...\")\n",
    "    with open(output_fp, 'w') as f:\n",
    "        json.dump(metadata_dict, f, indent=2)\n",
    "\n",
    "def prepare_stream_network(stream_path, simplify_tolerance, crs):\n",
    "    streams = gpd.read_file(stream_path).to_crs(crs)\n",
    "    streams = streams.drop_duplicates(subset='geometry').reset_index(drop=True)\n",
    "    dissolved = unary_union(streams.geometry)\n",
    "    simplified = gpd.GeoDataFrame(geometry=[dissolved], crs=crs).explode(index_parts=False).reset_index(drop=True)\n",
    "\n",
    "    if simplify_tolerance is not None:\n",
    "        simplified[\"geometry\"] = simplified.simplify(tolerance=simplify_tolerance, preserve_topology=True)\n",
    "\n",
    "    return simplified\n",
    "    \n",
    "def prepare_presence_points(presence_path, crs):\n",
    "    presence = gpd.read_file(presence_path)\n",
    "    return presence.to_crs(crs)\n",
    "def run_stream_distance_model(state_abbr, streams_all, presence_all, output_dir):\n",
    "    return process_state(\n",
    "        state_abbr=state_abbr,\n",
    "        streams_all=streams_all,\n",
    "        presence_all=presence_all,\n",
    "        my_path=output_dir\n",
    "    )    \n",
    "\n",
    "def postprocess_stream_network(vertices_fp, streams_fp, water_fp):\n",
    "    streams = add_line_ids(safe_read_file(streams_fp))\n",
    "    vertices = safe_read_file(vertices_fp)\n",
    "    waterbodies = safe_read_file(water_fp)\n",
    "    \n",
    "    vertices = assign_vertices_to_lines(vertices.to_crs(streams.crs), streams)\n",
    "    bad_line_ids = filter_disconnected_lines(vertices)\n",
    "    wb_connected, wb_disconnected = split_waterbodies_by_connection(waterbodies, streams, bad_line_ids)\n",
    "    \n",
    "    return vertices, wb_connected, wb_disconnected, bad_line_ids\n",
    "\n",
    "def make_stream_distance_raster(vertices, wb_connected, wb_disconnected, output_fp, metadata_fp, nas_name, pixel_size=100, state_abbr=None):\n",
    "    array = rasterize_distances(vertices, wb_connected, wb_disconnected, output_fp, nas_name, pixel_size)\n",
    "    log_metadata(metadata_fp, {\n",
    "        'state': state_abbr,\n",
    "        'pixel_size': pixel_size,\n",
    "        'bad_line_ids': list(filter_disconnected_lines(vertices)),\n",
    "        'vertex_count': len(vertices),\n",
    "        'raster_output': output_fp\n",
    "    })\n",
    "    return array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22bdbf4-1a25-42f6-a5ca-89ea3cd8b447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "stream_path = os.path.join(training_path, f\"full_streams.shp\")\n",
    "presence_path = os.path.join(training_path, f\"{training_state_abbr}_{nas_name}_pos_data.shp\")\n",
    "output_raster_fp = os.path.join(training_path, f\"{training_state_abbr}_dist_to_src_river_{nas_name}.tif\")\n",
    "metadata_fp = output_raster_fp.replace('.tif', '_metadata.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac67c9a-a217-42ff-a20c-e501af7a8f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare inputs\n",
    "streams_all = prepare_stream_network(stream_path, simplify_tolerance=1000, crs=my_crs)\n",
    "presence_all = prepare_presence_points(presence_path, crs=my_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797c1bc3-50a0-41e6-828a-0f3147056837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Run Dijkstra model\n",
    "run_stream_distance_model(training_state_abbr, streams_all, presence_all, training_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d74554-55c4-4448-b6fa-a46997b7241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Postprocess results\n",
    "vertices_fp = os.path.join(training_path, f\"{training_state_abbr}_vertex_distances.gpkg\")\n",
    "streams_fp = stream_path\n",
    "water_fp = os.path.join(training_path, f\"{training_state_abbr}_buffered_water.shp\")\n",
    "vertices, wb_conn, wb_disc, _ = postprocess_stream_network(vertices_fp, streams_fp, water_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09ab9da-3756-4146-a470-65d2aff38cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Rasterize\n",
    "raster_array = make_stream_distance_raster(vertices, wb_conn, wb_disc, output_raster_fp, metadata_fp, nas_name, pixel_size=100, state_abbr=training_state_abbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf31ded-47b0-4e3f-a13b-a18f2bd8b350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the predictors you just created and export a final combined raster \n",
    "# Define input files\n",
    "input_files_dist_training = [\n",
    "    training_path + training_state_abbr +'_road_distance_' + nas_name + \".tif\", \n",
    "    training_path + training_state_abbr + '_dist_to_src_river_' + nas_name + '.tif']\n",
    "#Combine rasters into a multi-band GeoTIFF\n",
    "output_file_dist_training = training_path + training_state_abbr + '_' + nas_name + \"_combined_dist.tif\" # this will replace the other combined tif you made.\n",
    "combine_geotiffs(input_files_dist_training, output_file_dist_training)\n",
    "\n",
    "print(f\"‚úÖ Multi-band raster saved as {output_file_dist_training}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249553ef-b341-45e8-ae99-47d94447f86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Homerange similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b627c1e-d186-46d8-a59e-d9f6d03b4d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***You need to run this GEE script for your state and homerange first***\n",
    "# https://code.earthengine.google.com/b5d49bb675cc2d6583e866dc1dfb440b\n",
    "#get taxon key for your AIS from gbif\n",
    "#species.name_backbone(name='Dreissena polymorpha', kingdom='animal')\n",
    "my_training_state = 'MN' # should be the postal code abbreviation for the state you created the environmental raster for....\n",
    "my_nas_id = 5 # go to USGS NAS database for species_ids (e.g., 5 = Zebra Mussels; 237 = Eurasian watermilfoil; 551 = Bighead carp)\n",
    "my_path = 'data/' + my_training_state + '/'\n",
    "homerange_raster = my_path + \"homerange_2003_2022.tif\"\n",
    "invaded_raster = my_path + \"inv_rsd_2003_2022.tif\"\n",
    "my_countries = [\"RU\", \"UA\", \"BG\", \"RO\", \"GE\", \"AZ\", \"TM\", \"KZ\"] # Endemic range countries for your taxa\n",
    "my_taxon = 2287072  # gbif taxon id ; Eurasian watermilfoil = 2362486; Zebra mussels = 2362486\n",
    "limit = 10000 # This is for the gbif function so you don't blow up your computer... Just kidding that shouldn't happen : )\n",
    "my_scale = 1000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f680bb5-e796-487e-9837-54a155328219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def gbif_api_call(taxon, country, limit):\n",
    "    \"\"\"Fetch GBIF occurrences for a given taxon and country.\"\"\"\n",
    "    URL_BASE = 'https://api.gbif.org/v1/'\n",
    "    url_request = f\"{URL_BASE}occurrence/search?taxonKey={taxon}&country={country}&limit={limit}\"  \n",
    "    response = requests.get(url_request, timeout=30)\n",
    "    return response.json()  # Return the JSON response directly\n",
    "\n",
    "def nas_api_call(nas_id, state):\n",
    "    URL_BASE = 'http://nas.er.usgs.gov/api/v2/'\n",
    "    url_request = f\"{URL_BASE}/occurrence/search?species_ID={nas_id}&state={my_training_state}\"\n",
    "    response = requests.get(url_request, timeout=None).json()\n",
    "    results = pd.json_normalize(response, 'results')\n",
    "    return results\n",
    "\n",
    "\n",
    "def sample_multiband_geotiff_with_names(raster_path, gdf):\n",
    "    \"\"\"\n",
    "    Samples a multi-band GeoTIFF at specified point locations from a GeoDataFrame,\n",
    "    using band names from the raster.\n",
    "\n",
    "    Parameters:\n",
    "    - raster_path (str): Path to the GeoTIFF file.\n",
    "    - gdf (GeoDataFrame): GeoDataFrame containing point geometries.\n",
    "\n",
    "    Returns:\n",
    "    - GeoDataFrame with additional columns for each band, using raster band names.\n",
    "    \"\"\"\n",
    "\n",
    "    # Open the raster file\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        # Reproject GeoDataFrame to match raster CRS if needed\n",
    "        if gdf.crs != src.crs:\n",
    "            gdf = gdf.to_crs(src.crs)\n",
    "\n",
    "        # Convert point geometries to raster pixel coordinates\n",
    "        coords = [(geom.x, geom.y) for geom in gdf.geometry]\n",
    "\n",
    "        # Sample raster at point locations (returns a list of tuples with values per band)\n",
    "        sampled_values = list(src.sample(coords))\n",
    "\n",
    "        # Get band names (if available, otherwise use default names)\n",
    "        band_names = src.descriptions if all(src.descriptions) else [f\"band_{i+1}\" for i in range(src.count)]\n",
    "\n",
    "        # Create new columns in the GeoDataFrame with the corresponding band names\n",
    "        for band_idx, band_name in enumerate(band_names):\n",
    "            gdf[band_name] = [val[band_idx] for val in sampled_values]\n",
    "\n",
    "    return gdf\n",
    "\n",
    "def filter_dataframe_columns(df, feature_choices):\n",
    "    return df[[col for col in df.columns if col in feature_choices or col == \"geometry\"]]\n",
    "\n",
    "def extract_fields(data):\n",
    "    \"\"\"Extract relevant fields from GBIF response.\"\"\"\n",
    "    extracted_data = []\n",
    "    for record in data:\n",
    "        entry = {\n",
    "            'key': record.get('key'),\n",
    "            'species': record.get('species'),\n",
    "            'decimalLatitude': record.get('decimalLatitude'),\n",
    "            'decimalLongitude': record.get('decimalLongitude'),\n",
    "            'countryCode': record.get('countryCode'),\n",
    "            'year': record.get('year')\n",
    "        }\n",
    "        extracted_data.append(entry)\n",
    "    return extracted_data\n",
    "\n",
    "def MESS(ref_df, pred_df):\n",
    "    # Extract geometry before dropping it\n",
    "    geometry = None\n",
    "    if \"geometry\" in pred_df.columns:\n",
    "        geometry = pred_df[\"geometry\"].copy()  # Save geometry separately\n",
    "        pred_df = pred_df.drop(columns=[\"geometry\", \"predID\"])  # Drop before calculations\n",
    "\n",
    "    # Ensure reference DataFrame does not include geometry\n",
    "    ref_numeric = ref_df.drop(columns=[\"geometry\"], errors=\"ignore\")  # Avoid geometry errors\n",
    "\n",
    "    # Compute min and max values for each variable\n",
    "    mins = dict(ref_numeric.min())\n",
    "    maxs = dict(ref_numeric.max())\n",
    "\n",
    "    def calculate_s(column):\n",
    "        values = ref_numeric[column]  # Reference values\n",
    "        sims = []\n",
    "\n",
    "        for element in np.array(pred_df[column]):\n",
    "            f = np.count_nonzero((values < element)) / values.size\n",
    "\n",
    "            if f == 0:\n",
    "                sim = ((element - mins[column]) / (maxs[column] - mins[column]))\n",
    "            elif 0 < f <= 50:\n",
    "                sim = 2 * f\n",
    "            elif 50 < f < 100:\n",
    "                sim = 2 * (1 - f)\n",
    "            elif f == 100:\n",
    "                sim = ((maxs[column] - element) / (maxs[column] - mins[column]))\n",
    "\n",
    "            sims.append(sim)\n",
    "\n",
    "        return sims\n",
    "\n",
    "    # Compute similarity scores for each predictor\n",
    "    sim_df = pd.DataFrame()\n",
    "    for c in pred_df.columns:\n",
    "        sim_df[c] = calculate_s(c)\n",
    "\n",
    "    # Compute MESS values\n",
    "    min_similarity = sim_df.min(axis=1)  # Least similar predictor's score\n",
    "    MoD = sim_df.idxmin(axis=1)  # Least similar predictor's name\n",
    "\n",
    "    # Combine results\n",
    "    MESS = pd.concat([min_similarity, MoD], axis=1)\n",
    "    MESS.columns = [\"MESS_Score\", \"Least_Similar_Variable\"]\n",
    "\n",
    "    # Reattach geometry if it was present\n",
    "    if geometry is not None:\n",
    "        print(\"Before reattaching geometry:\", MESS.dtypes)  # Debug print\n",
    "    \n",
    "        MESS[\"geometry\"] = geometry  # Re-add geometry\n",
    "        MESS = gpd.GeoDataFrame(MESS, geometry=\"geometry\", crs=4269)  # Convert back to GeoDataFrame\n",
    "        \n",
    "        print(\"After reattaching geometry:\", MESS.dtypes)  # Debug print\n",
    "        print(\"Geometry column exists?\", \"geometry\" in MESS.columns)\n",
    "    \n",
    "    return MESS\n",
    "\n",
    "def export_mess(joined_gdf: gpd.GeoDataFrame, resolution: int = my_scale):\n",
    "    # Ensure CRS is projected (use EPSG:5070 or appropriate for your region)\n",
    "    if joined_gdf.crs.to_epsg() != 5070:\n",
    "        joined_gdf = joined_gdf.to_crs(epsg=5070)\n",
    "\n",
    "    # Get bounds\n",
    "    bounds = joined_gdf.total_bounds  # [minx, miny, maxx, maxy]\n",
    "    print(f\"Bounds in projected CRS: {bounds}\")\n",
    "\n",
    "    # Compute raster size\n",
    "    width = int(np.ceil((bounds[2] - bounds[0]) / resolution))\n",
    "    height = int(np.ceil((bounds[3] - bounds[1]) / resolution))\n",
    "\n",
    "    if width <= 0 or height <= 0:\n",
    "        raise ValueError(f\"Invalid raster dimensions: width={width}, height={height}\")\n",
    "\n",
    "    # Define transform\n",
    "    transform = rasterio.transform.from_origin(bounds[0], bounds[3], resolution, resolution)\n",
    "\n",
    "    # Ensure \"mess\" column exists and is numeric\n",
    "    column_name = \"MESS_Score\"\n",
    "    if column_name not in joined_gdf.columns:\n",
    "        raise KeyError(f\"Column '{column_name}' is missing from the GeoDataFrame!\")\n",
    "\n",
    "    joined_gdf[column_name] = joined_gdf[column_name].fillna(0).astype(float)\n",
    "\n",
    "    # Prepare shapes for rasterization\n",
    "    shapes = [(geom, value) for geom, value in zip(joined_gdf.geometry, joined_gdf[column_name]) if not np.isnan(value)]\n",
    "\n",
    "    # Create raster\n",
    "    raster = rasterize(\n",
    "        shapes=shapes,\n",
    "        out_shape=(height, width),\n",
    "        transform=transform,\n",
    "        fill=0,\n",
    "        dtype=np.float32\n",
    "    )\n",
    "\n",
    "    # Save to file\n",
    "    output_filename = f\"{my_path}{my_training_state}_{column_name}.tif\"\n",
    "    with rasterio.open(\n",
    "        output_filename, \"w\",\n",
    "        driver=\"GTiff\",\n",
    "        height=height,\n",
    "        width=width,\n",
    "        count=1,\n",
    "        dtype=rasterio.float32,\n",
    "        crs=joined_gdf.crs,  # Use the same projected CRS\n",
    "        transform=transform\n",
    "    ) as dst:\n",
    "        dst.write(raster, 1)\n",
    "        dst.set_band_description(1, column_name)\n",
    "\n",
    "    # Check raster output\n",
    "    #print(f\"Raster saved as: {output_filename}\")\n",
    "    #print(f\"Unique raster values: {np.unique(raster)}\")  # Ensure non-zero values exist\n",
    "\n",
    "    # Plot the raster\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(raster, cmap=\"viridis\", extent=[bounds[0], bounds[2], bounds[1], bounds[3]])\n",
    "    plt.colorbar(label=f'{column_name}')\n",
    "    plt.title('Rasterized MESS')\n",
    "    plt.xlabel('X (meters)')\n",
    "    plt.ylabel('Y (meters)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cb3fc8-3387-422a-9106-187619853ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbif_result = []\n",
    "for country in my_countries:\n",
    "    result = gbif_api_call(my_taxon, country, limit)\n",
    "    gbif_result.extend(result.get(\"results\", []))  # Append results directly\n",
    "# Extract fields from all collected results\n",
    "homerange_points = pd.DataFrame(extract_fields(gbif_result))\n",
    "homerange_points = gpd.GeoDataFrame(\n",
    "    homerange_points, geometry=gpd.points_from_xy(homerange_points.decimalLongitude, homerange_points.decimalLatitude)).dropna().set_crs(4269).to_crs(5070)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1803dc-9311-4bce-a1ed-da9dbbafac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raster dataset (assume multiband raster where each band is a predictor)\n",
    "raster_path = invaded_raster\n",
    "with rasterio.open(raster_path) as src:\n",
    "    out_image = src.read()  # Read all bands without masking\n",
    "    meta = src.meta  # Store metadata for later use\n",
    "    transform = src.transform  # Affine transform for georeferencing\n",
    "\n",
    "    # Extract band names or fallback to generic names\n",
    "    band_names = [src.descriptions[i] if src.descriptions and src.descriptions[i] else f\"Band_{i+1}\" \n",
    "                  for i in range(src.count)]\n",
    "    print(\"Extracted Band Names:\", band_names)  # Debugging step\n",
    "\n",
    "# Convert extracted raster data to a DataFrame\n",
    "bands, height, width = out_image.shape\n",
    "pixels = out_image.reshape(bands, -1).T  # Flatten to (num_pixels, num_bands)\n",
    "pred_data = pd.DataFrame(pixels, columns=band_names)\n",
    "\n",
    "# Handle NoData values (if applicable)\n",
    "if meta.get(\"nodata\") is not None:\n",
    "    pred_data.replace(meta[\"nodata\"], np.nan, inplace=True)\n",
    "\n",
    "# Generate coordinates for each pixel\n",
    "row_indices, col_indices = np.indices((height, width))\n",
    "x_coords, y_coords = rasterio.transform.xy(transform, row_indices.flatten(), col_indices.flatten())\n",
    "\n",
    "# Create geometries (Point objects)\n",
    "geometries = [Point(x, y) for x, y in zip(x_coords, y_coords)]\n",
    "\n",
    "# Convert DataFrame to GeoDataFrame\n",
    "my_pred_data = gpd.GeoDataFrame(pred_data, geometry=geometries, crs=4269).reset_index().rename(columns ={'index':'predID'})\n",
    "\n",
    "print(my_pred_data.head())  # Check first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257c4fa9-4fef-4260-9851-ed8424b2e72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_choices = ['NDBI', 'NDTI', 'NDSI', 'NDCI', 'GPP_Summer', 'gHM', \n",
    "            'Heat_Insolation', 'Topo_Diversity', 'Flashiness', 'LST_Summer',\n",
    "            'LST_Winter','NDVI','LST_Spring','LST_Fall', 'Precip_Winter', \n",
    "            'Precip_Spring', 'Precip_Summer', 'Precip_Fall', 'Drawdown', 'Runoff', 'geometry', 'predID']\n",
    "ref_data = sample_multiband_geotiff_with_names(homerange_raster, homerange_points)\n",
    "my_ref_data = filter_dataframe_columns(ref_data, feature_choices).dropna()\n",
    "my_pred_data = filter_dataframe_columns(my_pred_data, feature_choices).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cf78e4-3181-42be-a5ba-0cd62f6132c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_mess = MESS(my_ref_data, my_pred_data)\n",
    "my_mess_clean = my_mess.dropna()\n",
    "export_mess(my_mess_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f399d0-14ae-4d0a-98fd-6492ee0ca03f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
