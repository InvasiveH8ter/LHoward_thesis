{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6b12e1-c5a3-4dcc-9810-567be116b9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required packages\n",
    "import ee\n",
    "import os\n",
    "import geemap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from functools import reduce\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, RandomForestRegressor, GradientBoostingRegressor, VotingRegressor, StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.base import clone\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.feature_selection import RFE, SelectFromModel\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFECV\n",
    "from shapely.geometry import shape\n",
    "from tqdm import tqdm\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from rasterio.mask import mask\n",
    "import seaborn as sns\n",
    "import branca.colormap as bcm\n",
    "import folium\n",
    "from matplotlib import cm, colors\n",
    "from folium.raster_layers import ImageOverlay\n",
    "import matplotlib.colors as mcolors\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from PIL import Image\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ba05cd-43dd-45da-be56-1191c8ac4849",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_cloud_project = 'ee-YourProject' # your GEE cloud project ID\n",
    "ee.Authenticate() \n",
    "ee.Initialize(project=my_cloud_project) # your GEE cloud project\n",
    "my_scale = 1000\n",
    "my_state_abbrv = 'MN'\n",
    "all_params = [\n",
    "    'Ca',\n",
    "    'pH',\n",
    "    'Phosphorus',\n",
    "    'Nitrogen',\n",
    "    'Boater_Visitation',\n",
    "    'Inv_Algae',\n",
    "    'Inv_Crust',\n",
    "    'Inv_Fish',\n",
    "    'Inv_Mollusk',\n",
    "    'Inv_Plants',\n",
    "    'Native_Fish',\n",
    "    'homerange_sim',\n",
    "    'Freeze_Up',\n",
    "    # 'Ice_Melt',\n",
    "    'Spawn_Start',\n",
    "    'Spawn_End',\n",
    "    # 'Growing_Season_Ideal',\n",
    "    # 'Growing_Season_Length',\n",
    "    'Precip_Winter',\n",
    "    # 'Precip_Spring',\n",
    "    'Precip_Summer',\n",
    "    # 'Precip_Fall',\n",
    "    'Flashiness',\n",
    "    # 'Runoff',\n",
    "    #'Drawdown',\n",
    "    #'LST_Annual',\n",
    "    'LST_Summer',\n",
    "    'LST_Winter',\n",
    "    # 'LST_Spring',\n",
    "    # 'LST_Fall',\n",
    "    'NDVI',\n",
    "    'GPP_Annual',\n",
    "    # 'GPP_Summer',\n",
    "    'Heat_Insolation',\n",
    "    'Topo_Diversity',\n",
    "    'gHM',\n",
    "    'NDTI',\n",
    "    'NDBI',\n",
    "    'NDCI',\n",
    "    'NDSI',\n",
    "    'Distance'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2986a985-442e-4717-91fd-cd6115717125",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ee.FeatureCollection(\"TIGER/2018/States\")\n",
    "subbasins = ee.FeatureCollection(\"USGS/WBD/2017/HUC08\")\n",
    "my_predictors  = ee.Image('projects/' + my_cloud_project + '/assets/your_predictors').select(all_params)\n",
    "geo = states.filter(ee.Filter.eq(\"STUSPS\", my_state_abbrv)).geometry()\n",
    "pos_fc = ee.FeatureCollection('projects/' + my_cloud_project + '/assets/your_pos_data').filterBounds(geo)\n",
    "bg_fc = ee.FeatureCollection('projects/' + my_cloud_project + '/assets/your_bg_data').filterBounds(geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5c6f70-c88b-4394-aa43-20b0564742d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tag each point with the HUC8 code\n",
    "def add_huc8(fc, subbasins):\n",
    "    def add_props(f):\n",
    "        huc = ee.Feature(subbasins.filterBounds(f.geometry()).first())\n",
    "        return f.set('huc8', huc.get('huc8'))\n",
    "    return fc.map(add_props)\n",
    "\n",
    "# Apply to presence and background\n",
    "pos_with_huc = add_huc8(pos_fc, subbasins)\n",
    "bg_with_huc = add_huc8(bg_fc, subbasins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c5afe6-d17f-492a-b45b-32fb44db1913",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sample = my_predictors.reduceRegions(**{\n",
    "                          'collection': pos_with_huc,\n",
    "                          'reducer': ee.Reducer.mean(),\n",
    "                          'crs': 'EPSG:4326',\n",
    "                          'scale': my_scale,\n",
    "                          'tileScale': 16})\n",
    "pos_sample_df = ee.data.computeFeatures({\n",
    "    'expression': pos_sample,\n",
    "    'fileFormat': 'PANDAS_DATAFRAME'\n",
    "}).drop(columns = [\"geo\", \"lakeID\"]).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7216d0-2ff1-4da7-902e-dc88e4cc0372",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_fc_rand = bg_with_huc.randomColumn('rand')\n",
    "\n",
    "# Split into two roughly equal parts\n",
    "bg_fc_1 = bg_fc_rand.filter(ee.Filter.lt('rand', 0.5))\n",
    "bg_fc_2 = bg_fc_rand.filter(ee.Filter.gte('rand', 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a091fe98-9132-45ee-89dd-80f1978faa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_sample_fc_1 = my_predictors.reduceRegions(**{\n",
    "                          'collection': bg_fc_1,\n",
    "                          'reducer': ee.Reducer.mean(),\n",
    "                          'crs': 'EPSG:4326',\n",
    "                          'scale': my_scale,\n",
    "                          'tileScale': 16})\n",
    "bg_sample_fc_1_df = ee.data.computeFeatures({\n",
    "    'expression': bg_sample_fc_1,\n",
    "    'fileFormat': 'PANDAS_DATAFRAME'\n",
    "}).drop(columns = [\"geo\", \"lakeID\", \"rand\", \"rand_point\"]).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465f9f03-e99f-421d-aee6-969dc3fe80c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_sample_fc_2 = my_predictors.reduceRegions(**{\n",
    "                          'collection': bg_fc_2,\n",
    "                          'reducer': ee.Reducer.mean(),\n",
    "                          'crs': 'EPSG:4326',\n",
    "                          'scale': my_scale,\n",
    "                          'tileScale': 16})\n",
    "bg_sample_fc_2_df = ee.data.computeFeatures({\n",
    "    'expression': bg_sample_fc_2,\n",
    "    'fileFormat': 'PANDAS_DATAFRAME'\n",
    "}).drop(columns = [\"geo\", \"lakeID\", \"rand\", \"rand_point\"]).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac832fd-515c-460b-a93a-715296efaf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_points_df = pd.concat([pos_sample_df, bg_sample_fc_1_df, bg_sample_fc_2_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0668fb-8624-469d-8f5b-437a386bb340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only HUCs with both 0 and 1\n",
    "valid_hucs = (\n",
    "    all_points_df.groupby(\"huc8\")[\"Present\"]\n",
    "    .nunique()\n",
    "    .loc[lambda x: x > 1]\n",
    "    .index\n",
    ")\n",
    "\n",
    "dataset = all_points_df[all_points_df[\"huc8\"].isin(valid_hucs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bf4e6e-389d-410c-bd98-2933b722d5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the True Skill Statistic (TSS).\n",
    "    TSS = Sensitivity + Specificity - 1\n",
    "    \"\"\"\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    # Sensitivity (True Positive Rate)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "\n",
    "    # Specificity (True Negative Rate)\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "\n",
    "    tss = sensitivity + specificity - 1\n",
    "    return tss\n",
    "\n",
    "def leave_one_huc_out(df):\n",
    "    hucs = df[\"huc8\"].unique()\n",
    "    n_folds = len(hucs)\n",
    "    print(f\"Running leave-one-HUC-out cross validation with {n_folds} folds...\")\n",
    "\n",
    "    results = {m: [] for m in [\"MaxEnt\", \"RF\", \"DT\", \"BRT\", \"MLP\", \"Voting\", \"Stacking\"]}\n",
    "\n",
    "    # Progress bar over folds\n",
    "    for test_huc in tqdm(hucs, desc=\"Processing folds\", unit=\"fold\"):\n",
    "        train = df[df[\"huc8\"] != test_huc]\n",
    "        test = df[df[\"huc8\"] == test_huc]\n",
    "\n",
    "        X_train = train.drop(columns=[\"Present\", \"huc8\"])\n",
    "        y_train = train[\"Present\"]\n",
    "        X_test = test.drop(columns=[\"Present\", \"huc8\"])\n",
    "        y_test = test[\"Present\"]\n",
    "\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train_scaled = scaler.transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # Define models\n",
    "        models = {\n",
    "            \"MaxEnt\": LogisticRegression(max_iter=10000),\n",
    "            \"RF\": RandomForestClassifier(n_estimators=1000),\n",
    "            \"DT\": DecisionTreeClassifier(),\n",
    "            \"BRT\": GradientBoostingClassifier(n_estimators=1000),\n",
    "            \"MLP\": MLPClassifier(max_iter=10000)\n",
    "        }\n",
    "\n",
    "        # Fit base models\n",
    "        for name, model in models.items():\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            preds = model.predict(X_test_scaled)\n",
    "            results[name].append(compute_tss(y_test, preds))\n",
    "\n",
    "        # Ensembles\n",
    "        vc_names = [(k, v) for k, v in models.items()]\n",
    "        vc = VotingClassifier(estimators=vc_names, voting=\"soft\")\n",
    "        vc.fit(X_train_scaled, y_train)\n",
    "        results[\"Voting\"].append(compute_tss(y_test, vc.predict(X_test_scaled)))\n",
    "\n",
    "        stack = StackingClassifier(estimators=vc_names, final_estimator=RandomForestClassifier())\n",
    "        stack.fit(X_train_scaled, y_train)\n",
    "        results[\"Stacking\"].append(compute_tss(y_test, stack.predict(X_test_scaled)))\n",
    "\n",
    "    print(\"Cross validation complete.\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22f4e35-3a93-49f7-b8a1-d7a2b78eac68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first three unique HUCs\n",
    "first_three_hucs = dataset[\"huc8\"].unique()[:3]\n",
    "\n",
    "# Subset dataset to only those HUCs\n",
    "test_subset = dataset[dataset[\"huc8\"].isin(first_three_hucs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8135d876-7fcd-44cf-b414-bbba5c8193ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tss_results = leave_one_huc_out(test_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e3e081-050c-42d0-9291-c9eb86e388b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Step 1: Collect replicate TSS results into tidy DataFrame\n",
    "# -----------------\n",
    "rows = []\n",
    "for model_name, scores in tss_results.items():\n",
    "    for tss in scores:\n",
    "        rows.append({'Model': model_name, 'TSS': tss})\n",
    "tss_df = pd.DataFrame(rows)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.boxplot(data=tss_df, x='Model', y='TSS', palette='Set2')\n",
    "sns.swarmplot(data=tss_df, x='Model', y='TSS', color='k', alpha=0.5)\n",
    "plt.title('Distribution of TSS by Algorithm')\n",
    "plt.ylabel('TSS')\n",
    "plt.xlabel('Algorithm')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------\n",
    "# Step 2: Pick best model by mean TSS\n",
    "# -----------------\n",
    "avg_scores = {model: np.mean(scores) for model, scores in tss_results.items()}\n",
    "best_model_name = max(avg_scores, key=avg_scores.get)\n",
    "print(f\"✅ Best model: {best_model_name}\")\n",
    "\n",
    "# -----------------\n",
    "# Step 3: Fit best model on full dataset\n",
    "# -----------------\n",
    "X = all_points_df.drop(columns=[\"Present\", \"huc8\"])\n",
    "y = all_points_df[\"Present\"]\n",
    "feature_names = X.columns\n",
    "\n",
    "scaler = StandardScaler().fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "best_model = models[best_model_name]\n",
    "best_model.fit(X_scaled, y)\n",
    "print(f\"✅ Final {best_model_name} trained on all data\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b0d3bd-7984-4ff7-8401-f4485f90bfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Step 4: Feature importance (RFE, permutation, drop-column)\n",
    "# -----------------\n",
    "rfe_features = []\n",
    "min_features_rfe = 3\n",
    "if hasattr(best_model, \"coef_\") or hasattr(best_model, \"feature_importances_\"):\n",
    "    try:\n",
    "        rfe_selector = RFECV(\n",
    "            estimator=best_model,\n",
    "            min_features_to_select=min_features_rfe,\n",
    "            step=1,\n",
    "            cv=3,\n",
    "            scoring=\"roc_auc\"\n",
    "        )\n",
    "        rfe_selector.fit(X_scaled, y)\n",
    "        rfe_features = [feature_names[i] for i, keep in enumerate(rfe_selector.support_) if keep]\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Skipping RFE for {best_model_name}: {e}\")\n",
    "\n",
    "# Permutation importance (works for all)\n",
    "perm_imp = permutation_importance(best_model, X_scaled, y, n_repeats=3, random_state=42)\n",
    "perm_features = pd.Series(perm_imp.importances_mean, index=feature_names)\\\n",
    "                  .sort_values(ascending=False).index[:3].tolist()\n",
    "\n",
    "# Drop-column importance\n",
    "drop_col_feats = drop_col(best_model, X_scaled, y, feature_names=feature_names)\n",
    "drop_features = drop_col_feats.sort_values('feature_importance', ascending=False)['feature'][:3].tolist()\n",
    "\n",
    "# -----------------\n",
    "# Step 5: Plot feature importance comparison\n",
    "# -----------------\n",
    "fi_df = pd.DataFrame({\n",
    "    'RFE': rfe_features if rfe_features else [None]*len(feature_names),\n",
    "    'Permutation': perm_features,\n",
    "    'Drop-column': drop_features\n",
    "})\n",
    "\n",
    "# Melt + tidy for plotting\n",
    "fi_long = fi_df.melt(var_name='Method', value_name='Feature').dropna()\n",
    "fi_counts = fi_long.groupby(['Method','Feature']).size().reset_index(name='Count')\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(data=fi_counts, x='Feature', y='Count', hue='Method', palette='Set2')\n",
    "plt.title(f'Top Features for {best_model_name}')\n",
    "plt.ylabel('Importance / Selection Count')\n",
    "plt.xlabel('Feature')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Method')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883876b1-f6a4-4e06-9ee4-1c23bbac477d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Safe MESS function\n",
    "# -------------------------------\n",
    "def MESS_safe(ref_df, pred_df):\n",
    "    mins = dict(ref_df.min())\n",
    "    maxs = dict(ref_df.max())\n",
    "    epsilon = 1e-10\n",
    "\n",
    "    def calculate_s(column):\n",
    "        values = ref_df[column]\n",
    "        denom = maxs[column] - mins[column]\n",
    "        if denom == 0:\n",
    "            return [1.0] * len(pred_df)\n",
    "        sims = []\n",
    "        for element in np.array(pred_df[column]):\n",
    "            f = np.count_nonzero(values < element)/values.size\n",
    "            if f == 0:\n",
    "                sim = (element - mins[column])/denom\n",
    "            elif 0 < f <= 0.5:\n",
    "                sim = 2*f\n",
    "            elif 0.5 < f < 1:\n",
    "                sim = 2*(1-f)\n",
    "            else:  # f == 1\n",
    "                sim = (maxs[column] - element)/denom\n",
    "            sims.append(sim)\n",
    "        return sims\n",
    "\n",
    "    sim_df = pd.DataFrame({c: calculate_s(c) for c in pred_df.columns})\n",
    "    min_similarity = sim_df.min(axis=1, skipna=True)\n",
    "    MoD = sim_df.idxmin(axis=1, skipna=True).fillna('all_constant')\n",
    "    return pd.concat([min_similarity.rename('MESS'), MoD.rename('MoD')], axis=1)\n",
    "\n",
    "# -------------------------------\n",
    "# HUC-level pointwise MESS\n",
    "# -------------------------------\n",
    "def huc_pointwise_MESS(points_df, exclude_cols=['huc8','presence','geo','rand','year']):\n",
    "    \"\"\"\n",
    "    Computes point-level MESS and aggregates to HUC-level summaries.\n",
    "    \"\"\"\n",
    "    # Keep only numeric predictors\n",
    "    predictor_cols = [c for c in points_df.columns if c not in exclude_cols]\n",
    "    predictor_cols = [c for c in predictor_cols if pd.api.types.is_numeric_dtype(points_df[c])]\n",
    "    points_df[predictor_cols] = points_df[predictor_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    huc_results = []\n",
    "\n",
    "    for ref_huc in points_df['huc8'].unique():\n",
    "        ref_points = points_df[points_df['huc8'] == ref_huc][predictor_cols]\n",
    "        pred_points = points_df[points_df['huc8'] != ref_huc][predictor_cols]\n",
    "\n",
    "        mess_out = MESS_safe(ref_points, pred_points)\n",
    "        mess_out['ref_huc8'] = ref_huc\n",
    "        mess_out['pred_huc8'] = points_df.loc[points_df['huc8'] != ref_huc, 'huc8'].values\n",
    "\n",
    "        huc_results.append(mess_out)\n",
    "\n",
    "    huc_mess_df = pd.concat(huc_results, ignore_index=True)\n",
    "\n",
    "    # Aggregate to HUC-level summaries\n",
    "    huc_summary = huc_mess_df.groupby('ref_huc8').agg(\n",
    "        median_MESS=('MESS','median'),\n",
    "        IQR_MESS=('MESS', lambda x: np.percentile(x, 75)-np.percentile(x,25)),\n",
    "        top_MoD=('MoD', lambda x: x.value_counts().idxmax())\n",
    "    ).reset_index()\n",
    "\n",
    "    return huc_summary\n",
    "\n",
    "\n",
    "\n",
    "def predict_to_geotiff_full(clf, scaler, features, predictors_tif, \n",
    "                            boundary_shapefile=None, outdir=\"predictions\",\n",
    "                            out_name=\"prediction.tif\", block_size=512, normalize=True,\n",
    "                            return_image=False):\n",
    "    \"\"\"\n",
    "    Predict SDM probabilities to every pixel in a raster with optional masking\n",
    "    and normalization to 0-1. Optionally returns an RGBA array for Folium.\n",
    "    \"\"\"\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    out_path = os.path.join(outdir, out_name)\n",
    "\n",
    "    # Load boundary if provided\n",
    "    if boundary_shapefile:\n",
    "        boundary_gdf = gpd.read_file(boundary_shapefile).to_crs(epsg=5070)\n",
    "\n",
    "    with rasterio.open(predictors_tif) as src:\n",
    "        # Map feature names to band indices (1-based)\n",
    "        try:\n",
    "            band_indices = [src.descriptions.index(f) + 1 for f in features]\n",
    "        except ValueError as e:\n",
    "            missing = [f for f in features if f not in src.descriptions]\n",
    "            raise ValueError(f\"Missing features in raster bands: {missing}\")\n",
    "\n",
    "        height, width = src.height, src.width\n",
    "        prob_raster = np.full((height, width), np.nan, dtype=\"float32\")\n",
    "\n",
    "        # Block-wise prediction\n",
    "        for i in range(0, height, block_size):\n",
    "            for j in range(0, width, block_size):\n",
    "                h = min(block_size, height - i)\n",
    "                w = min(block_size, width - j)\n",
    "                window = Window(j, i, w, h)\n",
    "                data = src.read(band_indices, window=window)\n",
    "                n_bands, n_rows, n_cols = data.shape\n",
    "\n",
    "                X_block = data.reshape(n_bands, -1).T\n",
    "                mask_valid = ~np.any(np.isnan(X_block), axis=1)\n",
    "\n",
    "                if np.any(mask_valid):\n",
    "                    X_block_df = pd.DataFrame(X_block[mask_valid], columns=features)\n",
    "                    X_block_df = X_block_df[scaler.feature_names_in_]  # ensure correct order\n",
    "                    X_block_scaled = scaler.transform(X_block_df)\n",
    "                    probs = clf.predict_proba(X_block_scaled)[:, 1]\n",
    "\n",
    "                    flat_block = np.full((n_rows * n_cols,), np.nan, dtype=\"float32\")\n",
    "                    flat_block[mask_valid] = probs\n",
    "                    prob_raster[i:i+h, j:j+w] = flat_block.reshape((n_rows, n_cols))\n",
    "\n",
    "        raster_crs = src.crs\n",
    "        raster_transform = src.transform\n",
    "\n",
    "    # Normalize globally if requested\n",
    "    if normalize:\n",
    "        valid_mask = ~np.isnan(prob_raster)\n",
    "        min_val, max_val = prob_raster[valid_mask].min(), prob_raster[valid_mask].max()\n",
    "        prob_raster[valid_mask] = (prob_raster[valid_mask] - min_val) / (max_val - min_val + 1e-8)\n",
    "\n",
    "    # Prepare metadata for writing\n",
    "    out_meta = {\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"height\": height,\n",
    "        \"width\": width,\n",
    "        \"count\": 1,\n",
    "        \"dtype\": \"float32\",\n",
    "        \"crs\": raster_crs,\n",
    "        \"transform\": raster_transform\n",
    "    }\n",
    "\n",
    "    # Optional masking to boundary\n",
    "    if boundary_shapefile:\n",
    "        with rasterio.open(predictors_tif) as src_raster:\n",
    "            masked, out_transform = mask(src_raster, boundary_gdf.geometry, crop=False, nodata=np.nan)\n",
    "            prob_raster = masked[0]\n",
    "            out_meta[\"transform\"] = out_transform\n",
    "            out_meta[\"height\"] = masked.shape[1]\n",
    "            out_meta[\"width\"] = masked.shape[2]\n",
    "\n",
    "    # Write output raster\n",
    "    with rasterio.open(out_path, \"w\", **out_meta) as dest:\n",
    "        dest.write(prob_raster, 1)\n",
    "\n",
    "    print(f\"Prediction saved to {out_path}\")\n",
    "\n",
    "    rgba_img = None\n",
    "    if return_image:\n",
    "        # Mask nodata\n",
    "        prob_array = np.where(np.isnan(prob_raster), 0, prob_raster)\n",
    "        # Normalize 0-1\n",
    "        norm = colors.Normalize(vmin=0, vmax=1)\n",
    "        cmap = cm.get_cmap('YlOrRd')\n",
    "        rgba_img = cmap(norm(prob_array))\n",
    "        rgba_img = (rgba_img * 255).astype(np.uint8)\n",
    "        rgba_img = Image.fromarray(rgba_img)\n",
    "\n",
    "    if return_image:\n",
    "        return out_path, rgba_img\n",
    "    else:\n",
    "        return out_path\n",
    "\n",
    "\n",
    "def folium_heatmap_from_tif(tif_path, map_center=[46.5, -94.2], zoom_start=6, colormap='YlOrRd', opacity=0.6):\n",
    "    \"\"\"\n",
    "    Create a Folium map with a raster heatmap overlay from a GeoTIFF.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tif_path : str\n",
    "        Path to the predicted GeoTIFF (any CRS).\n",
    "    map_center : list\n",
    "        [lat, lon] center of the map.\n",
    "    zoom_start : int\n",
    "        Initial zoom level.\n",
    "    colormap : str\n",
    "        Matplotlib colormap name.\n",
    "    opacity : float\n",
    "        Overlay opacity.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    folium.Map object with heatmap overlay.\n",
    "    \"\"\"\n",
    "    # Open raster\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        src_array = src.read(1)\n",
    "        src_nodata = src.nodata\n",
    "        src_crs = src.crs\n",
    "        src_transform = src.transform\n",
    "        \n",
    "        # Mask nodata\n",
    "        src_array = np.where(src_array == src_nodata, np.nan, src_array)\n",
    "        \n",
    "        # Reproject to 4326\n",
    "        transform, width, height = calculate_default_transform(\n",
    "            src_crs, 'EPSG:4326', src.width, src.height, *src.bounds)\n",
    "        dst_array = np.empty((height, width), dtype=src_array.dtype)\n",
    "        \n",
    "        reproject(\n",
    "            source=src_array,\n",
    "            destination=dst_array,\n",
    "            src_transform=src_transform,\n",
    "            src_crs=src_crs,\n",
    "            dst_transform=transform,\n",
    "            dst_crs='EPSG:4326',\n",
    "            resampling=Resampling.nearest\n",
    "        )\n",
    "    \n",
    "    # Normalize for colormap\n",
    "    valid = ~np.isnan(dst_array)\n",
    "    vmin, vmax = dst_array[valid].min(), dst_array[valid].max()\n",
    "    norm_array = (dst_array - vmin) / (vmax - vmin + 1e-8)\n",
    "    norm_array = np.clip(norm_array, 0, 1)\n",
    "    \n",
    "    # Apply colormap\n",
    "    cmap = cm.get_cmap(colormap)\n",
    "    rgba_array = (cmap(norm_array) * 255).astype(np.uint8)\n",
    "    rgba_array[np.isnan(norm_array)] = [0,0,0,0]  # transparent for nodata\n",
    "    \n",
    "    # Save temporary PNG\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".png\", delete=False) as tmpfile:\n",
    "        tmp_path = tmpfile.name\n",
    "        Image.fromarray(rgba_array).save(tmp_path)\n",
    "    \n",
    "    # Compute bounds in 4326\n",
    "    left, bottom = transform * (0, height)\n",
    "    right, top = transform * (width, 0)\n",
    "    bounds = [[bottom, left], [top, right]]\n",
    "    \n",
    "    # Create Folium map\n",
    "    m = folium.Map(location=map_center, zoom_start=zoom_start)\n",
    "    \n",
    "    folium.raster_layers.ImageOverlay(\n",
    "        image=tmp_path,\n",
    "        bounds=bounds,\n",
    "        opacity=opacity,\n",
    "        interactive=True,\n",
    "        name=\"Predicted Suitability\"\n",
    "    ).add_to(m)\n",
    "    \n",
    "    folium.LayerControl().add_to(m)\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911617e5-08c8-488a-b547-b0f751ea52c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Example usage\n",
    "# -------------------------------\n",
    "huc_summary = huc_pointwise_MESS(all_points_df)\n",
    "print(huc_summary.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fe0f0c-516c-45ca-9573-2ab6e537e0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path, rgba_img = predict_to_geotiff_full(\n",
    "    clf=best_model,\n",
    "    scaler=scaler,\n",
    "    features=all_params,\n",
    "    predictors_tif=\"'predictors_your_taxa.tif\",\n",
    "    boundary_shapefile=\"your_state_border.shp\",\n",
    "    outdir=\".\",\n",
    "    out_name=\"pred.tif\",\n",
    "    return_image=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d075bc-ac1a-4080-80ee-9559f116cf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_pred = folium_heatmap_from_tif(\"pred.tif\")\n",
    "m_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfd7e52-4688-4277-aa9a-180e435d25f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "huc_gdf = gpd.read_file(\"huc_8.shp\")\n",
    "# Ensure column types match\n",
    "huc_gdf['huc8'] = huc_gdf['huc8'].astype(str)\n",
    "huc_summary['ref_huc8'] = huc_summary['ref_huc8'].astype(str)\n",
    "# Assuming huc_summary has 'median_MESS' column\n",
    "mess_min = huc_summary['median_MESS'].min()\n",
    "mess_max = huc_summary['median_MESS'].max()\n",
    "\n",
    "# Normalize to -1 → 1\n",
    "huc_summary['median_MESS_norm'] = 2 * ((huc_summary['median_MESS'] - mess_min) / (mess_max - mess_min)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d8c562-8e1e-4655-ae3f-1ee9aee31cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize negative MESS values to 0-1 (absolute distance from 0)\n",
    "neg_mask = huc_summary['median_MESS'] < 0\n",
    "neg_values = huc_summary.loc[neg_mask, 'median_MESS']\n",
    "max_neg = abs(neg_values.min()) if len(neg_values) > 0 else 1\n",
    "huc_summary['median_MESS_norm'] = 0  # default for >=0\n",
    "huc_summary.loc[neg_mask, 'median_MESS_norm'] = -neg_values / max_neg  # normalized 0 -> 1 for negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109c7058-b3df-4515-bee6-d19bbd4f798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "huc_mess_gdf = huc_gdf.merge(huc_summary, left_on='huc8', right_on='ref_huc8', how='left')\n",
    "huc_mess_gdf = huc_mess_gdf.dropna(subset=['median_MESS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f171b9-0fb9-4030-b498-05f2fd83b868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ensure no NaNs and define MESS color mapping ---\n",
    "def mess_to_color(mess_val, min_neg):\n",
    "    \"\"\"\n",
    "    Convert median MESS to grayscale.\n",
    "    Negative MESS -> white (most novel) to dark grey (less novel)\n",
    "    Non-negative MESS -> dark grey\n",
    "    \"\"\"\n",
    "    if mess_val >= 0:\n",
    "        return '#444444'  # solid dark grey\n",
    "    else:\n",
    "        norm_val = min(abs(mess_val)/abs(min_neg), 1)\n",
    "        grey_shade = 1 - norm_val  # 1=white, 0=black/dark\n",
    "        return mcolors.to_hex((grey_shade, grey_shade, grey_shade))\n",
    "\n",
    "min_neg_mess = huc_mess_gdf['median_MESS'][huc_mess_gdf['median_MESS'] < 0].min()\n",
    "huc_mess_gdf['fill_color'] = huc_mess_gdf['median_MESS'].apply(lambda x: mess_to_color(x, min_neg_mess))\n",
    "huc_mess_gdf = huc_mess_gdf.dropna(subset=['median_MESS'])\n",
    "\n",
    "# --- Create base map ---\n",
    "m_mess = folium.Map(location=[46.5, -94.2], zoom_start=6)\n",
    "\n",
    "# --- Add HUC8 MESS polygons ---\n",
    "folium.GeoJson(\n",
    "    huc_mess_gdf.to_json(),\n",
    "    style_function=lambda feature: {\n",
    "        'fillColor': feature['properties']['fill_color'],\n",
    "        'color': 'black',\n",
    "        'weight': 0.5,\n",
    "        'fillOpacity': 0.7\n",
    "    },\n",
    "    name=\"HUC8 MESS\"\n",
    ").add_to(m_mess)\n",
    "\n",
    "m_mess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fb5530-b1cb-4c8a-be28-50f953c83806",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
